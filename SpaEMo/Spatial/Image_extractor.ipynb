{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification, AutoImageProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model.eval()\n",
    "\n",
    "feature_extractor = model.get_image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from torchvision import transforms\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "dir = '../../../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px/train/01December_2011_Thursday_tagesschau-3479'\n",
    "# PyTorch transformation to resize and convert to tensor\n",
    "image_paths = os.listdir(dir)\n",
    "images = [] \n",
    "for path in image_paths: \n",
    "    actual_path = os.path.join(dir, path)\n",
    "    \n",
    "    # Load the image\n",
    "\n",
    "    img = Image.open(actual_path).convert(\"RGB\")  # Ensures image is in RGB format\n",
    "    images.append(img)\n",
    "\n",
    "processed_images = processor(images=images , return_tensors=\"pt\")\n",
    "print(processed_images.pixel_values.shape)\n",
    "# output = model.get_image_features(**processed_images)\n",
    "# feature_extractor = model.get_image_features\n",
    "# print(feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor(processed_images.pixel_values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in the model: 427616513\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters in the model: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating S^2 wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  ------------------------------------------------------------------------------------------\n",
    "#  Copyright (c) 2024 Baifeng Shi.\n",
    "#  All rights reserved.\n",
    "#\n",
    "#  Licensed under the MIT License (MIT). See LICENSE in the repo root for license information.\n",
    "#  ------------------------------------------------------------------------------------------\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "\n",
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model = AutoModelForZeroShotImageClassification.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "model.eval()\n",
    "feature_extractor = model.get_image_features\n",
    "\n",
    "def split_chessboard(x, num_split):\n",
    "    \"\"\"\n",
    "        x: b * c * h * w\n",
    "        Deividing x into num_split**2 sub-squares, and concatenate all the sub-squares on the batch dimension\n",
    "        E.g: If num_split =2, x will be divided into 4 sub-squares, and the output will be a tensor of shape 4b * c * h/2 * w/2\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    assert H % num_split == 0 and W % num_split == 0\n",
    "    h, w = H // num_split, W // num_split\n",
    "    x_split = torch.cat([x[:, :, i*h:(i+1)*h, j*w:(j+1)*w] for i in range(num_split) for j in range(num_split)], dim=0)\n",
    "    return x_split\n",
    "\n",
    "def merge_chessboard(x, num_split):\n",
    "    \"\"\"\n",
    "        x: b * c * h * w\n",
    "        Assuming x contains num_split**2 sub-squares concatenated along batch dimension, merge the sub-squares back to the original whole square.\n",
    "        (inverse of split_chessboard)\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    assert B % (num_split**2) == 0\n",
    "    b = B // (num_split**2)\n",
    "    x_merge = torch.cat([torch.cat([x[(i*num_split + j)*b:(i*num_split + j + 1)*b] for j in range(num_split)], dim=-1)\n",
    "                         for i in range(num_split)], dim=-2)\n",
    "    return x_merge\n",
    "\n",
    "\n",
    "def forward(\n",
    "    model, \n",
    "    input, \n",
    "    scales=None, \n",
    "    img_sizes=None, \n",
    "    max_split_size=None, \n",
    "    resize_output_to_idx=0, \n",
    "    num_prefix_token=0,\n",
    "    output_shape='bnc',\n",
    "):\n",
    "    assert input.dim() == 4, \"Input image must be in the shape of BxCxHxW.\"\n",
    "    assert input.shape[2] == input.shape[3], \"Currently only square images are supported.\"\n",
    "    assert output_shape in ['bnc', 'bchw'], \"Output shape should be either BxNxC (e.g., ViT) or BxCxHxW (e.g., ConvNet).\"\n",
    "    assert output_shape == 'bnc' or num_prefix_token == 0, \"For ConvNet there shouldn't be any prefix token.\"\n",
    "\n",
    "    b, c, input_size, _ = input.shape\n",
    "\n",
    "    # image size for each scale\n",
    "    assert scales is not None or img_sizes is not None, \"Please assign either scales or img_sizes.\"\n",
    "    \n",
    "    img_sizes = img_sizes or [int(input_size * scale) for scale in scales]\n",
    "    # img_sizes is a list of sizes to cut the image up into\n",
    "    # img_sizes should be bigger than 1 (best to be multiples of 2) or scales of 224\n",
    "    \n",
    "    # prepare multiscale inputs\n",
    "    max_split_size = max_split_size or input_size   # Default = 224; The maximum size of each split of image. Set as the input size by default\n",
    "    num_splits = [math.ceil(size / max_split_size) for size in img_sizes]   # number of splits each scale\n",
    "    input_multiscale = []\n",
    "    for size, num_split in zip(img_sizes, num_splits):\n",
    "        print(f\"size: {size}, num_split: {num_split}\")\n",
    "        x = F.interpolate(input.to(torch.float32), size=size, mode='bicubic').to(input.dtype) # resize the input image to the larger target size\n",
    "        x = split_chessboard(x, num_split=num_split)\n",
    "        input_multiscale.append(x)\n",
    "    print(len(input_multiscale))\n",
    "    print(input_multiscale[0].shape)    \n",
    "    # run feedforward on each scale\n",
    "    outs_multiscale = [model(x) for x in input_multiscale]\n",
    "    if num_prefix_token > 0:\n",
    "        outs_prefix_multiscale = [out[:, :num_prefix_token] for out in outs_multiscale]\n",
    "        outs_multiscale = [out[:, num_prefix_token:] for out in outs_multiscale]\n",
    "    if output_shape == 'bnc':\n",
    "        outs_multiscale = [rearrange(out, 'b (h w) c -> b c h w', h=int(out.shape[1] ** 0.5), w=int(out.shape[1] ** 0.5))\n",
    "                           for out in outs_multiscale]\n",
    "    \n",
    "    # merge outputs of different splits for each scale separately\n",
    "    outs_multiscale = [merge_chessboard(out, num_split=num_split) for num_split, out in zip(num_splits, outs_multiscale)]\n",
    "    \n",
    "    # interpolate outputs from different scales and concat together\n",
    "    output_size = outs_multiscale[resize_output_to_idx].shape[-2]\n",
    "    out = torch.cat([F.interpolate(outs_multiscale[i].to(torch.float32), size=output_size,\n",
    "                                   mode='area').to(outs_multiscale[i].dtype)\n",
    "                     for i in range(len(outs_multiscale))], dim=1)\n",
    "    \n",
    "    if output_shape == 'bnc':\n",
    "        out = rearrange(out, 'b c h w -> b (h w) c')\n",
    "    if num_prefix_token > 0:\n",
    "        # take the mean of prefix tokens from different splits for each scale\n",
    "        outs_prefix_multiscale = [torch.stack(out.split(b, dim=0), dim=0).mean(dim=0) for out in outs_prefix_multiscale]\n",
    "        out_prefix_multiscale = torch.cat(outs_prefix_multiscale, dim=-1)\n",
    "        out = torch.cat([out_prefix_multiscale, out], dim=1)\n",
    "    \n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 448, num_split: 2\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::upsample_bicubic2d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_ENABLE_MPS_FALLBACK\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m img_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m224\u001b[39m\n\u001b[0;32m----> 6\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessed_images\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m448\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SLT_FYP/SpaEMo/Spatial/s2wrapper.py:66\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, input, scales, img_sizes, max_split_size, resize_output_to_idx, num_prefix_token, output_shape)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size, num_split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(img_sizes, num_splits):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_split\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbicubic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m split_chessboard(x, num_split\u001b[38;5;241m=\u001b[39mnum_split)\n\u001b[1;32m     68\u001b[0m     input_multiscale\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/functional.py:4073\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   4071\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[1;32m   4072\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_upsample_bicubic2d_aa(\u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[0;32m-> 4073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsample_bicubic2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   4076\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot 3D input, but bilinear mode needs 4D input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::upsample_bicubic2d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "from s2wrapper import * \n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "img_sizes = 224\n",
    "\n",
    "out = forward(feature_extractor, processed_images.pixel_values.to(\"mps\"),img_sizes=[448])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
