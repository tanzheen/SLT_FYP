experiment:
  project: "SpaEMo_P14"
  name: "SpaEMo_P14_run1"
  output_dir: "../SpaEMo_P14_run1"
  save_every: 1
  log_every: 0.1
  eval_every: 1
  translate_every: 0.25
  log_grad_norm_every: 0.1
  resume: True
  init_weight:

model:
  tokenizer: text/m2m_1.2b
  emotion_model: trpakov/vit-face-expression
  spatio_model: openai/clip-vit-large-patch14
  motion_model: MCG-NJU/videomae-large-finetuned-kinetics
  llm: text/m2m_1.2b
  face_detector: emo/face_detection_yunet_2023mar.onnx
  transformer_type: seq2seq # "seq2seq" or "causal"
  emo_hiddim: 768
  spatio_hiddim: 768
  motion_hiddim: 1024
  multilingual: True 
  llm_hiddim: 1024
  adaptor_type : 1

dataset:
  name: PHOENIX2014T
  prompt: "Translate the following sentence into spoken German:"
  lang: de_DE
  train: ../../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/annotations/processed/labels_train.pkl
  dev: ../../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/annotations/processed/labels_dev.pkl
  test: ../../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/annotations/processed/labels_test.pkl
  img_path: ../../PHOENIX-2014-T-release-v3/PHOENIX-2014-T/features/fullFrame-210x260px # + <train/dev/test> + <video_name>
  max_length: 300
  params:
    num_workers: 4

optimizer:
  name: adamw
  params:
    learning_rate: 1e-4
    beta1: 0.9
    beta2: 0.98
    weight_decay: 0.01

lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 4000
    end_lr: 5e-5

training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 8
  mixed_precision: "no"
  enable_tf32: False
  enable_wandb: False # Set to True to enable wandb logging
  use_ema: False
  seed: 42
  num_translated_images: 2
  max_grad_norm: 1.0
  num_epochs: 200
  scale_embedding: False
  token_usage: False # Set to True to use token usage
  vt_align: True # Set to True to use visual token alignment
  s2_scale: False
  vt_steps: 3000
