{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Causal model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.models.mbart.modeling_mbart import shift_tokens_right\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gemma_instruct_7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gemma_instruct_7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,  33571,  41516,   3670,   1600, 172299,  78450,   2029, 212951,\n",
      "           6765, 235292,  45310,    603,    490,  36236,   9070,    753,  42442,\n",
      "           1748, 235265]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "<bos>Übersetzen Sie den gegebenen Satz ins Chinesische: Mutter isst gerne Brathähnchen.\n",
      "\n",
      "Mutter isst gerne Brathähnchen.\n",
      "\n",
      "→ 母亲喜欢吃烤鸡。<eos>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Übersetzen Sie den gegebenen Satz ins Chinesische: Mutter isst gerne Brathähnchen.\"\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "outputs = model.generate(**input_ids, max_length = 50 )\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     2,  49688,    573,   2764,  13060,   1280,   8974, 235292,  17025,\n",
       "          16147,    577,   7812,  30196,  53190, 235265, 235248,    109,  50039,\n",
       "         235465,  56506]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,  49688,    573,   2764,  13060,   1280,   8974, 235292,    590,\n",
      "           1154,    577,   7812,  30196,  12254, 235265]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Translate the given sentence into Chinese: I like to eat fried chicken.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 2048])\n"
     ]
    }
   ],
   "source": [
    "# Extract input embeddings using the model's embedding layer\n",
    "import torch \n",
    "with torch.no_grad():\n",
    "    # Get the input embeddings\n",
    "    input_embeds = model.get_input_embeddings()(input_ids[\"input_ids\"])\n",
    "    print(input_embeds.shape)\n",
    "#output = model(inputs_embeds = input_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_ids = input_ids.input_ids, labels = input_ids.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing gemma masking for causal finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gemma_instruct_2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gemma_instruct_2b\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 235248], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<eos>', ' 你']\n",
      "['<eos>', ' ']\n",
      "['<eos>', '喜欢']\n",
      "[' ', '妈妈', '喜欢', '吃', '炸', '鸡', '。']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(torch.tensor([1,  30485], dtype=torch.long)))\n",
    "print(tokenizer.batch_decode(torch.tensor([1,  235248], dtype=torch.long)))\n",
    "print(tokenizer.batch_decode(torch.tensor([1,  19891], dtype=torch.long)))\n",
    "print(tokenizer.batch_decode(torch.tensor([235248,  42130,  19891, 236280, 238069, 237619, 235362], dtype=torch.long)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lowest loss: \n",
    "concated then tokenized tensor([[     2,  49688,    573,   2764,  13060,   1280,   8974, 235292,   1646,\n",
    "           2182,    577,   7812,  30196,  53190, 235265,  30485,  19891, 236280,\n",
    "         238069, 237619, 235362]])\n",
    "         \n",
    "Remember to absorb the space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 15])\n",
      "combined_ids: torch.Size([1, 22])\n",
      "ans_text: torch.Size([1, 8])\n",
      "15\n",
      "22\n",
      "tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100, 235248,  42130,  19891,\n",
      "        236280, 238069, 237619, 235362])\n",
      "len(labels): 22\n",
      "len(combined_ids): 22\n",
      "len(ans_ids): 8\n",
      "concated_ids: torch.Size([1, 22])\n",
      "['<bos>Translate the given sentence into Chinese: Mother loves to eat fried chickens. 妈妈喜欢吃炸鸡。']\n",
      "['<bos>Translate the given sentence into Chinese: Mother loves to eat fried chickens. 妈妈喜欢吃炸鸡。']\n",
      "tensor([[     2,  49688,    573,   2764,  13060,   1280,   8974, 235292,  17025,\n",
      "          16147,    577,   7812,  30196,  53190, 235265, 235248,  42130,  19891,\n",
      "         236280, 238069, 237619, 235362]])\n",
      "tensor([[     2,  49688,    573,   2764,  13060,   1280,   8974, 235292,  17025,\n",
      "          16147,    577,   7812,  30196,  53190, 235265, 235248,  42130,  19891,\n",
      "         236280, 238069, 237619, 235362]])\n",
      "tensor(1.4907, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4907, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_text = \"Translate the given sentence into Chinese: Mother loves to eat fried chickens.\"\n",
    "ans_text = \" 妈妈喜欢吃炸鸡。\"\n",
    "combined_text = input_text +  ans_text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "combined_ids = tokenizer(combined_text, return_tensors=\"pt\")\n",
    "ans_ids = tokenizer(ans_text, return_tensors=\"pt\")\n",
    "\n",
    "#19891 --》 194816\n",
    "\n",
    "\n",
    "\n",
    "labels = combined_ids[\"input_ids\"][0].clone()\n",
    "labels[:len(input_ids.input_ids[0])] = -100\n",
    "print(f\"input_ids: {input_ids.input_ids.shape}\")\n",
    "print(f\"combined_ids: {combined_ids.input_ids.shape}\")\n",
    "print(f\"ans_text: {ans_ids.input_ids.shape}\")\n",
    "print(len(input_ids[\"input_ids\"][0]))\n",
    "print(len(combined_ids[\"input_ids\"][0]))\n",
    "print(labels)\n",
    "print(f\"len(labels): {len(labels)}\")\n",
    "print(f\"len(combined_ids): {len(combined_ids['input_ids'][0])}\")\n",
    "print(f\"len(ans_ids): {len(ans_ids['input_ids'][0])}\")\n",
    "concated_ids = torch.cat([input_ids[\"input_ids\"], ans_ids[\"input_ids\"][:,1:]], dim = 1)\n",
    "print(f\"concated_ids: {concated_ids.shape}\")    \n",
    "print(tokenizer.batch_decode(concated_ids , skip_special_tokens=False))\n",
    "print(tokenizer.batch_decode(combined_ids[\"input_ids\"], skip_special_tokens=False))\n",
    "#print(model(input_ids = combined_ids.input_ids, labels = labels.unsqueeze(0))) \n",
    "print(concated_ids)\n",
    "print(combined_ids[\"input_ids\"])\n",
    "print(model(input_ids = concated_ids, labels = labels.unsqueeze(0)).loss)\n",
    "print(model(input_ids = combined_ids['input_ids'], labels = labels.unsqueeze(0)).loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 13])\n",
      "combined_ids: torch.Size([1, 18])\n",
      "ans_text: torch.Size([1, 6])\n",
      "13\n",
      "18\n",
      "tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,  25736,  19891, 236280, 237098, 235362])\n",
      "len(labels): 18\n",
      "len(combined_ids): 18\n",
      "len(ans_ids): 6\n",
      "concated_ids: torch.Size([1, 18])\n",
      "['<bos>Translate the given sentence into Chinese: I enjoy eating fish. 我喜欢吃鱼。']\n",
      "['<bos>Translate the given sentence into Chinese: I enjoy eating fish. 我喜欢吃鱼。']\n",
      "tensor([[     2,  49688,    573,   2764,  13060,   1280,   8974, 235292,    590,\n",
      "           4313,  12150,   5001, 235265,  25736,  19891, 236280, 237098, 235362]])\n",
      "tensor([[     2,  49688,    573,   2764,  13060,   1280,   8974, 235292,    590,\n",
      "           4313,  12150,   5001, 235265,  25736,  19891, 236280, 237098, 235362]])\n",
      "tensor(3.6193, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6193, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_text = \"Translate the given sentence into Chinese: I enjoy eating fish.\"\n",
    "ans_text = \" 我喜欢吃鱼。\"\n",
    "combined_text = input_text +  ans_text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "combined_ids = tokenizer(combined_text, return_tensors=\"pt\")\n",
    "ans_ids = tokenizer(ans_text, return_tensors=\"pt\")\n",
    "\n",
    "#19891 --》 194816\n",
    "\n",
    "\n",
    "\n",
    "labels = combined_ids[\"input_ids\"][0].clone()\n",
    "labels[:len(input_ids.input_ids[0])] = -100\n",
    "print(f\"input_ids: {input_ids.input_ids.shape}\")\n",
    "print(f\"combined_ids: {combined_ids.input_ids.shape}\")\n",
    "print(f\"ans_text: {ans_ids.input_ids.shape}\")\n",
    "print(len(input_ids[\"input_ids\"][0]))\n",
    "print(len(combined_ids[\"input_ids\"][0]))\n",
    "print(labels)\n",
    "print(f\"len(labels): {len(labels)}\")\n",
    "print(f\"len(combined_ids): {len(combined_ids['input_ids'][0])}\")\n",
    "print(f\"len(ans_ids): {len(ans_ids['input_ids'][0])}\")\n",
    "concated_ids = torch.cat([input_ids[\"input_ids\"], ans_ids[\"input_ids\"][:,1:]], dim = 1)\n",
    "print(f\"concated_ids: {concated_ids.shape}\")    \n",
    "print(tokenizer.batch_decode(concated_ids , skip_special_tokens=False))\n",
    "print(tokenizer.batch_decode(combined_ids[\"input_ids\"], skip_special_tokens=False))\n",
    "#print(model(input_ids = combined_ids.input_ids, labels = labels.unsqueeze(0))) \n",
    "print(concated_ids)\n",
    "print(combined_ids[\"input_ids\"])\n",
    "print(model(input_ids = concated_ids, labels = labels.unsqueeze(0)).loss)\n",
    "print(model(input_ids = combined_ids['input_ids'], labels = labels.unsqueeze(0)).loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
    "           -100,   -100,   -100,   -100,   -100,   -100, 235248,  42130,  19891,\n",
    "         236280, 238069, 237619, 235362],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is a difference between concatenating them as a combined sentence then tokenising versus tokenising individual parts and then concatenating  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Gemma with embeddings and see if the loss is still the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 2048])\n",
      "torch.Size([1, 6, 2048])\n",
      "torch.Size([1, 18, 2048])\n",
      "torch.Size([1, 18, 2048])\n",
      "tensor(3.6193, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Get the input embeddings\n",
    "    input_embeds = model.get_input_embeddings()(input_ids[\"input_ids\"])\n",
    "    print(input_embeds.shape)\n",
    "    ans_embeds = model.get_input_embeddings()(ans_ids[\"input_ids\"])\n",
    "    print(ans_embeds.shape)\n",
    "    combined_embeds = model.get_input_embeddings()(combined_ids[\"input_ids\"])\n",
    "    combined_embed_v2 = torch.cat((input_embeds, ans_embeds[:,1:]), dim = 1)\n",
    "    print(combined_embeds.shape)\n",
    "    print(combined_embed_v2.shape)\n",
    "\n",
    "print(model(inputs_embeds = combined_embed_v2, labels = labels.unsqueeze(0)).loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([1, 15])\n",
      "combined_ids: torch.Size([1, 22])\n",
      "ans_text: torch.Size([1, 8])\n",
      "15\n",
      "22\n",
      "tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100, 235248,  42130,  19891,\n",
      "        236280, 238069, 237619, 235362])\n",
      "len(labels): 22\n",
      "len(combined_ids): 22\n",
      "len(ans_ids): 8\n",
      "concated_ids: torch.Size([1, 22])\n",
      "['<bos>Translate the given sentence into Chinese: Mother loves to eat fried chickens. 妈妈喜欢吃炸鸡。']\n",
      "['<bos>Translate the given sentence into Chinese: Mother loves to eat fried chickens. 妈妈喜欢吃炸鸡。']\n",
      "tensor([[     2,  49688,    573,   2764,  13060,   1280,   8974, 235292,  17025,\n",
      "          16147,    577,   7812,  30196,  53190, 235265, 235248,  42130,  19891,\n",
      "         236280, 238069, 237619, 235362]])\n",
      "tensor([[     2,  49688,    573,   2764,  13060,   1280,   8974, 235292,  17025,\n",
      "          16147,    577,   7812,  30196,  53190, 235265, 235248,  42130,  19891,\n",
      "         236280, 238069, 237619, 235362]])\n",
      "tensor(1.4907, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.4907, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "input_text = \"Translate the given sentence into Chinese: Mother loves to eat fried chickens.\"\n",
    "ans_text = \" 妈妈喜欢吃炸鸡。\"\n",
    "combined_text = input_text +  ans_text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "combined_ids = tokenizer(combined_text, return_tensors=\"pt\")\n",
    "ans_ids = tokenizer(ans_text, return_tensors=\"pt\")\n",
    "\n",
    "#19891 --》 194816\n",
    "\n",
    "space_token = 235248 \n",
    "\n",
    "labels = combined_ids[\"input_ids\"][0].clone()\n",
    "labels[:len(input_ids.input_ids[0])] = -100\n",
    "print(f\"input_ids: {input_ids.input_ids.shape}\")\n",
    "print(f\"combined_ids: {combined_ids.input_ids.shape}\")\n",
    "print(f\"ans_text: {ans_ids.input_ids.shape}\")\n",
    "print(len(input_ids[\"input_ids\"][0]))\n",
    "print(len(combined_ids[\"input_ids\"][0]))\n",
    "print(labels)\n",
    "print(f\"len(labels): {len(labels)}\")\n",
    "print(f\"len(combined_ids): {len(combined_ids['input_ids'][0])}\")\n",
    "print(f\"len(ans_ids): {len(ans_ids['input_ids'][0])}\")\n",
    "concated_ids = torch.cat([input_ids[\"input_ids\"], ans_ids[\"input_ids\"][:,1:]], dim = 1)\n",
    "print(f\"concated_ids: {concated_ids.shape}\")    \n",
    "print(tokenizer.batch_decode(concated_ids , skip_special_tokens=False))\n",
    "print(tokenizer.batch_decode(combined_ids[\"input_ids\"], skip_special_tokens=False))\n",
    "#print(model(input_ids = combined_ids.input_ids, labels = labels.unsqueeze(0))) \n",
    "print(concated_ids)\n",
    "print(combined_ids[\"input_ids\"])\n",
    "print(model(input_ids = concated_ids, labels = labels.unsqueeze(0)).loss)\n",
    "print(model(input_ids = combined_ids['input_ids'], labels = labels.unsqueeze(0)).loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeds = model.get_input_embeddings()(combined_ids['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21])\n"
     ]
    }
   ],
   "source": [
    "print(labels.unsqueeze(0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though there is a difference in the loss values, let's just go with it first. （check if still applicable）\n",
    "\n",
    "My stance now is that we put the space before the target sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这句话的意思是：我喜欢吃炸鸡。\n",
      "\n",
      "This translation is correct. It accurately captures the meaning of the sentence \"I like to eat fried chicken.\"<eos>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0][len(input_ids.input_ids[0]):], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Causal inputs and Causal outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,  17025,  16147,    577,   7812,  30196,  53190, 235265]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor(1.4907, grad_fn=<NllLossBackward0>)\n",
      "{'input_ids': tensor([[     2,    590,   4313,  12150,   5001, 235265]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "tensor(3.6193, grad_fn=<NllLossBackward0>)\n",
      "input ids {'input_ids': tensor([[     2,  17025,  16147,    577,   7812,  30196,  53190, 235265],\n",
      "        [     2,    590,   4313,  12150,   5001, 235265,      0,      0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "tgt_ids  {'input_ids': tensor([[     2, 235248,  42130,  19891, 236280, 238069, 237619, 235362],\n",
      "        [     2,  25736,  19891, 236280, 237098, 235362,      0,      0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "torch.Size([2, 22, 2048])\n",
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100, 235248,  42130,  19891,\n",
      "         236280, 238069, 237619, 235362],\n",
      "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,  25736,  19891, 236280, 237098, 235362,\n",
      "           -100,   -100,   -100,   -100]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.3777, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_causal_inputs(visual_feats ,visual_attn,  tgt_ids): \n",
    "    prompt = \"Translate the given sentence into Chinese:\" # The space will be added by the model itself\n",
    "    prompt_ids = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    prompt_len = len(prompt_ids[\"input_ids\"][0])\n",
    "    #print(f\"prompt_len: {prompt_len}\")\n",
    "    #print(f\"visual feats shape: {visual_feats.shape}\")\n",
    "    prompt_embeds = model.get_input_embeddings()(prompt_ids[\"input_ids\"]).squeeze()\n",
    "    tgt_embeds = model.get_input_embeddings()(tgt_ids[\"input_ids\"])\n",
    "\n",
    "    #print(f\"tgt embeds: {tgt_embeds.shape}\")\n",
    "    tgt_attn = tgt_ids[\"attention_mask\"]\n",
    "    #print(f\"tgt attn: {tgt_attn.shape}\")\n",
    "    new_input_embeds = [] \n",
    "    new_labels = []\n",
    "\n",
    "    for i in range(tgt_embeds.shape[0]): # batch size\n",
    "        curr_vis_feats = visual_feats[i] # take the current visual features\n",
    "        curr_vis_attn = visual_attn[i] # take the current visual attention mask\n",
    "        curr_vis_feats = curr_vis_feats[curr_vis_attn==1] # only take the visual features that are attended to\n",
    "\n",
    "        curr_vis_len = curr_vis_feats.shape[0] # get the length of the visual features\n",
    "\n",
    "        curr_tgt_embeds = tgt_embeds[i] # take the current target embeddings\n",
    "\n",
    "        curr_tgt_feats = curr_tgt_embeds[tgt_attn[i]==1][1:] # only take the target embeddings that are attended to, remove the bos token \n",
    " \n",
    "        combined_embeds = torch.cat((prompt_embeds, curr_vis_feats, curr_tgt_feats), dim = 0) #Concat all the embeddings\n",
    "        #print(f\"combined_embeds: {combined_embeds}\")\n",
    "        new_input_embeds.append(combined_embeds) \n",
    "        negate_tgt = torch.full((1, prompt_len + curr_vis_len), -100) # create the -100 labels for the model (only the target text is not -100)\n",
    "        #print(f\"negate_tgt: {negate_tgt.shape}\")\n",
    "        labels =torch.cat([negate_tgt,  tgt_ids[\"input_ids\"][i][tgt_attn[i]==1][1:].clone().unsqueeze(0)], dim =1).permute(1,0) # Concat both the -100s and the target text\n",
    "\n",
    "        new_labels.append(labels) # append the labels\n",
    "        assert labels.shape[0] == len(combined_embeds), f\"len labels: {labels.shape} vs len combined_embeds: {combined_embeds.shape}\" \n",
    "        # assert the length of the labels is the same as the combined embeddings\n",
    "    \n",
    "    # perform padding for the batch before returning\n",
    "    new_input_embeds = torch.nn.utils.rnn.pad_sequence(new_input_embeds, batch_first=True, padding_value=0)\n",
    "    #print(\"HERE\", [labels.shape for labels in new_labels])\n",
    "    new_labels = torch.nn.utils.rnn.pad_sequence(new_labels, batch_first=True, padding_value=-100).squeeze()\n",
    "    new_labels[new_labels==0]=-100\n",
    "    return new_input_embeds, new_labels\n",
    "\n",
    "'''Batch of 1 test case'''\n",
    "input_text = \" Mother loves to eat fried chickens.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "\n",
    "input_embeds = model.get_input_embeddings()(input_ids[\"input_ids\"])[:, 1: ] # special case need to cut the extra bos token in front for the english sentence too\n",
    "ans_text = \" 妈妈喜欢吃炸鸡。\"\n",
    "tgt_ids = tokenizer(ans_text , return_tensors=\"pt\", add_special_tokens=True)\n",
    "new_input_embeds , new_labels=  create_causal_inputs(input_embeds,input_ids.attention_mask[:, 1:],  tgt_ids)\n",
    "\n",
    "print(model(inputs_embeds= new_input_embeds, labels =new_labels).loss) \n",
    "\n",
    "'''Batch of 1 test case NUMBER 2'''\n",
    "input_text = \" I enjoy eating fish.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "\n",
    "input_embeds = model.get_input_embeddings()(input_ids[\"input_ids\"])[:, 1: ] # special case need to cut the extra bos token in front for the english sentence too\n",
    "ans_text = \" 我喜欢吃鱼。\"\n",
    "tgt_ids = tokenizer(ans_text , return_tensors=\"pt\", add_special_tokens=True)\n",
    "new_input_embeds , new_labels=  create_causal_inputs(input_embeds, input_ids.attention_mask[:, 1:], tgt_ids)\n",
    "print(model(inputs_embeds= new_input_embeds, labels =new_labels).loss) \n",
    "    \n",
    "'''Batch of 2 test case'''\n",
    "input_text = [\" Mother loves to eat fried chickens.\",\" I enjoy eating fish.\"]\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", padding = True )\n",
    "print(\"input ids\", input_ids)\n",
    "input_embeds = model.get_input_embeddings()(input_ids[\"input_ids\"])[:, 1: ] # special case need to cut the extra bos token in front for the english sentence too\n",
    "ans_text = [\" 妈妈喜欢吃炸鸡。\", \" 我喜欢吃鱼。\"]\n",
    "tgt_tokenizer = AutoTokenizer.from_pretrained(\"gemma_instruct_2b\")\n",
    "tgt_tokenizer.padding_side = \"right\"\n",
    "tgt_ids = tgt_tokenizer(ans_text , return_tensors=\"pt\", add_special_tokens=True, padding = True)\n",
    "print(\"tgt_ids \", tgt_ids)\n",
    "new_input_embeds , new_labels =  create_causal_inputs(input_embeds, input_ids.attention_mask [:, 1:], tgt_ids)\n",
    "print(new_input_embeds.shape)\n",
    "print(new_labels)\n",
    "model(inputs_embeds= new_input_embeds, labels =new_labels).loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1035,  0.0052, -0.0330,  ..., -0.0170, -0.0087, -0.0099],\n",
       "         [ 0.2734, -0.0055,  0.0292,  ..., -0.0077,  0.0947,  0.0315],\n",
       "         [ 0.2373, -0.0249, -0.0918,  ..., -0.0045,  0.0388,  0.0212],\n",
       "         ...,\n",
       "         [ 0.2490, -0.0132, -0.0410,  ..., -0.0186,  0.0549,  0.0041],\n",
       "         [ 0.2344, -0.0138,  0.0146,  ..., -0.0047,  0.0442,  0.0232],\n",
       "         [ 0.1895, -0.0265, -0.0408,  ..., -0.0092, -0.0200, -0.0006]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_embeds.shape)\n",
    "test_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.1035,  0.0052, -0.0330,  ..., -0.0170, -0.0087, -0.0099],\n",
       "         [ 0.2734, -0.0055,  0.0292,  ..., -0.0077,  0.0947,  0.0315],\n",
       "         [ 0.2373, -0.0249, -0.0918,  ..., -0.0045,  0.0388,  0.0212],\n",
       "         ...,\n",
       "         [ 0.2197,  0.0217, -0.1196,  ...,  0.0535, -0.0075, -0.0242],\n",
       "         [ 0.2197,  0.0217, -0.1196,  ...,  0.0535, -0.0075, -0.0242],\n",
       "         [ 0.2197,  0.0217, -0.1196,  ...,  0.0535, -0.0075, -0.0242]],\n",
       "        grad_fn=<CatBackward0>)]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_input_embeds.shape)\n",
    "new_input_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "          -100,   -100,   -100,   -100,   -100,   -100, 235248,  42130,  19891,\n",
       "        236280, 238069, 237619, 235362])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "         1., 0., 0.]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,  25736,  19891, 236280,\n",
       "         238069, 237619, 235362],\n",
       "        [  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,  25736, 113066, 107590, 235362,\n",
       "           -100,   -100,   -100]])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 21, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(new_input_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "         1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out M2M seq2seq training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Life is like a box of chocolate.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "hi_text = \"जीवन एक चॉकलेट बॉक्स की तरह है।\"\n",
    "chinese_text = \"生活就像一盒巧克力。\"\n",
    "\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"m2m_1.2b\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"m2m_1.2b\")\n",
    "\n",
    "# # translate Hindi to French\n",
    "# tokenizer.src_lang = \"hi\"\n",
    "# encoded_hi = tokenizer(hi_text, return_tensors=\"pt\")\n",
    "# generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\n",
    "# tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "# # => \"La vie est comme une boîte de chocolat.\"\n",
    "\n",
    "# translate Chinese to English\n",
    "tokenizer.src_lang = \"zh\"\n",
    "encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "# => \"Life is like a box of chocolate.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d9d92e9dfd4b02a24b55665b00430b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load Gemma tokenizer and model\n",
    "model_name = \"gemma_instruct_2b\"  # Replace with your specific Gemma model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,538,944 || all params: 1,243,009,024 || trainable%: 0.2847\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",   # Type of task\n",
    "    inference_mode=False,    # Enable training\n",
    "    r=8,                     # Low-rank dimension\n",
    "    lora_alpha=16,           # Scaling factor\n",
    "    lora_dropout=0.1,        # Dropout rate for LoRA\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Target modules (Gemma specific)\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GemmaForCausalLM(\n",
       "      (model): GemmaModel(\n",
       "        (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-17): 18 x GemmaDecoderLayer(\n",
       "            (self_attn): GemmaSdpaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): GemmaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): GemmaMLP(\n",
       "              (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "              (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "              (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 1,241,829,376 || trainable%: 0.1900\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "# Load M2M100 model and tokenizer\n",
    "model_name = \"m2m_1.2b\"  # Or \"facebook/m2m100_1.2B\"\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"SEQ_2_SEQ_LM\",   # Task type for sequence-to-sequence models\n",
    "    inference_mode=False,       # Enable training mode\n",
    "    r=8,                        # Low-rank dimension\n",
    "    lora_alpha=16,              # Scaling factor\n",
    "    lora_dropout=0.1,           # Dropout rate\n",
    "    target_modules=[\"q_proj\", \"k_proj\"]  # Target specific layers in the attention mechanism\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "lora_model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0y/141839514p7_t64fbxyb46x40000gn/T/ipykernel_52301/4201358118.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  lgt = torch.tensor(lgt)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "\n",
    "def update_lgt(self, lgt):\n",
    "    lgt = torch.tensor(lgt)\n",
    "    feat_len = copy.deepcopy(lgt)  # Deep copy the input\n",
    "    for ks in self.kernel_size:\n",
    "        if ks[0] == 'P':\n",
    "            feat_len = torch.div(feat_len, 2)\n",
    "        else:\n",
    "            feat_len -= int(ks[1]) - 1\n",
    "    lgt = lgt.cpu().to(torch.int).tolist()\n",
    "    return feat_len\n",
    "\n",
    "# Example usage\n",
    "class ExampleModel:\n",
    "    def __init__(self):\n",
    "        self.kernel_size = [('P', 2), ('C', 3)]\n",
    "\n",
    "model = ExampleModel()\n",
    "\n",
    "# Python list\n",
    "lgt_list = [8, 16, 32]\n",
    "\n",
    "# Convert to tensor\n",
    "lgt_tensor = torch.tensor(lgt_list, dtype=torch.int32)\n",
    "\n",
    "# Call the function\n",
    "updated_lgt = update_lgt(model, lgt_tensor)\n",
    "\n",
    "print(updated_lgt[1].item())  # Output will be a PyTorch tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Translate the following English sentence into French: The weather is beautiful today.\n",
      "Output Text: La météo est superbe aujourd'hui.\n",
      "Embedding Shape: torch.Size([1, 16, 768])\n",
      "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[-0.0636,  0.0402,  0.1111,  ..., -0.0441, -0.0149,  0.1803],\n",
      "         [-0.0834,  0.0254,  0.1176,  ..., -0.0673, -0.0142,  0.1538],\n",
      "         [-0.0116, -0.0046,  0.0072,  ...,  0.0003,  0.0025, -0.0064],\n",
      "         ...,\n",
      "         [ 0.0530, -0.0140, -0.0764,  ...,  0.0693,  0.0731,  0.1969],\n",
      "         [ 0.0275,  0.0268, -0.0502,  ..., -0.1072, -0.0836,  0.0221],\n",
      "         [ 0.0106,  0.0061,  0.0141,  ...,  0.0032, -0.0032,  0.0017]]],\n",
      "       grad_fn=<MulBackward0>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the Flan-T5 model and tokenizer\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example input sentence\n",
    "input_text = \"Translate the following English sentence into French: The weather is beautiful today.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Retrieve embeddings\n",
    "embed_tokens = model.get_input_embeddings()\n",
    "embeddings = embed_tokens(input_ids)\n",
    "input_embeds = model.encoder(input_ids)\n",
    "# Generate output (translation)\n",
    "output_ids = model.generate(input_ids)\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input Text: {input_text}\")\n",
    "print(f\"Output Text: {output_text}\")\n",
    "print(f\"Embedding Shape: {embeddings.shape}\")  # (batch_size, seq_length, hidden_size)\n",
    "print(input_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeds.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:991: UserWarning: Not enough free disk space to download the file. The expected file size is: 1935.80 MB. The target location C:\\Users\\User\\.cache\\huggingface\\hub\\models--facebook--m2m100_418M\\blobs only has 1017.19 MB free disk space.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "model_name = \"facebook/m2m100_418M\"  # You can also use \"facebook/m2m100_1.2B\" for a larger model\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, src_lang, tgt_lang):\n",
    "    # Set the source language\n",
    "    tokenizer.src_lang = src_lang\n",
    "    # Tokenize the input text\n",
    "    encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Generate translation\n",
    "    print(f\"encoded: {len(encoded.input_ids[0]) }\")\n",
    "    generated_tokens = model.generate(\n",
    "        **encoded,\n",
    "        forced_bos_token_id=tokenizer.get_lang_id(tgt_lang)\n",
    "    )\n",
    "    # Decode the generated tokens\n",
    "    translation = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "    return translation, encoded.input_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: 8\n",
      "Translated Text: Le temps est beau aujourd’hui.\n"
     ]
    }
   ],
   "source": [
    "source_text = \"The weather is beautiful today.\"\n",
    "source_language = \"en\"\n",
    "target_language = \"fr\"\n",
    "\n",
    "translated_text, input_ids = translate(source_text, source_language, target_language)\n",
    "print(f\"Translated Text: {translated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): M2M100ScaledWordEmbedding(128112, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=128112, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embeddings Shape: torch.Size([1, 8, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Access the token embedding layer\n",
    "embed_tokens = model.get_input_embeddings()\n",
    "\n",
    "# Get embeddings for the input IDs\n",
    "embeddings = embed_tokens(input_ids)\n",
    "\n",
    "print(f\"Input Embeddings Shape: {embeddings.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeds = model.model.encoder(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1024])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeds.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6396,  2.0391,  4.0664,  ...,  2.8359, -1.1758,  1.7441],\n",
       "         [-0.1472,  1.4834,  1.0049,  ...,  0.5913, -0.0179,  0.7139],\n",
       "         [-3.8184,  2.4570,  0.9404,  ..., -2.0527, -0.1790, -0.2927],\n",
       "         ...,\n",
       "         [-0.4248, -0.7891,  1.0439,  ..., -0.5562, -0.7646, -1.5479],\n",
       "         [-0.1066,  1.3613,  0.1825,  ...,  0.1327,  0.9346,  0.2668],\n",
       "         [-1.2041, -0.9409, -1.0039,  ..., -0.9634, -1.1104, -0.9634]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-1.2980,  1.6914,  0.6505,  ...,  0.1016,  1.0246,  1.0725],\n",
       "         [-0.6048, -0.0893, -0.7881,  ..., -0.4916, -0.0806,  0.7622],\n",
       "         [-0.8598,  0.4656, -1.0828,  ..., -1.1058,  0.0023,  0.3622],\n",
       "         ...,\n",
       "         [ 0.1949, -0.4941,  0.9724,  ..., -0.3480,  0.6940, -0.2034],\n",
       "         [ 0.1504,  1.1086,  1.0632,  ...,  0.4844, -0.9822,  0.4157],\n",
       "         [ 0.0537,  0.0558, -0.0017,  ...,  0.0103,  0.0032, -0.0272]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
