{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Original: Hello, how are you?\n",
      "Translated: 你好,你好吗?\n",
      "\n",
      "Original: What have you been up to recently?\n",
      "Translated: 你最近做了些什么?\n",
      "\n",
      "Original: Do you want to go for a run?\n",
      "Translated: 你想跑吗?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# Load pre-trained MBART model and tokenizer (MBART-50 for multilingual tasks)\n",
    "model_name = \"../mbart_model\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example batch of input sentences in various languages\n",
    "batch_sentences = [\n",
    "    \"Hello, how are you?\",   # English\n",
    "    \"What have you been up to recently?\", # French\n",
    "    \"Do you want to go for a run?\",    # Spanish\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenize the input batch of sentences\n",
    "inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(f\"input attention mask: {inputs['attention_mask']}\")\n",
    "# Generate translations (for example, to French) or any other target language\n",
    "# Specify the target language for the model to generate in\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "\n",
    "# Perform inference with the model to generate translations\n",
    "outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], \n",
    "                         forced_bos_token_id=forced_bos_token_id)\n",
    "\n",
    "predictions = [] \n",
    "for i in range(len(outputs)): \n",
    "    predictions.append(outputs[i, :])\n",
    "\n",
    "pad_tensor = torch.ones(200-len(predictions[0]))\n",
    "predictions[0] = torch.cat((predictions[0],pad_tensor.long()),dim = 0)\n",
    "predictions = pad_sequence(predictions,batch_first=True,padding_value=1)\n",
    "\n",
    "# Decode the generated outputs back to text\n",
    "translated_sentences = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "# Print the generated translations\n",
    "for i, translation in enumerate(translated_sentences):\n",
    "    print(f\"Original: {batch_sentences[i]}\")\n",
    "    print(f\"Translated: {translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([2, 8])\n",
      "attention_mask shape: torch.Size([2, 8])\n",
      "Logits shape: torch.Size([2, 8, 250027])\n",
      "input_ids shape: torch.Size([2, 8])\n",
      "incorrect_attention_mask shape: torch.Size([2, 7])\n",
      "Error: too many indices for tensor of dimension 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "# Load the mBART model and tokenizer\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\"This is the first sentence.\", \"This is another sentence.\"]\n",
    "\n",
    "# Tokenize the input with padding\n",
    "inputs = tokenizer(sentences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# input_ids shape: (batch_size, sequence_length)\n",
    "# attention_mask shape: (batch_size, sequence_length)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Ensure the shapes are the same\n",
    "print(f\"input_ids shape: {input_ids.shape}\")         # Expected: (batch_size, sequence_length)\n",
    "print(f\"attention_mask shape: {attention_mask.shape}\")  # Expected: (batch_size, sequence_length)\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "\n",
    "# Check model outputs\n",
    "print(f\"Logits shape: {outputs.logits.shape}\")       # Expected: (batch_size, sequence_length, vocab_size)\n",
    "# Create an attention mask with incorrect shape\n",
    "incorrect_attention_mask = attention_mask[:, :-1]  # Removes one token from the attention mask, causing a mismatch\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"input_ids shape: {input_ids.shape}\")                  # (batch_size, sequence_length)\n",
    "print(f\"incorrect_attention_mask shape: {incorrect_attention_mask.shape}\")  # (batch_size, sequence_length - 1)\n",
    "\n",
    "# This will raise an error due to shape mismatch\n",
    "try:\n",
    "    outputs = model(inputs_embeds=input_ids, attention_mask=incorrect_attention_mask, labels=input_ids)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_embeds shape: torch.Size([2, 8, 1024])\n",
      "attention_mask shape: torch.Size([2, 8])\n",
      "Logits shape: torch.Size([2, 8, 250054])\n",
      "inputs_embeds shape: torch.Size([2, 8, 1024])\n",
      "incorrect_attention_mask shape: torch.Size([2, 7])\n",
      "Error: Attention mask should be of size (2, 1, 8, 8), but is torch.Size([2, 1, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBart50Tokenizer, MBartForConditionalGeneration\n",
    "\n",
    "# Load the mBART model and tokenizer\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\"This is the first sentence.\", \"This is another sentence.\"]\n",
    "\n",
    "# Tokenize the input with padding\n",
    "inputs = tokenizer(sentences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# input_ids shape: (batch_size, sequence_length)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Get the input embeddings from the model's embedding layer\n",
    "inputs_embeds = model.model.shared(input_ids)  # Shape: (batch_size, sequence_length, embed_dim)\n",
    "\n",
    "# Ensure the shapes are the same\n",
    "print(f\"inputs_embeds shape: {inputs_embeds.shape}\")         # (batch_size, sequence_length, embed_dim)\n",
    "print(f\"attention_mask shape: {attention_mask.shape}\")       # (batch_size, sequence_length)\n",
    "\n",
    "# Forward pass through the model using inputs_embeds instead of input_ids\n",
    "outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=input_ids)\n",
    "\n",
    "# Check model outputs\n",
    "print(f\"Logits shape: {outputs.logits.shape}\")  # Expected: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "# Create an attention mask with incorrect shape\n",
    "incorrect_attention_mask = attention_mask[:, :-1]  # Removes one token from the attention mask\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"inputs_embeds shape: {inputs_embeds.shape}\")                  # (batch_size, sequence_length, embed_dim)\n",
    "print(f\"incorrect_attention_mask shape: {incorrect_attention_mask.shape}\")  # (batch_size, sequence_length - 1)\n",
    "\n",
    "# This will raise an error due to shape mismatch\n",
    "try:\n",
    "    outputs = model(inputs_embeds=inputs_embeds, attention_mask=incorrect_attention_mask, labels=input_ids)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Accelerator' has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m multiprocessing\u001b[38;5;241m.\u001b[39mset_start_method(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfork\u001b[39m\u001b[38;5;124m'\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m accelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m config \u001b[38;5;241m=\u001b[39m OmegaConf\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/Sign2Text_CSL_config.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m logger \u001b[38;5;241m=\u001b[39m setup_logger(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSign2Text\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_level\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m         output_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./log\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccelerator\u001b[38;5;241m.\u001b[39mprocess_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Accelerator' has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "## Check on dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from train_sign_utils import * \n",
    "from signdata import SignTransDataset\n",
    "import torch\n",
    "import multiprocessing\n",
    "\n",
    "multiprocessing.set_start_method('fork', force=True)\n",
    "accelerator = Accelerator()\n",
    "config = OmegaConf.load(\"configs/Sign2Text_CSL_config.yaml\")\n",
    "logger = setup_logger(name=\"Sign2Text\", log_level=\"INFO\",\n",
    "        output_file=f\"./log{accelerator.process_index}.txt\")\n",
    "trainloader, devloader, testloader = create_signloader(config, logger,accelerator, tokenizer)\n",
    "\n",
    "for i, (src, tgt) in enumerate(tqdm(trainloader, desc=f\"Training!\")):\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']\n",
    "    print(batch.shape)\n",
    "    print(src_length)\n",
    "    print(tgt_attn.shape)\n",
    "    print(tgt_input.shape)\n",
    "    print(input_attn.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([48, 64, 64, 3]), torch.Size([48, 64, 64, 3]), torch.Size([48, 64, 64, 3])]\n",
      "[torch.Size([48, 64, 64, 3]), torch.Size([36, 64, 64, 3]), torch.Size([28, 64, 64, 3])]\n",
      "tmp: tensor([8., 8., 8., 8., 8., 8., 8., 8., 8.])\n",
      "tmp: tensor([8., 8., 8., 8., 8., 8.])\n",
      "tmp: tensor([8., 8., 8., 8.])\n",
      "mask_gen: tensor([[8., 8., 8., 8., 8., 8., 8., 8., 8.],\n",
      "        [8., 8., 8., 8., 8., 8., 0., 0., 0.],\n",
      "        [8., 8., 8., 8., 0., 0., 0., 0., 0.]])\n",
      "Shape of img_batch (stacked videos): torch.Size([112, 64, 64, 3])\n",
      "Shape of src_length_batch: torch.Size([3])\n",
      "Shape of new_src_lengths: torch.Size([3])\n",
      "Shape of img_padding_mask: torch.Size([3, 9])\n",
      "Shape of tgt_input['input_ids']: torch.Size([3, 31])\n",
      "Shape of tgt_input['attention_mask']: torch.Size([3, 31])\n",
      "Image padding masks:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
      "New src lengths:  tensor([9, 6, 4])\n",
      "Test completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "# Sample tokenizer mock (simulates tokenizer behavior)\n",
    "class MockTokenizer:\n",
    "    def as_target_tokenizer(self):\n",
    "        return self\n",
    "\n",
    "    def __call__(self, texts, return_tensors=\"pt\", padding=True, truncation=True):\n",
    "        # Mock tokenization: Returns a tensor with random token IDs and attention masks\n",
    "        max_len = max(len(t) for t in texts)  # Simulate max token length in batch\n",
    "        input_ids = [torch.randint(1, 100, (len(t),)) for t in texts]\n",
    "        input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "        attention_mask = (input_ids_padded != 0).long()\n",
    "        return {\n",
    "            'input_ids': input_ids_padded,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "# Sample collate function (from your code)\n",
    "def collate_fn(batch, tokenizer):\n",
    "    tgt_batch, img_tmp, src_length_batch, name_batch = [], [], [], []\n",
    "\n",
    "    # Separate the video frames and labels\n",
    "    for name_sample, img_sample, tgt_sample in batch:\n",
    "        name_batch.append(name_sample)\n",
    "        img_tmp.append(img_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "\n",
    "    max_len = max([len(vid) for vid in img_tmp])\n",
    "    video_length = torch.LongTensor([int(np.ceil(len(vid) / 4.0) * 4 + 16) for vid in img_tmp])\n",
    "    left_pad = 8\n",
    "    right_pad = int(np.ceil(max_len / 4.0)) * 4 - max_len + 8\n",
    "    max_len = max_len + left_pad + right_pad\n",
    "\n",
    "    padded_video = [torch.cat(\n",
    "        (\n",
    "            vid[0][None].expand(left_pad, -1, -1, -1),  # Padding at the start with the first frame.\n",
    "            vid,\n",
    "            vid[-1][None].expand(max_len - len(vid) - left_pad, -1, -1, -1),  # Padding at the end.\n",
    "        ), dim=0) for vid in img_tmp]\n",
    "    print([padded_video[i].shape for i in range(len(padded_video))])\n",
    "    img_tmp = [padded_video[i][0:video_length[i], :, :, :] for i in range(len(padded_video))]\n",
    "    print([img_tmp[i].shape for i in range(len(img_tmp))])\n",
    "\n",
    "    for i in range(len(img_tmp)):\n",
    "        src_length_batch.append(len(img_tmp[i]))\n",
    "    src_length_batch = torch.tensor(src_length_batch)\n",
    "\n",
    "    img_batch = torch.cat(img_tmp, 0)\n",
    "\n",
    "    new_src_lengths = (((src_length_batch - 5 + 1) / 2) - 5 + 1) / 2\n",
    "    new_src_lengths = new_src_lengths.long()\n",
    "\n",
    "    mask_gen = []\n",
    "    for i in new_src_lengths:\n",
    "        tmp = torch.ones([i]) + 7\n",
    "        print(f\"tmp: {tmp}\")\n",
    "        mask_gen.append(tmp)\n",
    "    mask_gen = pad_sequence(mask_gen, padding_value=0, batch_first=True)\n",
    "    print(f\"mask_gen: {mask_gen}\")\n",
    "    img_padding_mask = (mask_gen != 0).long()\n",
    "\n",
    "    # Tokenize the text labels\n",
    "    tgt_input = tokenizer(tgt_batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Print the shapes of the resulting tensors\n",
    "    print(f\"Shape of img_batch (stacked videos): {img_batch.shape}\")\n",
    "    print(f\"Shape of src_length_batch: {src_length_batch.shape}\")\n",
    "    print(f\"Shape of new_src_lengths: {new_src_lengths.shape}\")\n",
    "    print(f\"Shape of img_padding_mask: {img_padding_mask.shape}\")\n",
    "    print(f\"Shape of tgt_input['input_ids']: {tgt_input['input_ids'].shape}\")\n",
    "    print(f\"Shape of tgt_input['attention_mask']: {tgt_input['attention_mask'].shape}\")\n",
    "    print(\"Image padding masks: \", img_padding_mask)\n",
    "    print(\"New src lengths: \", new_src_lengths)\n",
    "\n",
    "\n",
    "    src_input = {\n",
    "        'input_ids': img_batch,\n",
    "        'attention_mask': img_padding_mask,\n",
    "        'name_batch': name_batch,\n",
    "        'src_length_batch': src_length_batch,\n",
    "        'new_src_length_batch': new_src_lengths\n",
    "    }\n",
    "\n",
    "    return src_input, tgt_input\n",
    "\n",
    "# Test the collate function\n",
    "def test_collate_fn():\n",
    "    # Create mock video data (batch of 3 videos with different lengths)\n",
    "    video1 = torch.randn(30, 64, 64, 3)  # 10 frames\n",
    "    video2 = torch.randn(20, 64, 64, 3)  # 12 frames\n",
    "    video3 = torch.randn(10, 64, 64, 3)   # 8 frames\n",
    "\n",
    "    # Corresponding text labels\n",
    "    labels = [\"sign language translation one\", \"sign language translation two\", \"sign language translation three\"]\n",
    "\n",
    "    # Create a batch (list of tuples: name_sample, video_sample, label)\n",
    "    batch = [\n",
    "        (\"sample1\", video1, labels[0]),\n",
    "        (\"sample2\", video2, labels[1]),\n",
    "        (\"sample3\", video3, labels[2])\n",
    "    ]\n",
    "\n",
    "    # Instantiate the mock tokenizer\n",
    "    tokenizer = MockTokenizer()\n",
    "\n",
    "    # Call the collate function\n",
    "    src_input, tgt_input = collate_fn(batch, tokenizer)\n",
    "\n",
    "    # Check the output shapes\n",
    "    print(\"Test completed.\")\n",
    "    \n",
    "# Run the test case\n",
    "test_collate_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM adaptor 2 shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding: torch.Size([10, 36, 32])\n",
      "After initial projection: torch.Size([10, 36, 512])\n",
      "After permute (before first conv): torch.Size([10, 512, 36])\n",
      "After first conv block: torch.Size([10, 512, 16])\n",
      "After intermediate projection: torch.Size([10, 16, 1024])\n",
      "After permute (before second conv): torch.Size([10, 1024, 16])\n",
      "After second conv block: torch.Size([10, 1024, 6])\n",
      "Final output shape: torch.Size([10, 6, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define a dummy PAD_IDX for padding purposes\n",
    "PAD_IDX = 0\n",
    "\n",
    "class LLMAdapter2(nn.Module):\n",
    "    '''\n",
    "    LLM adapter aims to capture temporal relations and transform 32 tokens into 1024 tokens.\n",
    "    This version introduces an additional projection layer between the two convolution layers.\n",
    "    '''\n",
    "    def __init__(self, num_tokens=32, hidden_dim=1024, kernel_size=5):\n",
    "        super(LLMAdapter2, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.num_tokens = num_tokens\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # First projection from input tokens to hidden_dim/2\n",
    "        self.proj = nn.Linear(self.num_tokens, self.hidden_dim // 2)\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv1d(self.hidden_dim // 2, self.hidden_dim // 2, kernel_size=kernel_size,stride =1, padding=0),\n",
    "            nn.BatchNorm1d(self.hidden_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, ceil_mode=False)\n",
    "        )\n",
    "\n",
    "        # New projection layer between convolution layers\n",
    "        self.intermediate_proj = nn.Linear(self.hidden_dim // 2, self.hidden_dim)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv1d(self.hidden_dim, self.hidden_dim, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, ceil_mode=False)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, src_length):\n",
    "        # Input shape: (batch_size, num_frames, num_tokens)\n",
    "        \n",
    "        # Split the input into individual batches according to src_length\n",
    "        start = 0\n",
    "        x_batch = []\n",
    "        for length in src_length:\n",
    "            end = start + length\n",
    "            x_batch.append(x[start:end])\n",
    "            start = end\n",
    "        \n",
    "        # Pad sequences to ensure uniform batch sizes\n",
    "        x = pad_sequence(x_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "        print(f\"After padding: {x.shape}\")  # Check padding result (batch_size, num_frames, num_tokens)\n",
    "        \n",
    "        # Apply the initial projection layer\n",
    "        x = self.proj(x)  # Shape: (batch_size, num_frames, hidden_dim / 2)\n",
    "        print(f\"After initial projection: {x.shape}\")  # Should be (batch_size, num_frames, 512)\n",
    "        \n",
    "        # Permute to (batch_size, hidden_dim / 2, num_frames) for Conv1d\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After permute (before first conv): {x.shape}\")  # Should be (batch_size, 512, num_frames)\n",
    "        \n",
    "        # First convolutional block\n",
    "        x = self.conv_block_1(x)  # Shape: (batch_size, hidden_dim / 2, reduced_num_frames)\n",
    "        print(f\"After first conv block: {x.shape}\")  # Check after first conv\n",
    "        \n",
    "        # Apply the intermediate projection layer\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch_size, reduced_num_frames, hidden_dim / 2)\n",
    "        x = self.intermediate_proj(x)  # Shape: (batch_size, reduced_num_frames, hidden_dim)\n",
    "        print(f\"After intermediate projection: {x.shape}\")  # Should be (batch_size, reduced_num_frames, 1024)\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch_size, hidden_dim, reduced_num_frames)\n",
    "        print(f\"After permute (before second conv): {x.shape}\")  # Check before second conv\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv_block_2(x)  # Shape: (batch_size, hidden_dim, further_reduced_num_frames)\n",
    "        print(f\"After second conv block: {x.shape}\")  # Check after second conv\n",
    "        \n",
    "        # Convert back to (batch_size, further_reduced_num_frames, hidden_dim)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"Final output shape: {x.shape}\")  # Should be (batch_size, further_reduced_num_frames, 1024)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of LLMAdapter2\n",
    "model = LLMAdapter2()\n",
    "\n",
    "# Test input\n",
    "batch_size = 10\n",
    "num_frames = 36 # Let's assume each sequence has 15 frames\n",
    "num_tokens = 32  # As specified in the model\n",
    "\n",
    "# Random test tensor simulating a batch of 10 sequences, each with 15 frames and 32 tokens\n",
    "test_input = torch.rand((batch_size * num_frames, num_tokens))\n",
    "\n",
    "# Source lengths for each batch (assuming all sequences have 15 frames)\n",
    "src_length = torch.tensor([num_frames] * batch_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(test_input, src_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM adaptor 3 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before padding: torch.Size([480, 32])\n",
      "After padding: torch.Size([10, 48, 32])\n",
      "After permute: torch.Size([10, 32, 48])\n",
      "After temporal_conv: torch.Size([10, 128, 9])\n",
      "After second permute: torch.Size([10, 9, 128])\n",
      "After final_proj: torch.Size([10, 9, 1024])\n",
      "before out shape : torch.Size([10, 9, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define a dummy PAD_IDX for this example\n",
    "PAD_IDX = 0\n",
    "\n",
    "class LLMAdapter3(nn.Module):\n",
    "    def __init__(self, num_tokens=32, hidden_dim=1024, kernel_size=5):\n",
    "        super(LLMAdapter3, self).__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Temporal convolution over the time dimension\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv1d(self.num_tokens, self.num_tokens * 2, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.num_tokens * 2),  # Channels must match Conv1d output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Reduce kernel size for pooling to avoid sequence collapse\n",
    "            nn.AvgPool1d(kernel_size=2, ceil_mode=False),  \n",
    "\n",
    "            nn.Conv1d(self.num_tokens * 2, self.num_tokens * 4, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.num_tokens * 4),  # Channels must match Conv1d output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=2, ceil_mode=False)  # Adjusted pooling to avoid reducing size to zero\n",
    "        )\n",
    "        \n",
    "        # Final projection layer\n",
    "        self.final_proj = nn.Sequential(\n",
    "            nn.Linear(self.num_tokens * 4, self.hidden_dim)\n",
    "        )\n",
    "        self.out = nn.Sequential(nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x, src_length):\n",
    "        start = 0\n",
    "        x_batch = []\n",
    "        for length in src_length:\n",
    "            end = start + length\n",
    "            x_batch.append(x[start:end])\n",
    "            start = end\n",
    "        print(f\"Before padding: {x.shape}\") \n",
    "        x = pad_sequence(x_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "        print(f\"After padding: {x.shape}\")  # Print shape after padding\n",
    "        \n",
    "        # Permute to match Conv1d expected shape: (batch_size, channels, sequence_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After permute: {x.shape}\")  # Shape should now be (batch_size, num_tokens, num_frames)\n",
    "        \n",
    "        # Apply temporal convolution\n",
    "        x = self.temporal_conv(x)\n",
    "        print(f\"After temporal_conv: {x.shape}\")  # Check shape after convolution\n",
    "        \n",
    "        # Permute back to (batch_size, sequence_length, hidden_dim)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After second permute: {x.shape}\")  # Shape should be (batch_size, num_frames, num_tokens*4)\n",
    "        \n",
    "        # Apply final projection (we need to flatten or reshape input to match Linear input requirements)\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        x = self.final_proj(x)\n",
    "        #x = self.final_proj(x.reshape(batch_size * seq_len, hidden_dim))\n",
    "        print(f\"After final_proj: {x.shape}\")  # Check final shape\n",
    "\n",
    "        print(f\"before out shape : {x.shape}\")\n",
    "        x = self.out(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "# Create an instance of LLMAdapter3\n",
    "model = LLMAdapter3()\n",
    "\n",
    "# Test input\n",
    "batch_size = 10\n",
    "num_frames = 48 # Let's assume each sequence has 15 frames\n",
    "num_tokens = 32  # As specified in the model\n",
    "\n",
    "# Random test tensor simulating a batch of 10 sequences, each with 15 frames and 32 tokens\n",
    "test_input = torch.rand((batch_size * num_frames, num_tokens))\n",
    "\n",
    "# Source lengths for each batch (assuming all sequences have 15 frames)\n",
    "src_length = torch.tensor([num_frames] * batch_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(test_input, src_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Mask (src_mask):\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "Target Mask (tgt_mask):\n",
      "tensor([[0., -inf, -inf],\n",
      "        [0., 0., -inf],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Source Padding Mask (src_padding_mask):\n",
      "tensor([[False, False],\n",
      "        [False, False],\n",
      "        [False,  True],\n",
      "        [ True,  True]])\n",
      "\n",
      "Target Padding Mask (tgt_padding_mask):\n",
      "tensor([[False, False],\n",
      "        [False,  True],\n",
      "        [ True,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PAD_IDX = 0  # Assuming PAD_IDX is 0 (the usual padding token in tokenized sequences)\n",
    "\n",
    "# Sample sequences (src and tgt) with padding\n",
    "# Here 1, 2, 3 are tokens, and 0 is the padding token (PAD_IDX)\n",
    "src = torch.tensor([[1, 2, 3, 0], [1, 2, 0, 0]], device=DEVICE)  # shape: (batch_size=2, src_seq_len=4)\n",
    "tgt = torch.tensor([[1, 2, 0], [1, 0, 0]], device=DEVICE)        # shape: (batch_size=2, tgt_seq_len=3)\n",
    "\n",
    "# Function to generate a square subsequent mask\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# Function to create masks for src and tgt sequences\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[1]  # Take the sequence length dimension\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)  # shape: (src_seq_len, batch_size)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)  # shape: (tgt_seq_len, batch_size)\n",
    "    \n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "# Call create_mask function\n",
    "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "# Output the masks to verify them\n",
    "print(\"Source Mask (src_mask):\")\n",
    "print(src_mask)\n",
    "\n",
    "print(\"\\nTarget Mask (tgt_mask):\")\n",
    "print(tgt_mask)\n",
    "\n",
    "print(\"\\nSource Padding Mask (src_padding_mask):\")\n",
    "print(src_padding_mask)\n",
    "\n",
    "print(\"\\nTarget Padding Mask (tgt_padding_mask):\")\n",
    "print(tgt_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
      "        [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n",
      "\n",
      "Reshaped matrix:\n",
      "torch.Size([48])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 4 by 12 matrix\n",
    "matrix = torch.arange(4 * 12).reshape(4, 12)\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "# Reshape the matrix to 6 by 8\n",
    "reshaped_matrix = matrix.reshape(-1)\n",
    "print(\"\\nReshaped matrix:\")\n",
    "print(reshaped_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 50625])\n"
     ]
    }
   ],
   "source": [
    "logits= torch.arange(4 * 12 * 50625).reshape(4,12,50625)\n",
    "logits = logits.reshape(-1,logits.shape[-1])\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
