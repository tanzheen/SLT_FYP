{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, how are you?\n",
      "Translated: 你好,你好吗?\n",
      "\n",
      "Original: What have you been up to recently?\n",
      "Translated: 你最近做了些什么?\n",
      "\n",
      "Original: Do you want to go for a run?\n",
      "Translated: 你想跑吗?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# Load pre-trained MBART model and tokenizer (MBART-50 for multilingual tasks)\n",
    "model_name = \"../mbart_model\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example batch of input sentences in various languages\n",
    "batch_sentences = [\n",
    "    \"Hello, how are you?\",   # English\n",
    "    \"What have you been up to recently?\", # French\n",
    "    \"Do you want to go for a run?\",    # Spanish\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the input batch of sentences\n",
    "inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Generate translations (for example, to French) or any other target language\n",
    "# Specify the target language for the model to generate in\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "\n",
    "# Perform inference with the model to generate translations\n",
    "outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], \n",
    "                         forced_bos_token_id=forced_bos_token_id)\n",
    "\n",
    "predictions = [] \n",
    "for i in range(len(outputs)): \n",
    "    predictions.append(outputs[i, :])\n",
    "\n",
    "pad_tensor = torch.ones(200-len(predictions[0]))\n",
    "predictions[0] = torch.cat((predictions[0],pad_tensor.long()),dim = 0)\n",
    "predictions = pad_sequence(predictions,batch_first=True,padding_value=1)\n",
    "\n",
    "# Decode the generated outputs back to text\n",
    "translated_sentences = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "# Print the generated translations\n",
    "for i, translation in enumerate(translated_sentences):\n",
    "    print(f\"Original: {batch_sentences[i]}\")\n",
    "    print(f\"Translated: {translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM adaptor 2 shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding: torch.Size([10, 50, 32])\n",
      "After initial projection: torch.Size([10, 50, 512])\n",
      "After permute (before first conv): torch.Size([10, 512, 50])\n",
      "After first conv block: torch.Size([10, 512, 23])\n",
      "After intermediate projection: torch.Size([10, 23, 1024])\n",
      "After permute (before second conv): torch.Size([10, 1024, 23])\n",
      "After second conv block: torch.Size([10, 1024, 9])\n",
      "Final output shape: torch.Size([10, 9, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define a dummy PAD_IDX for padding purposes\n",
    "PAD_IDX = 0\n",
    "\n",
    "class LLMAdapter2(nn.Module):\n",
    "    '''\n",
    "    LLM adapter aims to capture temporal relations and transform 32 tokens into 1024 tokens.\n",
    "    This version introduces an additional projection layer between the two convolution layers.\n",
    "    '''\n",
    "    def __init__(self, num_tokens=32, hidden_dim=1024, kernel_size=5):\n",
    "        super(LLMAdapter2, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.num_tokens = num_tokens\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # First projection from input tokens to hidden_dim/2\n",
    "        self.proj = nn.Linear(self.num_tokens, self.hidden_dim // 2)\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv1d(self.hidden_dim // 2, self.hidden_dim // 2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.hidden_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=2, ceil_mode=False)\n",
    "        )\n",
    "\n",
    "        # New projection layer between convolution layers\n",
    "        self.intermediate_proj = nn.Linear(self.hidden_dim // 2, self.hidden_dim)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv1d(self.hidden_dim, self.hidden_dim, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=2, ceil_mode=False)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, src_length):\n",
    "        # Input shape: (batch_size, num_frames, num_tokens)\n",
    "        \n",
    "        # Split the input into individual batches according to src_length\n",
    "        start = 0\n",
    "        x_batch = []\n",
    "        for length in src_length:\n",
    "            end = start + length\n",
    "            x_batch.append(x[start:end])\n",
    "            start = end\n",
    "        \n",
    "        # Pad sequences to ensure uniform batch sizes\n",
    "        x = pad_sequence(x_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "        print(f\"After padding: {x.shape}\")  # Check padding result (batch_size, num_frames, num_tokens)\n",
    "        \n",
    "        # Apply the initial projection layer\n",
    "        x = self.proj(x)  # Shape: (batch_size, num_frames, hidden_dim / 2)\n",
    "        print(f\"After initial projection: {x.shape}\")  # Should be (batch_size, num_frames, 512)\n",
    "        \n",
    "        # Permute to (batch_size, hidden_dim / 2, num_frames) for Conv1d\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After permute (before first conv): {x.shape}\")  # Should be (batch_size, 512, num_frames)\n",
    "        \n",
    "        # First convolutional block\n",
    "        x = self.conv_block_1(x)  # Shape: (batch_size, hidden_dim / 2, reduced_num_frames)\n",
    "        print(f\"After first conv block: {x.shape}\")  # Check after first conv\n",
    "        \n",
    "        # Apply the intermediate projection layer\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch_size, reduced_num_frames, hidden_dim / 2)\n",
    "        x = self.intermediate_proj(x)  # Shape: (batch_size, reduced_num_frames, hidden_dim)\n",
    "        print(f\"After intermediate projection: {x.shape}\")  # Should be (batch_size, reduced_num_frames, 1024)\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch_size, hidden_dim, reduced_num_frames)\n",
    "        print(f\"After permute (before second conv): {x.shape}\")  # Check before second conv\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv_block_2(x)  # Shape: (batch_size, hidden_dim, further_reduced_num_frames)\n",
    "        print(f\"After second conv block: {x.shape}\")  # Check after second conv\n",
    "        \n",
    "        # Convert back to (batch_size, further_reduced_num_frames, hidden_dim)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"Final output shape: {x.shape}\")  # Should be (batch_size, further_reduced_num_frames, 1024)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of LLMAdapter2\n",
    "model = LLMAdapter2()\n",
    "\n",
    "# Test input\n",
    "batch_size = 10\n",
    "num_frames = 50  # Let's assume each sequence has 15 frames\n",
    "num_tokens = 32  # As specified in the model\n",
    "\n",
    "# Random test tensor simulating a batch of 10 sequences, each with 15 frames and 32 tokens\n",
    "test_input = torch.rand((batch_size * num_frames, num_tokens))\n",
    "\n",
    "# Source lengths for each batch (assuming all sequences have 15 frames)\n",
    "src_length = torch.tensor([num_frames] * batch_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(test_input, src_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM adaptor 3 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before padding: torch.Size([500, 32])\n",
      "After padding: torch.Size([10, 50, 32])\n",
      "After permute: torch.Size([10, 32, 50])\n",
      "After temporal_conv: torch.Size([10, 128, 42])\n",
      "After second permute: torch.Size([10, 42, 128])\n",
      "After final_proj: torch.Size([10, 42, 1024])\n",
      "before out shape : torch.Size([10, 42, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define a dummy PAD_IDX for this example\n",
    "PAD_IDX = 0\n",
    "\n",
    "class LLMAdapter3(nn.Module):\n",
    "    def __init__(self, num_tokens=32, hidden_dim=1024, kernel_size=5):\n",
    "        super(LLMAdapter3, self).__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Temporal convolution over the time dimension\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv1d(self.num_tokens, self.num_tokens * 2, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.num_tokens * 2),  # Channels must match Conv1d output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Reduce kernel size for pooling to avoid sequence collapse\n",
    "            nn.AvgPool1d(kernel_size=1, ceil_mode=False),  \n",
    "\n",
    "            nn.Conv1d(self.num_tokens * 2, self.num_tokens * 4, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.num_tokens * 4),  # Channels must match Conv1d output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=1, ceil_mode=False)  # Adjusted pooling to avoid reducing size to zero\n",
    "        )\n",
    "        \n",
    "        # Final projection layer\n",
    "        self.final_proj = nn.Sequential(\n",
    "            nn.Linear(self.num_tokens * 4, self.hidden_dim)\n",
    "        )\n",
    "        self.out = nn.Sequential(nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x, src_length):\n",
    "        start = 0\n",
    "        x_batch = []\n",
    "        for length in src_length:\n",
    "            end = start + length\n",
    "            x_batch.append(x[start:end])\n",
    "            start = end\n",
    "        print(f\"Before padding: {x.shape}\") \n",
    "        x = pad_sequence(x_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "        print(f\"After padding: {x.shape}\")  # Print shape after padding\n",
    "        \n",
    "        # Permute to match Conv1d expected shape: (batch_size, channels, sequence_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After permute: {x.shape}\")  # Shape should now be (batch_size, num_tokens, num_frames)\n",
    "        \n",
    "        # Apply temporal convolution\n",
    "        x = self.temporal_conv(x)\n",
    "        print(f\"After temporal_conv: {x.shape}\")  # Check shape after convolution\n",
    "        \n",
    "        # Permute back to (batch_size, sequence_length, hidden_dim)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After second permute: {x.shape}\")  # Shape should be (batch_size, num_frames, num_tokens*4)\n",
    "        \n",
    "        # Apply final projection (we need to flatten or reshape input to match Linear input requirements)\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        x = self.final_proj(x)\n",
    "        #x = self.final_proj(x.reshape(batch_size * seq_len, hidden_dim))\n",
    "        print(f\"After final_proj: {x.shape}\")  # Check final shape\n",
    "\n",
    "        print(f\"before out shape : {x.shape}\")\n",
    "        x = self.out(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "# Create an instance of LLMAdapter3\n",
    "model = LLMAdapter3()\n",
    "\n",
    "# Test input\n",
    "batch_size = 10\n",
    "num_frames = 50 # Let's assume each sequence has 15 frames\n",
    "num_tokens = 32  # As specified in the model\n",
    "\n",
    "# Random test tensor simulating a batch of 10 sequences, each with 15 frames and 32 tokens\n",
    "test_input = torch.rand((batch_size * num_frames, num_tokens))\n",
    "\n",
    "# Source lengths for each batch (assuming all sequences have 15 frames)\n",
    "src_length = torch.tensor([num_frames] * batch_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(test_input, src_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test some generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention mode is flash\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from omegaconf import OmegaConf\n",
    "import torch.distributed\n",
    "from train_sign_utils import * \n",
    "import os \n",
    "from accelerate import Accelerator\n",
    "from logger import setup_logger\n",
    "from accelerate.utils import set_seed\n",
    "import sys \n",
    "from transformers import MBart50Tokenizer\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/24 14:13:21 Sign2Text]: \u001b[0mCreating model and loss module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_33572\\4201969945.py:5: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titok weights loaded successfully from: TiTok_weights/ema_model/pytorch_model.bin\n",
      "TiTok weights are frozen chowwww!\n",
      "\u001b[32m[10/24 14:13:35 Sign2Text]: \u001b[0mloading weight from ./frozen_sign2text/ema_model/pytorch_model.bin, msg: <All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/24 14:13:37 Sign2Text]: \u001b[0mCreating Signloaders. Batch_size = 2\n",
      "train dataloader done!\n",
      "dev dataloader done!\n",
      "train dataloader done!\n"
     ]
    }
   ],
   "source": [
    "## Take in a configuration \n",
    "import train_sign_utils\n",
    "import signdata\n",
    "import seq_model\n",
    "from imp import reload\n",
    "reload(seq_model)\n",
    "reload(signdata)\n",
    "reload(train_sign_utils)\n",
    "from train_sign_utils import create_model, create_signloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = OmegaConf.load(\"./configs/Sign2Text_CSL_config_v3.yaml\")\n",
    "\n",
    "\n",
    "output_dir = config.experiment.output_dir\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "config.experiment.logging_dir = os.path.join(output_dir, \"logs\")\n",
    "# Load the model \n",
    "accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.training.gradient_accumulation_steps,\n",
    "        mixed_precision=config.training.mixed_precision,\n",
    "        project_dir=config.experiment.logging_dir,\n",
    "        split_batches=False\n",
    "    )\n",
    "\n",
    "\n",
    "logger = setup_logger(name=\"Sign2Text\", log_level=\"INFO\",\n",
    "    output_file=f\"{output_dir}/log{accelerator.process_index}.txt\")\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if config.training.seed is not None:\n",
    "    set_seed(config.training.seed, device_specific=True)\n",
    "\n",
    "# Create model \n",
    "model, ema_model = create_model(config, logger, accelerator)\n",
    "# Create signloaders \n",
    "tokenizer = MBart50Tokenizer.from_pretrained(config.training.tokenizer,\n",
    "                                            src_lang=config.dataset.lang,\n",
    "                                              tgt_lang= config.dataset.lang)\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_signloader(config, logger, accelerator, tokenizer, \"cpu\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_images(model, images, tgt_labels, input_attn, src_length, config, accelerator,  logger, tokenizer): \n",
    "\n",
    "    logger.info(\"Translating images...\")\n",
    "    model = accelerator.unwrap_model(model).to(\"cuda\")\n",
    "    images = torch.clone(images)\n",
    "    \n",
    "    # Set appropriate dtype based on mixed precision\n",
    "    dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "\n",
    "    with torch.no_grad(): \n",
    "\n",
    "        # Directly generate translations using model.generate\n",
    "        output = model.generate(\n",
    "            src_input=images, \n",
    "            src_attn=input_attn, \n",
    "            src_length=src_length,\n",
    "            max_new_tokens=150, \n",
    "            num_beams=4, \n",
    "            decoder_start_token_id=tokenizer.lang_code_to_id[config.dataset.lang]\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Use tokenizer to decode generated token IDs to translations\n",
    "        pred_translations = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "        # Decode the target labels (ground truth)\n",
    "        gt_translations = tokenizer.batch_decode(tgt_labels, skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    return pred_translations, gt_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating!:   0%|          | 0/9201 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape  torch.Size([280, 3, 256, 256])\n",
      "input attn shape torch.Size([2, 37])\n",
      "tgt attn shape torch.Size([2, 18])\n",
      "src length shape torch.Size([2])\n",
      "tgt input shape torch.Size([2, 18])\n",
      "fw_out \n",
      "\u001b[32m[10/24 14:22:22 Sign2Text]: \u001b[0mTranslating images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating!:   0%|          | 0/9201 [00:23<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: ['zh_CN 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。</s>', 'zh_CN 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。</s>']\n",
      "ground truth: ['zh_CN 他不会生气的,我很了解他。</s><pad><pad><pad><pad><pad><pad><pad>', 'zh_CN 动车的车票已经卖完了,只有坐普通车了。</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and dataloader \n",
    "for i, (src, tgt) in enumerate(tqdm(train_dataloader, desc=f\"Generating!\")):\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']\n",
    "\n",
    "    images = batch.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "    #print(f\"imagges type: {images.type()}\")\n",
    "    tgt_input = tgt['input_ids'].to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    input_attn = input_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    tgt_attn = tgt_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    \n",
    "    original_images= torch.clone(images)\n",
    "    print(\"images shape \", original_images.shape)\n",
    "    print(\"input attn shape\", input_attn.shape)\n",
    "    print(\"tgt attn shape\", tgt_attn.shape)\n",
    "    print(\"src length shape\", src_length.shape)\n",
    "    print(\"tgt input shape\", tgt_input.shape)\n",
    "    # Save a batch of translated images to check by reading\n",
    "\n",
    "    fw_out = model( src_input = original_images,tgt_input = tgt_input, src_attn=input_attn, tgt_attn = tgt_attn, src_length = src_length)\n",
    "    print(f\"fw_out \")\n",
    "    pred, gt = translate_images(\n",
    "        model=model,\n",
    "        images=images,\n",
    "        tgt_labels=tgt_input,\n",
    "        input_attn=input_attn, \n",
    "        src_length=src_length,\n",
    "        config=config,\n",
    "        accelerator=accelerator,\n",
    "        logger=logger, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    break \n",
    "\n",
    "\n",
    "print(f\"predictions: {pred}\")\n",
    "print(f\"ground truth: {gt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignModel(\n",
       "  (titok): TiTok(\n",
       "    (encoder): TiTokEncoder(\n",
       "      (patch_embed): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv_out): Conv2d(1024, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (decoder): TiTokDecoder(\n",
       "      (decoder_embed): Linear(in_features=12, out_features=1024, bias=True)\n",
       "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Tanh()\n",
       "        (2): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (conv_out): Identity()\n",
       "    )\n",
       "    (quantize): VectorQuantizer(\n",
       "      (embedding): Embedding(4096, 12)\n",
       "    )\n",
       "  )\n",
       "  (Mbart): MBartForConditionalGeneration(\n",
       "    (model): MBartModel(\n",
       "      (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (encoder): MBartEncoder(\n",
       "        (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "        (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x MBartEncoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): ReLU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): MBartDecoder(\n",
       "        (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "        (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x MBartDecoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       "  )\n",
       "  (adapter): LLMAdapter3(\n",
       "    (temporal_conv): Sequential(\n",
       "      (0): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (4): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
       "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    )\n",
       "    (final_proj): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking LLM adaptation and linking it to gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Reading in the labelled annotations\n",
    "import pickle\n",
    "def read_CSL_annotations(CSL_annot_path):\n",
    "    with open(CSL_annot_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "train_labels = read_CSL_annotations(\"../../CSL-Daily/sentence_label/processed/labels_train.pkl\") \n",
    "dev_labels = read_CSL_annotations(\"../../CSL-Daily/sentence_label/processed/labels_dev.pkl\")\n",
    "test_labels = read_CSL_annotations(\"../../CSL-Daily/sentence_label/processed/labels_test.pkl\")\n",
    "combined_labels = read_CSL_annotations(\"../../CSL-Daily/sentence_label/csl2020ct_v2.pkl\")\n",
    "\n",
    "def find_entry_by_name(data, name):\n",
    "    # Iterate through the list of dictionaries in 'info'\n",
    "    for entry in data['info']:\n",
    "        if entry['name'] == name:\n",
    "            return entry\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_gloss(combined_annotations, name): \n",
    "  # find entry in combined_annotations \n",
    "  entry = find_entry_by_name(combined_labels, name)\n",
    "  gloss_entry = entry['label_gloss']\n",
    "  return gloss_entry\n",
    "\n",
    "\n",
    "def match_frames_w_gloss(tensor_lst, gloss): \n",
    "  # divide the frames into glosses\n",
    "  num_tensors = len(tensor_lst)\n",
    "  num_gloss = len(gloss)\n",
    "  tensors_per_gloss = num_tensors//num_gloss\n",
    "\n",
    "  # create a dictionary to hold the frames referring to a gloss\n",
    "  gloss_dict = {}\n",
    "  for i, g in enumerate(gloss):\n",
    "    if i == num_gloss-1: \n",
    "      gloss_dict[g] = [tensor_lst[i*tensors_per_gloss:]]\n",
    "    else: \n",
    "      gloss_dict[g] = [tensor_lst[i*tensors_per_gloss:(i+1)*tensors_per_gloss]]\n",
    "  \n",
    "  return gloss_dict\n",
    "\n",
    "def find_and_combine_glossdicts(phase, num_samples = None , name_lst= None , dir=\"../../CSL-Daily/sentence/frames_512x512\"):\n",
    "  gloss_dict_lst = {}\n",
    "  ## make assertions to prevent error \n",
    "  assert phase in ['train', 'dev', 'test'], \"Phase must be either train, dev or test\"\n",
    "  assert num_samples is not None or name_lst is not None, \"Either number of samples or name list must be provided\"\n",
    "\n",
    "  if name_lst is None:\n",
    "    video_lst = os.listdir(f\"{dir}/{phase}\")\n",
    "    # Sample random number of videos\n",
    "    name_lst =  random.sample(video_lst, num_samples)\n",
    "\n",
    "  ## Gather various gloss dicts \n",
    "  for name in name_lst: \n",
    "    gloss = get_gloss(combined_labels, name)\n",
    "    tensor_lst = gather_vid_emb(name, phase)\n",
    "    gloss_dict_lst[name] = match_frames_w_gloss(tensor_lst, gloss)\n",
    "\n",
    "  # Comebine gloss dicts \n",
    "  combined_gloss_dict = {}\n",
    "  for name in name_lst: \n",
    "    for k, v in gloss_dict_lst[name].items(): \n",
    "      if k in combined_gloss_dict: \n",
    "        combined_gloss_dict[k].extend(v)\n",
    "      else: \n",
    "        combined_gloss_dict[k] = v\n",
    "  return gloss_dict_lst, combined_gloss_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapting train!:   0%|          | 11/9201 [00:55<11:17:56,  4.43s/it]"
     ]
    }
   ],
   "source": [
    "'''Checking after LLM adaptation'''\n",
    " \n",
    "# Separate TikTok model and LLM adapter for testing\n",
    "titok = model.titok\n",
    "adapter = model.adapter\n",
    "\n",
    "# run the video frames through the TikTok model \n",
    "# Load the dataset and dataloader \n",
    "for i, (src, tgt) in enumerate(tqdm(train_dataloader, desc=f\"adapting train!\")):\n",
    "    phase = \"train\"\n",
    "    titok.to(\"cuda\")\n",
    "    adapter.to(\"cuda\")\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']\n",
    "\n",
    "    images = batch.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "    #print(f\"imagges type: {images.type()}\")\n",
    "    tgt_input = tgt['input_ids'].to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    input_attn = input_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    tgt_attn = tgt_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    \n",
    "    original_images= torch.clone(images)\n",
    "    # print(f\"names: {src['name_batch']}\")\n",
    "    # print(\"images shape \", original_images.shape)\n",
    "    # print(\"input attn shape\", input_attn.shape)\n",
    "    # print(\"tgt attn shape\", tgt_attn.shape)\n",
    "    # print(\"src length shape\", src_length.shape)\n",
    "    # print(\"tgt input shape\", tgt_input.shape)\n",
    "    # Save a batch of translated images to check by reading\n",
    "    encoded_tokens = titok.encode(x=original_images)[1]['min_encoding_indices'].squeeze()\n",
    "    #print(f\"encoded tokens shape: {encoded_tokens.shape}\")\n",
    "    hidden_values = adapter(encoded_tokens.float(), src_length).squeeze()\n",
    "    #print(f\"hidden values shape: {hidden_values.shape}\")\n",
    "\n",
    "    for i,  hid_val in enumerate(hidden_values): \n",
    "        hid_val = hid_val[input_attn[i]==1]\n",
    "        #print(f\"hid val: {hid_val.shape}\")\n",
    "        assert hid_val.shape[0] == src_length[i]\n",
    "\n",
    "        for j, token in enumerate(hid_val): \n",
    "            # save the token as a .pth file \n",
    "            path = os.path.join(config.dataset.img_path,phase )\n",
    "            final_path = os.path.join(path, src['name_batch'][i])\n",
    "            #print(final_path)\n",
    "            torch.save(token, f\"{final_path}/aft_adapter_{j}.pth\")\n",
    "\n",
    "   \n",
    "\n",
    "# Save newly compressed and temporal conv as .pth files and also consider the gloss used for the translation\n",
    "for i, (src, tgt) in enumerate(tqdm(dev_dataloader, desc=f\"adapting dev!\")):\n",
    "    phase = \"dev\"\n",
    "    titok.to(\"cuda\")\n",
    "    adapter.to(\"cuda\")\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']\n",
    "\n",
    "    images = batch.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "    #print(f\"imagges type: {images.type()}\")\n",
    "    tgt_input = tgt['input_ids'].to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    input_attn = input_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    tgt_attn = tgt_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    \n",
    "    original_images= torch.clone(images)\n",
    "    # print(f\"names: {src['name_batch']}\")\n",
    "    # print(\"images shape \", original_images.shape)\n",
    "    # print(\"input attn shape\", input_attn.shape)\n",
    "    # print(\"tgt attn shape\", tgt_attn.shape)\n",
    "    # print(\"src length shape\", src_length.shape)\n",
    "    # print(\"tgt input shape\", tgt_input.shape)\n",
    "    # Save a batch of translated images to check by reading\n",
    "    encoded_tokens = titok.encode(x=original_images)[1]['min_encoding_indices'].squeeze()\n",
    "    #print(f\"encoded tokens shape: {encoded_tokens.shape}\")\n",
    "    hidden_values = adapter(encoded_tokens.float(), src_length).squeeze()\n",
    "    #print(f\"hidden values shape: {hidden_values.shape}\")\n",
    "\n",
    "    for i,  hid_val in enumerate(hidden_values): \n",
    "        hid_val = hid_val[input_attn[i]==1]\n",
    "        print(f\"hid val: {hid_val.shape}\")\n",
    "\n",
    "        for j, token in enumerate(hid_val): \n",
    "            # save the token as a .pth file \n",
    "            path = os.path.join(config.dataset.img_path,phase )\n",
    "            final_path = os.path.join(path, src['name_batch'][i])\n",
    "            #print(final_path)\n",
    "            torch.save(token, f\"{final_path}/aft_adapter_{j}.pth\")\n",
    "\n",
    "   \n",
    "\n",
    "# Save newly compressed and temporal conv as .pth files and also consider the gloss used for the translation\n",
    "for i, (src, tgt) in enumerate(tqdm(test_dataloader, desc=f\"adapting test!\")):\n",
    "    phase = \"test\"\n",
    "    titok.to(\"cuda\")\n",
    "    adapter.to(\"cuda\")\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']\n",
    "\n",
    "    images = batch.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "    #print(f\"imagges type: {images.type()}\")\n",
    "    tgt_input = tgt['input_ids'].to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    input_attn = input_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    tgt_attn = tgt_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    \n",
    "    original_images= torch.clone(images)\n",
    "    # print(f\"names: {src['name_batch']}\")\n",
    "    # print(\"images shape \", original_images.shape)\n",
    "    # print(\"input attn shape\", input_attn.shape)\n",
    "    # print(\"tgt attn shape\", tgt_attn.shape)\n",
    "    # print(\"src length shape\", src_length.shape)\n",
    "    # print(\"tgt input shape\", tgt_input.shape)\n",
    "    # Save a batch of translated images to check by reading\n",
    "    encoded_tokens = titok.encode(x=original_images)[1]['min_encoding_indices'].squeeze()\n",
    "    #print(f\"encoded tokens shape: {encoded_tokens.shape}\")\n",
    "    hidden_values = adapter(encoded_tokens.float(), src_length).squeeze()\n",
    "    #print(f\"hidden values shape: {hidden_values.shape}\")\n",
    "\n",
    "    for i,  hid_val in enumerate(hidden_values): \n",
    "        hid_val = hid_val[input_attn[i]==1]\n",
    "        print(f\"hid val: {hid_val.shape}\")\n",
    "\n",
    "        for j, token in enumerate(hid_val): \n",
    "            # save the token as a .pth file \n",
    "            path = os.path.join(config.dataset.img_path,phase )\n",
    "            final_path = os.path.join(path, src['name_batch'][i])\n",
    "            #print(final_path)\n",
    "            torch.save(token, f\"{final_path}/aft_adapter_{j}.pth\")\n",
    "\n",
    "   \n",
    "\n",
    "# Save newly compressed and temporal conv as .pth files and also consider the gloss used for the translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
