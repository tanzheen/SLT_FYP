{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Original: Hello, how are you?\n",
      "Translated: 你好,你好吗?\n",
      "\n",
      "Original: What have you been up to recently?\n",
      "Translated: 你最近做了些什么?\n",
      "\n",
      "Original: Do you want to go for a run?\n",
      "Translated: 你想跑吗?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# Load pre-trained MBART model and tokenizer (MBART-50 for multilingual tasks)\n",
    "model_name = \"../mbart_model\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example batch of input sentences in various languages\n",
    "batch_sentences = [\n",
    "    \"Hello, how are you?\",   # English\n",
    "    \"What have you been up to recently?\", # French\n",
    "    \"Do you want to go for a run?\",    # Spanish\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenize the input batch of sentences\n",
    "inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "print(f\"input attention mask: {inputs['attention_mask']}\")\n",
    "# Generate translations (for example, to French) or any other target language\n",
    "# Specify the target language for the model to generate in\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "\n",
    "# Perform inference with the model to generate translations\n",
    "outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], \n",
    "                         forced_bos_token_id=forced_bos_token_id)\n",
    "\n",
    "predictions = [] \n",
    "for i in range(len(outputs)): \n",
    "    predictions.append(outputs[i, :])\n",
    "\n",
    "pad_tensor = torch.ones(200-len(predictions[0]))\n",
    "predictions[0] = torch.cat((predictions[0],pad_tensor.long()),dim = 0)\n",
    "predictions = pad_sequence(predictions,batch_first=True,padding_value=1)\n",
    "\n",
    "# Decode the generated outputs back to text\n",
    "translated_sentences = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "# Print the generated translations\n",
    "for i, translation in enumerate(translated_sentences):\n",
    "    print(f\"Original: {batch_sentences[i]}\")\n",
    "    print(f\"Translated: {translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids shape: torch.Size([2, 8])\n",
      "attention_mask shape: torch.Size([2, 8])\n",
      "Logits shape: torch.Size([2, 8, 250027])\n",
      "input_ids shape: torch.Size([2, 8])\n",
      "incorrect_attention_mask shape: torch.Size([2, 7])\n",
      "Error: too many indices for tensor of dimension 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n",
    "\n",
    "# Load the mBART model and tokenizer\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-cc25\")\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\"This is the first sentence.\", \"This is another sentence.\"]\n",
    "\n",
    "# Tokenize the input with padding\n",
    "inputs = tokenizer(sentences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# input_ids shape: (batch_size, sequence_length)\n",
    "# attention_mask shape: (batch_size, sequence_length)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Ensure the shapes are the same\n",
    "print(f\"input_ids shape: {input_ids.shape}\")         # Expected: (batch_size, sequence_length)\n",
    "print(f\"attention_mask shape: {attention_mask.shape}\")  # Expected: (batch_size, sequence_length)\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "\n",
    "# Check model outputs\n",
    "print(f\"Logits shape: {outputs.logits.shape}\")       # Expected: (batch_size, sequence_length, vocab_size)\n",
    "# Create an attention mask with incorrect shape\n",
    "incorrect_attention_mask = attention_mask[:, :-1]  # Removes one token from the attention mask, causing a mismatch\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"input_ids shape: {input_ids.shape}\")                  # (batch_size, sequence_length)\n",
    "print(f\"incorrect_attention_mask shape: {incorrect_attention_mask.shape}\")  # (batch_size, sequence_length - 1)\n",
    "\n",
    "# This will raise an error due to shape mismatch\n",
    "try:\n",
    "    outputs = model(inputs_embeds=input_ids, attention_mask=incorrect_attention_mask, labels=input_ids)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_embeds shape: torch.Size([2, 8, 1024])\n",
      "attention_mask shape: torch.Size([2, 8])\n",
      "Logits shape: torch.Size([2, 8, 250054])\n",
      "inputs_embeds shape: torch.Size([2, 8, 1024])\n",
      "incorrect_attention_mask shape: torch.Size([2, 7])\n",
      "Error: Attention mask should be of size (2, 1, 8, 8), but is torch.Size([2, 1, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBart50Tokenizer, MBartForConditionalGeneration\n",
    "\n",
    "# Load the mBART model and tokenizer\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "\n",
    "# Example sentences\n",
    "sentences = [\"This is the first sentence.\", \"This is another sentence.\"]\n",
    "\n",
    "# Tokenize the input with padding\n",
    "inputs = tokenizer(sentences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# input_ids shape: (batch_size, sequence_length)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Get the input embeddings from the model's embedding layer\n",
    "inputs_embeds = model.model.shared(input_ids)  # Shape: (batch_size, sequence_length, embed_dim)\n",
    "\n",
    "# Ensure the shapes are the same\n",
    "print(f\"inputs_embeds shape: {inputs_embeds.shape}\")         # (batch_size, sequence_length, embed_dim)\n",
    "print(f\"attention_mask shape: {attention_mask.shape}\")       # (batch_size, sequence_length)\n",
    "\n",
    "# Forward pass through the model using inputs_embeds instead of input_ids\n",
    "outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=input_ids)\n",
    "\n",
    "# Check model outputs\n",
    "print(f\"Logits shape: {outputs.logits.shape}\")  # Expected: (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "# Create an attention mask with incorrect shape\n",
    "incorrect_attention_mask = attention_mask[:, :-1]  # Removes one token from the attention mask\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"inputs_embeds shape: {inputs_embeds.shape}\")                  # (batch_size, sequence_length, embed_dim)\n",
    "print(f\"incorrect_attention_mask shape: {incorrect_attention_mask.shape}\")  # (batch_size, sequence_length - 1)\n",
    "\n",
    "# This will raise an error due to shape mismatch\n",
    "try:\n",
    "    outputs = model(inputs_embeds=inputs_embeds, attention_mask=incorrect_attention_mask, labels=input_ids)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ar_AR': 250001, 'cs_CZ': 250002, 'de_DE': 250003, 'en_XX': 250004, 'es_XX': 250005, 'et_EE': 250006, 'fi_FI': 250007, 'fr_XX': 250008, 'gu_IN': 250009, 'hi_IN': 250010, 'it_IT': 250011, 'ja_XX': 250012, 'kk_KZ': 250013, 'ko_KR': 250014, 'lt_LT': 250015, 'lv_LV': 250016, 'my_MM': 250017, 'ne_NP': 250018, 'nl_XX': 250019, 'ro_RO': 250020, 'ru_RU': 250021, 'si_LK': 250022, 'tr_TR': 250023, 'vi_VN': 250024, 'zh_CN': 250025, 'af_ZA': 250026, 'az_AZ': 250027, 'bn_IN': 250028, 'fa_IR': 250029, 'he_IL': 250030, 'hr_HR': 250031, 'id_ID': 250032, 'ka_GE': 250033, 'km_KH': 250034, 'mk_MK': 250035, 'ml_IN': 250036, 'mn_MN': 250037, 'mr_IN': 250038, 'pl_PL': 250039, 'ps_AF': 250040, 'pt_XX': 250041, 'sv_SE': 250042, 'sw_KE': 250043, 'ta_IN': 250044, 'te_IN': 250045, 'th_TH': 250046, 'tl_XX': 250047, 'uk_UA': 250048, 'ur_PK': 250049, 'xh_ZA': 250050, 'gl_ES': 250051, 'sl_SI': 250052}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training!: 100%|██████████| 9201/9201 [2:05:47<00:00,  1.22it/s]  \n"
     ]
    }
   ],
   "source": [
    "## Check on dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from train_sign_utils import * \n",
    "from signdata import SignTransDataset\n",
    "import torch\n",
    "import multiprocessing\n",
    "from  omegaconf import OmegaConf\n",
    "from transformers import MBart50TokenizerFast\n",
    "from accelerate import Accelerator\n",
    "from imp import reload\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import signdata\n",
    "reload(signdata)\n",
    "\n",
    "multiprocessing.set_start_method('fork', force=True)\n",
    "accelerator = Accelerator()\n",
    "config = OmegaConf.load(\"configs/Sign2Text_CSL_config.yaml\")\n",
    "logger = setup_logger(name=\"Sign2Text\", log_level=\"INFO\",\n",
    "        output_file=f\"./log{accelerator.process_index}.txt\")\n",
    "\n",
    "\n",
    "## Testing dataset creation\n",
    "from signdata import SignTransDataset\n",
    "print(tokenizer.lang_code_to_id)\n",
    "# Set the target language\n",
    "tgt_lang = \"zh_CN\"  # For French, replace with the appropriate target language code\n",
    "tokenizer.tgt_lang = tgt_lang\n",
    "train_dataset = SignTransDataset(tokenizer, config,  'train')\n",
    "trainloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "# trainloader, devloader, testloader = create_signloader(config, logger,accelerator, tokenizer)\n",
    "\n",
    "for i, (src, tgt) in enumerate(tqdm(trainloader, desc=f\"Training!\")):\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']\n",
    "    #\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dev!:   0%|          | 0/539 [00:00<?, ?it/s]/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Dev!: 100%|██████████| 539/539 [07:31<00:00,  1.20it/s]\n",
      "Test!: 100%|██████████| 588/588 [08:28<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_dataset = SignTransDataset(tokenizer, config,  'dev')\n",
    "devloader = DataLoader(dev_dataset, batch_size=2, shuffle=True, collate_fn=dev_dataset.collate_fn)\n",
    "for i, (src, tgt) in enumerate(tqdm(devloader, desc=f\"Dev!\")):\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']\n",
    "\n",
    "test_dataset = SignTransDataset(tokenizer, config,  'test')\n",
    "testloader = DataLoader(test_dataset, batch_size=2, shuffle=True, collate_fn=test_dataset.collate_fn)\n",
    "for i, (src, tgt) in enumerate(tqdm(testloader, desc=f\"Test!\")):\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([48, 64, 64, 3]), torch.Size([48, 64, 64, 3]), torch.Size([48, 64, 64, 3])]\n",
      "[torch.Size([48, 64, 64, 3]), torch.Size([36, 64, 64, 3]), torch.Size([28, 64, 64, 3])]\n",
      "tmp: tensor([8., 8., 8., 8., 8., 8., 8., 8., 8.])\n",
      "tmp: tensor([8., 8., 8., 8., 8., 8.])\n",
      "tmp: tensor([8., 8., 8., 8.])\n",
      "mask_gen: tensor([[8., 8., 8., 8., 8., 8., 8., 8., 8.],\n",
      "        [8., 8., 8., 8., 8., 8., 0., 0., 0.],\n",
      "        [8., 8., 8., 8., 0., 0., 0., 0., 0.]])\n",
      "Shape of img_batch (stacked videos): torch.Size([112, 64, 64, 3])\n",
      "Shape of src_length_batch: torch.Size([3])\n",
      "Shape of new_src_lengths: torch.Size([3])\n",
      "Shape of img_padding_mask: torch.Size([3, 9])\n",
      "Shape of tgt_input['input_ids']: torch.Size([3, 31])\n",
      "Shape of tgt_input['attention_mask']: torch.Size([3, 31])\n",
      "Image padding masks:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
      "New src lengths:  tensor([9, 6, 4])\n",
      "Test completed.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "# Sample tokenizer mock (simulates tokenizer behavior)\n",
    "class MockTokenizer:\n",
    "    def as_target_tokenizer(self):\n",
    "        return self\n",
    "\n",
    "    def __call__(self, texts, return_tensors=\"pt\", padding=True, truncation=True):\n",
    "        # Mock tokenization: Returns a tensor with random token IDs and attention masks\n",
    "        max_len = max(len(t) for t in texts)  # Simulate max token length in batch\n",
    "        input_ids = [torch.randint(1, 100, (len(t),)) for t in texts]\n",
    "        input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "        attention_mask = (input_ids_padded != 0).long()\n",
    "        return {\n",
    "            'input_ids': input_ids_padded,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "# Sample collate function (from your code)\n",
    "def collate_fn(batch, tokenizer):\n",
    "    tgt_batch, img_tmp, src_length_batch, name_batch = [], [], [], []\n",
    "\n",
    "    # Separate the video frames and labels\n",
    "    for name_sample, img_sample, tgt_sample in batch:\n",
    "        name_batch.append(name_sample)\n",
    "        img_tmp.append(img_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "\n",
    "    max_len = max([len(vid) for vid in img_tmp])\n",
    "    video_length = torch.LongTensor([int(np.ceil(len(vid) / 4.0) * 4 + 16) for vid in img_tmp])\n",
    "    left_pad = 8\n",
    "    right_pad = int(np.ceil(max_len / 4.0)) * 4 - max_len + 8\n",
    "    max_len = max_len + left_pad + right_pad\n",
    "\n",
    "    padded_video = [torch.cat(\n",
    "        (\n",
    "            vid[0][None].expand(left_pad, -1, -1, -1),  # Padding at the start with the first frame.\n",
    "            vid,\n",
    "            vid[-1][None].expand(max_len - len(vid) - left_pad, -1, -1, -1),  # Padding at the end.\n",
    "        ), dim=0) for vid in img_tmp]\n",
    "    print([padded_video[i].shape for i in range(len(padded_video))])\n",
    "    img_tmp = [padded_video[i][0:video_length[i], :, :, :] for i in range(len(padded_video))]\n",
    "    print([img_tmp[i].shape for i in range(len(img_tmp))])\n",
    "\n",
    "    for i in range(len(img_tmp)):\n",
    "        src_length_batch.append(len(img_tmp[i]))\n",
    "    src_length_batch = torch.tensor(src_length_batch)\n",
    "\n",
    "    img_batch = torch.cat(img_tmp, 0)\n",
    "\n",
    "    new_src_lengths = (((src_length_batch - 5 + 1) / 2) - 5 + 1) / 2\n",
    "    new_src_lengths = new_src_lengths.long()\n",
    "\n",
    "    mask_gen = []\n",
    "    for i in new_src_lengths:\n",
    "        tmp = torch.ones([i]) + 7\n",
    "        print(f\"tmp: {tmp}\")\n",
    "        mask_gen.append(tmp)\n",
    "    mask_gen = pad_sequence(mask_gen, padding_value=0, batch_first=True)\n",
    "    print(f\"mask_gen: {mask_gen}\")\n",
    "    img_padding_mask = (mask_gen != 0).long()\n",
    "\n",
    "    # Tokenize the text labels\n",
    "    tgt_input = tokenizer(tgt_batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Print the shapes of the resulting tensors\n",
    "    print(f\"Shape of img_batch (stacked videos): {img_batch.shape}\")\n",
    "    print(f\"Shape of src_length_batch: {src_length_batch.shape}\")\n",
    "    print(f\"Shape of new_src_lengths: {new_src_lengths.shape}\")\n",
    "    print(f\"Shape of img_padding_mask: {img_padding_mask.shape}\")\n",
    "    print(f\"Shape of tgt_input['input_ids']: {tgt_input['input_ids'].shape}\")\n",
    "    print(f\"Shape of tgt_input['attention_mask']: {tgt_input['attention_mask'].shape}\")\n",
    "    print(\"Image padding masks: \", img_padding_mask)\n",
    "    print(\"New src lengths: \", new_src_lengths)\n",
    "\n",
    "\n",
    "    src_input = {\n",
    "        'input_ids': img_batch,\n",
    "        'attention_mask': img_padding_mask,\n",
    "        'name_batch': name_batch,\n",
    "        'src_length_batch': src_length_batch,\n",
    "        'new_src_length_batch': new_src_lengths\n",
    "    }\n",
    "\n",
    "    return src_input, tgt_input\n",
    "\n",
    "# Test the collate function\n",
    "def test_collate_fn():\n",
    "    # Create mock video data (batch of 3 videos with different lengths)\n",
    "    video1 = torch.randn(30, 64, 64, 3)  # 10 frames\n",
    "    video2 = torch.randn(20, 64, 64, 3)  # 12 frames\n",
    "    video3 = torch.randn(10, 64, 64, 3)   # 8 frames\n",
    "\n",
    "    # Corresponding text labels\n",
    "    labels = [\"sign language translation one\", \"sign language translation two\", \"sign language translation three\"]\n",
    "\n",
    "    # Create a batch (list of tuples: name_sample, video_sample, label)\n",
    "    batch = [\n",
    "        (\"sample1\", video1, labels[0]),\n",
    "        (\"sample2\", video2, labels[1]),\n",
    "        (\"sample3\", video3, labels[2])\n",
    "    ]\n",
    "\n",
    "    # Instantiate the mock tokenizer\n",
    "    tokenizer = MockTokenizer()\n",
    "\n",
    "    # Call the collate function\n",
    "    src_input, tgt_input = collate_fn(batch, tokenizer)\n",
    "\n",
    "    # Check the output shapes\n",
    "    print(\"Test completed.\")\n",
    "    \n",
    "# Run the test case\n",
    "test_collate_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM adaptor 2 shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding: torch.Size([10, 36, 32])\n",
      "After initial projection: torch.Size([10, 36, 512])\n",
      "After permute (before first conv): torch.Size([10, 512, 36])\n",
      "After first conv block: torch.Size([10, 512, 16])\n",
      "After intermediate projection: torch.Size([10, 16, 1024])\n",
      "After permute (before second conv): torch.Size([10, 1024, 16])\n",
      "After second conv block: torch.Size([10, 1024, 6])\n",
      "Final output shape: torch.Size([10, 6, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define a dummy PAD_IDX for padding purposes\n",
    "PAD_IDX = 0\n",
    "\n",
    "class LLMAdapter2(nn.Module):\n",
    "    '''\n",
    "    LLM adapter aims to capture temporal relations and transform 32 tokens into 1024 tokens.\n",
    "    This version introduces an additional projection layer between the two convolution layers.\n",
    "    '''\n",
    "    def __init__(self, num_tokens=32, hidden_dim=1024, kernel_size=5):\n",
    "        super(LLMAdapter2, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.num_tokens = num_tokens\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # First projection from input tokens to hidden_dim/2\n",
    "        self.proj = nn.Linear(self.num_tokens, self.hidden_dim // 2)\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv1d(self.hidden_dim // 2, self.hidden_dim // 2, kernel_size=kernel_size,stride =1, padding=0),\n",
    "            nn.BatchNorm1d(self.hidden_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, ceil_mode=False)\n",
    "        )\n",
    "\n",
    "        # New projection layer between convolution layers\n",
    "        self.intermediate_proj = nn.Linear(self.hidden_dim // 2, self.hidden_dim)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv1d(self.hidden_dim, self.hidden_dim, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2, ceil_mode=False)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, src_length):\n",
    "        # Input shape: (batch_size, num_frames, num_tokens)\n",
    "        \n",
    "        # Split the input into individual batches according to src_length\n",
    "        start = 0\n",
    "        x_batch = []\n",
    "        for length in src_length:\n",
    "            end = start + length\n",
    "            x_batch.append(x[start:end])\n",
    "            start = end\n",
    "        \n",
    "        # Pad sequences to ensure uniform batch sizes\n",
    "        x = pad_sequence(x_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "        print(f\"After padding: {x.shape}\")  # Check padding result (batch_size, num_frames, num_tokens)\n",
    "        \n",
    "        # Apply the initial projection layer\n",
    "        x = self.proj(x)  # Shape: (batch_size, num_frames, hidden_dim / 2)\n",
    "        print(f\"After initial projection: {x.shape}\")  # Should be (batch_size, num_frames, 512)\n",
    "        \n",
    "        # Permute to (batch_size, hidden_dim / 2, num_frames) for Conv1d\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After permute (before first conv): {x.shape}\")  # Should be (batch_size, 512, num_frames)\n",
    "        \n",
    "        # First convolutional block\n",
    "        x = self.conv_block_1(x)  # Shape: (batch_size, hidden_dim / 2, reduced_num_frames)\n",
    "        print(f\"After first conv block: {x.shape}\")  # Check after first conv\n",
    "        \n",
    "        # Apply the intermediate projection layer\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch_size, reduced_num_frames, hidden_dim / 2)\n",
    "        x = self.intermediate_proj(x)  # Shape: (batch_size, reduced_num_frames, hidden_dim)\n",
    "        print(f\"After intermediate projection: {x.shape}\")  # Should be (batch_size, reduced_num_frames, 1024)\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch_size, hidden_dim, reduced_num_frames)\n",
    "        print(f\"After permute (before second conv): {x.shape}\")  # Check before second conv\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv_block_2(x)  # Shape: (batch_size, hidden_dim, further_reduced_num_frames)\n",
    "        print(f\"After second conv block: {x.shape}\")  # Check after second conv\n",
    "        \n",
    "        # Convert back to (batch_size, further_reduced_num_frames, hidden_dim)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"Final output shape: {x.shape}\")  # Should be (batch_size, further_reduced_num_frames, 1024)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of LLMAdapter2\n",
    "model = LLMAdapter2()\n",
    "\n",
    "# Test input\n",
    "batch_size = 10\n",
    "num_frames = 36 # Let's assume each sequence has 15 frames\n",
    "num_tokens = 32  # As specified in the model\n",
    "\n",
    "# Random test tensor simulating a batch of 10 sequences, each with 15 frames and 32 tokens\n",
    "test_input = torch.rand((batch_size * num_frames, num_tokens))\n",
    "\n",
    "# Source lengths for each batch (assuming all sequences have 15 frames)\n",
    "src_length = torch.tensor([num_frames] * batch_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(test_input, src_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM adaptor 3 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before padding: torch.Size([480, 32])\n",
      "After padding: torch.Size([10, 48, 32])\n",
      "After permute: torch.Size([10, 32, 48])\n",
      "After temporal_conv: torch.Size([10, 128, 9])\n",
      "After second permute: torch.Size([10, 9, 128])\n",
      "After final_proj: torch.Size([10, 9, 1024])\n",
      "before out shape : torch.Size([10, 9, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define a dummy PAD_IDX for this example\n",
    "PAD_IDX = 0\n",
    "\n",
    "class LLMAdapter3(nn.Module):\n",
    "    def __init__(self, num_tokens=32, hidden_dim=1024, kernel_size=5):\n",
    "        super(LLMAdapter3, self).__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Temporal convolution over the time dimension\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv1d(self.num_tokens, self.num_tokens * 2, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.num_tokens * 2),  # Channels must match Conv1d output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Reduce kernel size for pooling to avoid sequence collapse\n",
    "            nn.AvgPool1d(kernel_size=2, ceil_mode=False),  \n",
    "\n",
    "            nn.Conv1d(self.num_tokens * 2, self.num_tokens * 4, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.num_tokens * 4),  # Channels must match Conv1d output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=2, ceil_mode=False)  # Adjusted pooling to avoid reducing size to zero\n",
    "        )\n",
    "        \n",
    "        # Final projection layer\n",
    "        self.final_proj = nn.Sequential(\n",
    "            nn.Linear(self.num_tokens * 4, self.hidden_dim)\n",
    "        )\n",
    "        self.out = nn.Sequential(nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x, src_length):\n",
    "        start = 0\n",
    "        x_batch = []\n",
    "        for length in src_length:\n",
    "            end = start + length\n",
    "            x_batch.append(x[start:end])\n",
    "            start = end\n",
    "        print(f\"Before padding: {x.shape}\") \n",
    "        x = pad_sequence(x_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "        print(f\"After padding: {x.shape}\")  # Print shape after padding\n",
    "        \n",
    "        # Permute to match Conv1d expected shape: (batch_size, channels, sequence_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After permute: {x.shape}\")  # Shape should now be (batch_size, num_tokens, num_frames)\n",
    "        \n",
    "        # Apply temporal convolution\n",
    "        x = self.temporal_conv(x)\n",
    "        print(f\"After temporal_conv: {x.shape}\")  # Check shape after convolution\n",
    "        \n",
    "        # Permute back to (batch_size, sequence_length, hidden_dim)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After second permute: {x.shape}\")  # Shape should be (batch_size, num_frames, num_tokens*4)\n",
    "        \n",
    "        # Apply final projection (we need to flatten or reshape input to match Linear input requirements)\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        x = self.final_proj(x)\n",
    "        #x = self.final_proj(x.reshape(batch_size * seq_len, hidden_dim))\n",
    "        print(f\"After final_proj: {x.shape}\")  # Check final shape\n",
    "\n",
    "        print(f\"before out shape : {x.shape}\")\n",
    "        x = self.out(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "# Create an instance of LLMAdapter3\n",
    "model = LLMAdapter3()\n",
    "\n",
    "# Test input\n",
    "batch_size = 10\n",
    "num_frames = 48 # Let's assume each sequence has 15 frames\n",
    "num_tokens = 32  # As specified in the model\n",
    "\n",
    "# Random test tensor simulating a batch of 10 sequences, each with 15 frames and 32 tokens\n",
    "test_input = torch.rand((batch_size * num_frames, num_tokens))\n",
    "\n",
    "# Source lengths for each batch (assuming all sequences have 15 frames)\n",
    "src_length = torch.tensor([num_frames] * batch_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(test_input, src_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test some generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention mode is flash\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from omegaconf import OmegaConf\n",
    "import torch.distributed\n",
    "from train_sign_utils import * \n",
    "import os \n",
    "from accelerate import Accelerator\n",
    "from logger import setup_logger\n",
    "from accelerate.utils import set_seed\n",
    "import sys \n",
    "from transformers import MBart50Tokenizer\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/24 14:13:21 Sign2Text]: \u001b[0mCreating model and loss module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_33572\\4201969945.py:5: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titok weights loaded successfully from: TiTok_weights/ema_model/pytorch_model.bin\n",
      "TiTok weights are frozen chowwww!\n",
      "\u001b[32m[10/24 14:13:35 Sign2Text]: \u001b[0mloading weight from ./frozen_sign2text/ema_model/pytorch_model.bin, msg: <All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/24 14:13:37 Sign2Text]: \u001b[0mCreating Signloaders. Batch_size = 2\n",
      "train dataloader done!\n",
      "dev dataloader done!\n",
      "train dataloader done!\n"
     ]
    }
   ],
   "source": [
    "## Take in a configuration \n",
    "import train_sign_utils\n",
    "import signdata\n",
    "import seq_model\n",
    "from imp import reload\n",
    "reload(seq_model)\n",
    "reload(signdata)\n",
    "reload(train_sign_utils)\n",
    "from train_sign_utils import create_model, create_signloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = OmegaConf.load(\"./configs/Sign2Text_CSL_config_v3.yaml\")\n",
    "\n",
    "\n",
    "output_dir = config.experiment.output_dir\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "config.experiment.logging_dir = os.path.join(output_dir, \"logs\")\n",
    "# Load the model \n",
    "accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.training.gradient_accumulation_steps,\n",
    "        mixed_precision=config.training.mixed_precision,\n",
    "        project_dir=config.experiment.logging_dir,\n",
    "        split_batches=False\n",
    "    )\n",
    "\n",
    "\n",
    "logger = setup_logger(name=\"Sign2Text\", log_level=\"INFO\",\n",
    "    output_file=f\"{output_dir}/log{accelerator.process_index}.txt\")\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if config.training.seed is not None:\n",
    "    set_seed(config.training.seed, device_specific=True)\n",
    "\n",
    "# Create model \n",
    "model, ema_model = create_model(config, logger, accelerator)\n",
    "# Create signloaders \n",
    "tokenizer = MBart50Tokenizer.from_pretrained(config.training.tokenizer,\n",
    "                                            src_lang=config.dataset.lang,\n",
    "                                              tgt_lang= config.dataset.lang)\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_signloader(config, logger, accelerator, tokenizer, \"cpu\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_images(model, images, tgt_labels, input_attn, src_length, config, accelerator,  logger, tokenizer): \n",
    "\n",
    "    logger.info(\"Translating images...\")\n",
    "    model = accelerator.unwrap_model(model).to(\"cuda\")\n",
    "    images = torch.clone(images)\n",
    "    \n",
    "    # Set appropriate dtype based on mixed precision\n",
    "    dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "\n",
    "    with torch.no_grad(): \n",
    "\n",
    "        # Directly generate translations using model.generate\n",
    "        output = model.generate(\n",
    "            src_input=images, \n",
    "            src_attn=input_attn, \n",
    "            src_length=src_length,\n",
    "            max_new_tokens=150, \n",
    "            num_beams=4, \n",
    "            decoder_start_token_id=tokenizer.lang_code_to_id[config.dataset.lang]\n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        # Use tokenizer to decode generated token IDs to translations\n",
    "        pred_translations = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "        # Decode the target labels (ground truth)\n",
    "        gt_translations = tokenizer.batch_decode(tgt_labels, skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    return pred_translations, gt_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating!:   0%|          | 0/9201 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape  torch.Size([280, 3, 256, 256])\n",
      "input attn shape torch.Size([2, 37])\n",
      "tgt attn shape torch.Size([2, 18])\n",
      "src length shape torch.Size([2])\n",
      "tgt input shape torch.Size([2, 18])\n",
      "fw_out \n",
      "\u001b[32m[10/24 14:22:22 Sign2Text]: \u001b[0mTranslating images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating!:   0%|          | 0/9201 [00:23<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: ['zh_CN 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。</s>', 'zh_CN 。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。</s>']\n",
      "ground truth: ['zh_CN 他不会生气的,我很了解他。</s><pad><pad><pad><pad><pad><pad><pad>', 'zh_CN 动车的车票已经卖完了,只有坐普通车了。</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and dataloader \n",
    "for i, (src, tgt) in enumerate(tqdm(train_dataloader, desc=f\"Generating!\")):\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']\n",
    "\n",
    "    images = batch.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "    #print(f\"imagges type: {images.type()}\")\n",
    "    tgt_input = tgt['input_ids'].to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    input_attn = input_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    tgt_attn = tgt_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    \n",
    "    original_images= torch.clone(images)\n",
    "    print(\"images shape \", original_images.shape)\n",
    "    print(\"input attn shape\", input_attn.shape)\n",
    "    print(\"tgt attn shape\", tgt_attn.shape)\n",
    "    print(\"src length shape\", src_length.shape)\n",
    "    print(\"tgt input shape\", tgt_input.shape)\n",
    "    # Save a batch of translated images to check by reading\n",
    "\n",
    "    fw_out = model( src_input = original_images,tgt_input = tgt_input, src_attn=input_attn, tgt_attn = tgt_attn, src_length = src_length)\n",
    "    print(f\"fw_out \")\n",
    "    pred, gt = translate_images(\n",
    "        model=model,\n",
    "        images=images,\n",
    "        tgt_labels=tgt_input,\n",
    "        input_attn=input_attn, \n",
    "        src_length=src_length,\n",
    "        config=config,\n",
    "        accelerator=accelerator,\n",
    "        logger=logger, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    break \n",
    "\n",
    "\n",
    "print(f\"predictions: {pred}\")\n",
    "print(f\"ground truth: {gt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignModel(\n",
       "  (titok): TiTok(\n",
       "    (encoder): TiTokEncoder(\n",
       "      (patch_embed): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv_out): Conv2d(1024, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (decoder): TiTokDecoder(\n",
       "      (decoder_embed): Linear(in_features=12, out_features=1024, bias=True)\n",
       "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Tanh()\n",
       "        (2): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (conv_out): Identity()\n",
       "    )\n",
       "    (quantize): VectorQuantizer(\n",
       "      (embedding): Embedding(4096, 12)\n",
       "    )\n",
       "  )\n",
       "  (Mbart): MBartForConditionalGeneration(\n",
       "    (model): MBartModel(\n",
       "      (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (encoder): MBartEncoder(\n",
       "        (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "        (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x MBartEncoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): ReLU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): MBartDecoder(\n",
       "        (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "        (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x MBartDecoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       "  )\n",
       "  (adapter): LLMAdapter3(\n",
       "    (temporal_conv): Sequential(\n",
       "      (0): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (4): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
       "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    )\n",
       "    (final_proj): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Checking after LLM adaptation'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Mask (src_mask):\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n",
      "\n",
      "Target Mask (tgt_mask):\n",
      "tensor([[0., -inf, -inf],\n",
      "        [0., 0., -inf],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Source Padding Mask (src_padding_mask):\n",
      "tensor([[False, False],\n",
      "        [False, False],\n",
      "        [False,  True],\n",
      "        [ True,  True]])\n",
      "\n",
      "Target Padding Mask (tgt_padding_mask):\n",
      "tensor([[False, False],\n",
      "        [False,  True],\n",
      "        [ True,  True]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define constants\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PAD_IDX = 0  # Assuming PAD_IDX is 0 (the usual padding token in tokenized sequences)\n",
    "\n",
    "# Sample sequences (src and tgt) with padding\n",
    "# Here 1, 2, 3 are tokens, and 0 is the padding token (PAD_IDX)\n",
    "src = torch.tensor([[1, 2, 3, 0], [1, 2, 0, 0]], device=DEVICE)  # shape: (batch_size=2, src_seq_len=4)\n",
    "tgt = torch.tensor([[1, 2, 0], [1, 0, 0]], device=DEVICE)        # shape: (batch_size=2, tgt_seq_len=3)\n",
    "\n",
    "# Function to generate a square subsequent mask\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# Function to create masks for src and tgt sequences\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[1]  # Take the sequence length dimension\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)  # shape: (src_seq_len, batch_size)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)  # shape: (tgt_seq_len, batch_size)\n",
    "    \n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "# Call create_mask function\n",
    "src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "# Output the masks to verify them\n",
    "print(\"Source Mask (src_mask):\")\n",
    "print(src_mask)\n",
    "\n",
    "print(\"\\nTarget Mask (tgt_mask):\")\n",
    "print(tgt_mask)\n",
    "\n",
    "print(\"\\nSource Padding Mask (src_padding_mask):\")\n",
    "print(src_padding_mask)\n",
    "\n",
    "print(\"\\nTarget Padding Mask (tgt_padding_mask):\")\n",
    "print(tgt_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
      "        [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]])\n",
      "\n",
      "Reshaped matrix:\n",
      "torch.Size([48])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 4 by 12 matrix\n",
    "matrix = torch.arange(4 * 12).reshape(4, 12)\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "\n",
    "# Reshape the matrix to 6 by 8\n",
    "reshaped_matrix = matrix.reshape(-1)\n",
    "print(\"\\nReshaped matrix:\")\n",
    "print(reshaped_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48, 50625])\n"
     ]
    }
   ],
   "source": [
    "logits= torch.arange(4 * 12 * 50625).reshape(4,12,50625)\n",
    "logits = logits.reshape(-1,logits.shape[-1])\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
