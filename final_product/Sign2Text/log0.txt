[32m[09/20 18:38:08 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/20 18:38:08 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.1
  save_every: 0.0001
  log_every: 0.01
  eval_every: 1
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/20 18:38:08 Sign2Text]: [0mCreating model and loss module.
[32m[09/20 18:39:08 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/20 18:39:08 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.1
  save_every: 0.0001
  log_every: 0.01
  eval_every: 1
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/20 18:39:08 Sign2Text]: [0mCreating model and loss module.
[32m[09/20 18:42:17 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/20 18:42:17 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.1
  save_every: 0.0001
  log_every: 0.01
  eval_every: 1
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/20 18:42:17 Sign2Text]: [0mCreating model and loss module.
[32m[09/20 18:43:44 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/20 18:43:44 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.1
  save_every: 0.0001
  log_every: 0.01
  eval_every: 1
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/20 18:43:44 Sign2Text]: [0mCreating model and loss module.
[32m[09/20 18:43:50 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/20 18:44:36 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/20 18:44:36 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.1
  save_every: 0.0001
  log_every: 0.01
  eval_every: 1
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/20 18:44:36 Sign2Text]: [0mCreating model and loss module.
[32m[09/20 18:44:43 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/20 18:44:43 Sign2Text]: [0mCreating optimizers.
[32m[09/20 18:44:43 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/20 18:44:43 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/20 18:44:43 Sign2Text]: [0mAll globbed checkpoints are: []
[32m[09/20 18:44:43 Sign2Text]: [0mTraining from scratch.
[32m[09/20 23:33:54 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/20 23:33:54 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.1
  save_every: 0.0001
  log_every: 0.01
  eval_every: 1
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/20 23:33:54 Sign2Text]: [0mCreating model and loss module.
[32m[09/20 23:34:10 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/20 23:34:10 Sign2Text]: [0mCreating optimizers.
[32m[09/20 23:34:10 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/20 23:34:10 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/20 23:34:10 Sign2Text]: [0mAll globbed checkpoints are: []
[32m[09/20 23:34:10 Sign2Text]: [0mTraining from scratch.
[32m[09/20 23:38:44 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/20 23:38:44 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.1
  save_every: 0.0001
  log_every: 0.01
  eval_every: 1
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/20 23:38:44 Sign2Text]: [0mCreating model and loss module.
[32m[09/20 23:38:50 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/20 23:38:50 Sign2Text]: [0mCreating optimizers.
[32m[09/20 23:38:50 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/20 23:38:50 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/20 23:38:51 Sign2Text]: [0mAll globbed checkpoints are: []
[32m[09/20 23:38:51 Sign2Text]: [0mTraining from scratch.
[32m[09/20 23:40:19 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/20 23:40:19 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.1
  save_every: 0.0001
  log_every: 0.01
  eval_every: 1
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/20 23:40:19 Sign2Text]: [0mCreating model and loss module.
[32m[09/20 23:40:25 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/20 23:40:25 Sign2Text]: [0mCreating optimizers.
[32m[09/20 23:40:25 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/20 23:40:25 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/20 23:40:25 Sign2Text]: [0mAll globbed checkpoints are: []
[32m[09/20 23:40:25 Sign2Text]: [0mTraining from scratch.
[32m[09/20 23:42:40 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/20 23:42:40 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.1
  save_every: 0.1
  log_every: 0.1
  eval_every: 1
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/20 23:42:40 Sign2Text]: [0mCreating model and loss module.
[32m[09/20 23:42:47 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/20 23:42:47 Sign2Text]: [0mCreating optimizers.
[32m[09/20 23:42:47 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/20 23:42:47 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/20 23:42:47 Sign2Text]: [0mAll globbed checkpoints are: []
[32m[09/20 23:42:47 Sign2Text]: [0mTraining from scratch.
[32m[09/20 23:44:11 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/20 23:44:11 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.1
  save_every: 0.1
  log_every: 0.1
  eval_every: 1
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/20 23:44:11 Sign2Text]: [0mCreating model and loss module.
[32m[09/20 23:44:18 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/20 23:44:18 Sign2Text]: [0mCreating optimizers.
[32m[09/20 23:44:18 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/20 23:44:18 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/20 23:44:18 Sign2Text]: [0mAll globbed checkpoints are: []
[32m[09/20 23:44:18 Sign2Text]: [0mTraining from scratch.
[32m[09/23 10:37:56 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 10:37:56 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 10:37:56 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 10:38:11 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 10:38:11 Sign2Text]: [0mCreating optimizers.
[32m[09/23 10:38:11 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 10:38:11 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 10:38:12 Sign2Text]: [0mAll globbed checkpoints are: []
[32m[09/23 10:38:12 Sign2Text]: [0mTraining from scratch.
[32m[09/23 10:38:46 Sign2Text]: [0mData (t): 0.2783, 1.08/s/gpu Batch (t): 3.7104 LR: 0.000000 Step: 4 Total Loss: 74.2898 Recon Loss: 17.1627 
[32m[09/23 10:38:51 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-4
[32m[09/23 10:39:07 Sign2Text]: [0mTranslating images...
[32m[09/23 10:39:58 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 10:39:58 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.1
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 10:39:58 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 10:40:04 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 10:40:04 Sign2Text]: [0mCreating optimizers.
[32m[09/23 10:40:04 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 10:40:04 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 10:40:05 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-4']
[32m[09/23 10:40:05 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-4
[32m[09/23 10:40:14 Sign2Text]: [0mResuming at global_step 4
[32m[09/23 10:40:54 Sign2Text]: [0mData (t): 0.3000, 0.54/s/gpu Batch (t): 7.3575 LR: 0.000000 Step: 8 Total Loss: 73.7952 Recon Loss: 17.2953 
[32m[09/23 10:40:59 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-8
[32m[09/23 10:44:27 Sign2Text]: [0mTranslating images...
[32m[09/23 10:46:09 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 10:46:09 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 10:46:09 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 10:46:15 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 10:46:15 Sign2Text]: [0mCreating optimizers.
[32m[09/23 10:46:15 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 10:46:15 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 10:46:16 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-4', 'Sign2Text\\checkpoint-8']
[32m[09/23 10:46:16 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-8
[32m[09/23 10:46:28 Sign2Text]: [0mResuming at global_step 8
[32m[09/23 10:47:44 Sign2Text]: [0mData (t): 0.7915, 0.94/s/gpu Batch (t): 4.2621 LR: 0.000000 Step: 12 Total Loss: 73.0066 Recon Loss: 17.4144 
[32m[09/23 10:48:23 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-12
[32m[09/23 10:51:29 Sign2Text]: [0mTranslating images...
[32m[09/23 10:53:48 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 10:53:48 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 10:53:48 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 10:53:54 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 10:53:54 Sign2Text]: [0mCreating optimizers.
[32m[09/23 10:53:54 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 10:53:54 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 10:53:55 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-12', 'Sign2Text\\checkpoint-4', 'Sign2Text\\checkpoint-8']
[32m[09/23 10:53:55 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-12
[32m[09/23 10:54:07 Sign2Text]: [0mResuming at global_step 12
[32m[09/23 10:54:47 Sign2Text]: [0mData (t): 0.3463, 0.92/s/gpu Batch (t): 4.3604 LR: 0.000000 Step: 16 Total Loss: 74.0327 Recon Loss: 17.0340 
[32m[09/23 10:54:53 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-16
[32m[09/23 11:00:55 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 11:00:55 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 11:00:55 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 11:01:01 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 11:01:01 Sign2Text]: [0mCreating optimizers.
[32m[09/23 11:01:01 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 11:01:01 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 11:01:02 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-12', 'Sign2Text\\checkpoint-16', 'Sign2Text\\checkpoint-4', 'Sign2Text\\checkpoint-8']
[32m[09/23 11:01:02 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-16
[32m[09/23 11:01:51 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 11:01:51 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 11:01:51 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 11:01:56 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 11:01:56 Sign2Text]: [0mCreating optimizers.
[32m[09/23 11:01:56 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 11:01:56 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 11:01:57 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-12', 'Sign2Text\\checkpoint-4', 'Sign2Text\\checkpoint-8']
[32m[09/23 11:01:57 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-12
[32m[09/23 11:02:09 Sign2Text]: [0mResuming at global_step 12
[32m[09/23 11:02:43 Sign2Text]: [0mData (t): 0.3332, 1.04/s/gpu Batch (t): 3.8378 LR: 0.000000 Step: 16 Total Loss: 74.0327 Recon Loss: 17.0340 
[32m[09/23 11:02:59 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-16
[32m[09/23 11:03:27 Sign2Text]: [0mTranslating images...
[32m[09/23 11:05:13 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 11:05:13 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 11:05:13 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 11:05:19 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 11:05:19 Sign2Text]: [0mCreating optimizers.
[32m[09/23 11:05:19 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 11:05:19 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 11:05:20 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-12', 'Sign2Text\\checkpoint-16', 'Sign2Text\\checkpoint-4', 'Sign2Text\\checkpoint-8']
[32m[09/23 11:05:20 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-16
[32m[09/23 11:05:33 Sign2Text]: [0mResuming at global_step 16
[32m[09/23 11:06:06 Sign2Text]: [0mData (t): 0.3614, 1.04/s/gpu Batch (t): 3.8569 LR: 0.000000 Step: 20 Total Loss: 73.9311 Recon Loss: 17.1105 
[32m[09/23 11:06:49 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-20
[32m[09/23 11:10:04 Sign2Text]: [0mTranslating images...
[32m[09/23 11:16:07 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 11:16:07 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 11:16:07 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 11:16:14 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 11:16:14 Sign2Text]: [0mCreating optimizers.
[32m[09/23 11:16:14 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 11:16:14 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 11:16:14 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-12', 'Sign2Text\\checkpoint-16', 'Sign2Text\\checkpoint-20', 'Sign2Text\\checkpoint-4', 'Sign2Text\\checkpoint-8']
[32m[09/23 11:16:14 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-20
[32m[09/23 11:16:27 Sign2Text]: [0mResuming at global_step 20
[32m[09/23 11:17:01 Sign2Text]: [0mData (t): 0.7672, 0.82/s/gpu Batch (t): 4.8701 LR: 0.000000 Step: 24 Total Loss: 74.0626 Recon Loss: 17.6188 
[32m[09/23 11:17:07 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-24
[32m[09/23 11:19:46 Sign2Text]: [0mTranslating images...
[32m[09/23 11:22:05 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 11:22:05 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 11:22:05 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 11:22:11 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 11:22:11 Sign2Text]: [0mCreating optimizers.
[32m[09/23 11:22:11 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 11:22:11 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 11:22:12 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-12', 'Sign2Text\\checkpoint-16', 'Sign2Text\\checkpoint-20', 'Sign2Text\\checkpoint-24', 'Sign2Text\\checkpoint-4', 'Sign2Text\\checkpoint-8']
[32m[09/23 11:22:12 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-24
[32m[09/23 11:22:24 Sign2Text]: [0mResuming at global_step 24
[32m[09/23 11:22:58 Sign2Text]: [0mData (t): 0.3894, 0.93/s/gpu Batch (t): 4.2955 LR: 0.000000 Step: 28 Total Loss: 74.9515 Recon Loss: 18.6622 
[32m[09/23 11:23:24 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-28
[32m[09/23 11:26:05 Sign2Text]: [0mTranslating images...
[32m[09/23 11:26:31 Sign2Text]: [0mComputing metrics on the validation set.
[32m[09/23 11:29:37 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 11:29:37 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 11:29:37 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 11:29:43 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 11:29:43 Sign2Text]: [0mCreating optimizers.
[32m[09/23 11:29:43 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 11:29:43 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 11:29:44 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-12', 'Sign2Text\\checkpoint-16', 'Sign2Text\\checkpoint-20', 'Sign2Text\\checkpoint-24', 'Sign2Text\\checkpoint-28', 'Sign2Text\\checkpoint-4', 'Sign2Text\\checkpoint-8']
[32m[09/23 11:29:44 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-28
[32m[09/23 11:29:56 Sign2Text]: [0mResuming at global_step 28
[32m[09/23 11:30:33 Sign2Text]: [0mData (t): 0.4504, 0.91/s/gpu Batch (t): 4.4048 LR: 0.000000 Step: 32 Total Loss: 73.4274 Recon Loss: 17.2492 
[32m[09/23 11:30:39 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-32
[32m[09/23 11:31:58 Sign2Text]: [0mTranslating images...
[32m[09/23 11:32:25 Sign2Text]: [0mComputing metrics on the validation set.
[32m[09/23 11:34:21 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 11:34:21 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 11:34:21 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 11:34:27 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 11:34:27 Sign2Text]: [0mCreating optimizers.
[32m[09/23 11:34:27 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 11:34:27 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 11:34:28 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-12', 'Sign2Text\\checkpoint-16', 'Sign2Text\\checkpoint-20', 'Sign2Text\\checkpoint-24', 'Sign2Text\\checkpoint-28', 'Sign2Text\\checkpoint-32', 'Sign2Text\\checkpoint-4', 'Sign2Text\\checkpoint-8']
[32m[09/23 11:34:28 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-32
[32m[09/23 11:34:41 Sign2Text]: [0mResuming at global_step 32
[32m[09/23 11:35:15 Sign2Text]: [0mData (t): 0.3724, 1.03/s/gpu Batch (t): 3.8670 LR: 0.000000 Step: 36 Total Loss: 75.2392 Recon Loss: 18.2693 
[32m[09/23 11:35:44 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-36
[32m[09/23 11:39:22 Sign2Text]: [0mTranslating images...
[32m[09/23 11:39:50 Sign2Text]: [0mComputing metrics on the validation set.
[32m[09/23 11:42:28 Sign2Text]: [0mSaving config to Sign2Text\config.yaml
[32m[09/23 11:42:28 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[09/23 11:42:28 Sign2Text]: [0mCreating model and loss module.
[32m[09/23 11:42:34 Sign2Text]: [0mCreating Signloaders. Batch_size = 4
[32m[09/23 11:42:34 Sign2Text]: [0mCreating optimizers.
[32m[09/23 11:42:34 Sign2Text]: [0mCreating lr_schedulers.
[32m[09/23 11:42:34 Sign2Text]: [0mPreparing model, optimizer and dataloaders
[32m[09/23 11:42:35 Sign2Text]: [0mAll globbed checkpoints are: ['Sign2Text\\checkpoint-12', 'Sign2Text\\checkpoint-16', 'Sign2Text\\checkpoint-20', 'Sign2Text\\checkpoint-24', 'Sign2Text\\checkpoint-28', 'Sign2Text\\checkpoint-32', 'Sign2Text\\checkpoint-36', 'Sign2Text\\checkpoint-4', 'Sign2Text\\checkpoint-8']
[32m[09/23 11:42:35 Sign2Text]: [0mLoad checkpoint from Sign2Text\checkpoint-36
[32m[09/23 11:42:47 Sign2Text]: [0mResuming at global_step 36
[32m[09/23 11:43:19 Sign2Text]: [0mData (t): 0.3603, 1.05/s/gpu Batch (t): 3.8226 LR: 0.000000 Step: 40 Total Loss: 74.5069 Recon Loss: 17.6848 
[32m[09/23 11:43:25 Sign2Text]: [0mSaved state to Sign2Text\checkpoint-40
[32m[09/23 11:44:58 Sign2Text]: [0mTranslating images...
[32m[09/23 11:45:23 Sign2Text]: [0mComputing metrics on the validation set.
[32m[10/08 09:55:10 Sign2Text]: [0mSaving config to Sign2Text/config.yaml
[32m[10/08 09:55:10 Sign2Text]: [0mConfig:
experiment:
  project: Sign2Text
  name: Sign2Text_run1
  output_dir: Sign2Text
  translate_every: 0.001
  save_every: 0.001
  log_every: 0.001
  eval_every: 0.001
  log_grad_norm_every: 0.001
  resume: true
  init_weight: ''
  logging_dir: Sign2Text/logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: large
    vit_dec_model_size: large
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 32
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
    init_weight: TiTok_weights/ema_model/pytorch_model.bin
  MBart_model:
    init_weight: facebook/mbart-large-50
losses:
  quantizer_weight: 1.0
dataset:
  name: CSL-Daily
  lang: zh_CN
  train: ../../CSL-Daily/sentence_label/processed/labels_train.pkl
  dev: ../../CSL-Daily/sentence_label/processed/labels_dev.pkl
  test: ../../CSL-Daily/sentence_label/processed/labels_test.pkl
  img_path: ../../CSL-Daily/sentence/frames_512x512/
  max_length: 300
  params:
    num_workers: 4
  preprocessing:
    crop_size: 256
    resize_shorter_edge: 256
    random_crop: true
    random_flip: true
    person_size: 410
optimizer:
  name: adamw
  params:
    learning_rate: 1.0e-05
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-06
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 4
  mixed_precision: 'no'
  enable_tf32: true
  enable_wandb: false
  use_ema: true
  seed: 42
  num_translated_images: 4
  max_grad_norm: 1.0
  num_epochs: 10
  frame_every: 5
  tokenizer: facebook/mbart-large-50
config: configs/Sign2Text_CSL_config.yaml
--experiment:
  project: Sign2Text_CSL
  name: Sign2Text_CSL_run1
  output_dir: Sign2Text_CSL_run1

[32m[10/08 09:55:10 Sign2Text]: [0mCreating model and loss module.
