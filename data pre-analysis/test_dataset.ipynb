{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *torch\n",
    "from pickletools import optimize\n",
    "# from sched import scheduler\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim import lr_scheduler as scheduler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# *transformers\n",
    "from transformers import MBartForConditionalGeneration, MBartTokenizer, MBartConfig\n",
    "\n",
    "\n",
    "import utils as utils\n",
    "\n",
    "\n",
    "# *basic\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import json, datetime\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import random\n",
    "import wandb\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import math\n",
    "import sys\n",
    "from typing import Iterable, Optional\n",
    "from loguru import logger\n",
    "\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER\n",
    "\n",
    "# *timm\n",
    "from timm.optim import create_optimizer\n",
    "from timm.scheduler import create_scheduler\n",
    "from timm.utils import NativeScaler\n",
    "from timm.loss import SoftTargetCrossEntropy\n",
    "from timm.optim import AdamW\n",
    "\n",
    "# visualization\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from PIL import Image\n",
    "import argparse\n",
    "from hpman.m import _\n",
    "import hpargparse\n",
    "\n",
    "# global definition\n",
    "from definition import *\n",
    "\n",
    "import gzip\n",
    "import pickle\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_args_parser' from 'prep_args' (e:\\SLT_FYP\\data pre-analysis\\prep_args.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_36072\\1650815014.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprep_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mprep_args\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_args_parser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Visual-Language-Pretraining (VLP) V2 scripts'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mget_args_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhpargparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'get_args_parser' from 'prep_args' (e:\\SLT_FYP\\data pre-analysis\\prep_args.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "from prep_args import * \n",
    "parser = argparse.ArgumentParser('Visual-Language-Pretraining (VLP) V2 scripts', parents=[get_args_parser()])\n",
    "\n",
    "hpargparse.bind(parser, _)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'GFSLT-VLP CSL-Daily',\n",
       " 'data': {'train': '../../CSL-Daily/sentence_label/processed/labels_train.pkl',\n",
       "  'dev': '../../CSL-Daily/sentence_label/processed/labels_dev.pkl',\n",
       "  'test': '../../CSL-Daily/sentence_label/processed/labels_test.pkl',\n",
       "  'img_path': '../../CSL-Daily/sentence/frames_512x512',\n",
       "  'max_length': 300},\n",
       " 'training': {'wandb': 'disabled', 'scale_embedding': False},\n",
       " 'model': {'transformer': './pretrain_models/CSL/MBart_trimmed',\n",
       "  'visual_encoder': './pretrain_models/CSL/mytran',\n",
       "  'sign_proj': True}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(args.config, 'r+', encoding='utf-8') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[250004,      6, 100013, 101676,  27683,   1344,     30,      2,      1],\n",
      "        [250004,      6,   8513,  83757, 101676,  27683,   1344,     30,      2]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBart50Tokenizer\n",
    "\n",
    "# Load the MBART tokenizer\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "# Example: Chinese sentences from the CSL-daily dataset\n",
    "texts = [\"这是一个例子句子。\", \"这是另一个例子句子。\"]\n",
    "\n",
    "# Tokenize the Chinese text data\n",
    "tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input_ids = tokenized_texts['input_ids']\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data_classes\n",
    "importlib.reload(data_classes)\n",
    "from data_classes import * \n",
    "\n",
    "train_dataset = S2T_Dataset(tokenizer, config, args, 'train', training_refurbish = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000000.jpg', '000001.jpg', '000002.jpg', '000003.jpg', '000004.jpg', '000005.jpg', '000006.jpg', '000007.jpg', '000008.jpg', '000009.jpg', '000010.jpg', '000011.jpg', '000012.jpg', '000013.jpg', '000014.jpg', '000015.jpg', '000016.jpg', '000017.jpg', '000018.jpg', '000019.jpg', '000020.jpg', '000021.jpg', '000022.jpg', '000023.jpg', '000024.jpg', '000025.jpg', '000026.jpg', '000027.jpg', '000028.jpg', '000029.jpg', '000030.jpg', '000031.jpg', '000032.jpg', '000033.jpg', '000034.jpg', '000035.jpg', '000036.jpg', '000037.jpg', '000038.jpg', '000039.jpg', '000040.jpg', '000041.jpg', '000042.jpg', '000043.jpg', '000044.jpg', '000045.jpg', '000046.jpg', '000047.jpg', '000048.jpg', '000049.jpg', '000050.jpg', '000051.jpg']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([52, 3, 224, 224])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
