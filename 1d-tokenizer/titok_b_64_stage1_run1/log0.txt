[32m[08/30 23:44:30 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[08/30 23:44:30 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 32
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 1000000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[08/30 23:44:31 TiTok]: [0mCreating model and loss module.
[32m[08/30 23:44:33 TiTok]: [0mCreating optimizers.
[32m[08/30 23:44:33 TiTok]: [0mCreating lr_schedulers.
[32m[08/30 23:44:33 TiTok]: [0mCreating dataloaders.
[32m[08/30 23:44:54 TiTok]: [0mCreating evaluator.
[32m[08/30 23:44:55 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[08/30 23:44:55 TiTok]: [0m***** Running training *****
[32m[08/30 23:44:55 TiTok]: [0m  Num training steps = 1000000
[32m[08/30 23:44:55 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[08/30 23:44:55 TiTok]: [0m  Instantaneous batch size per gpu = 32
[32m[08/30 23:44:55 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 32
[32m[08/30 23:44:55 TiTok]: [0mAll globbed checkpoints are: []
[32m[08/30 23:44:55 TiTok]: [0mTraining from scratch.
[32m[08/30 23:45:31 TiTok]: [0mData (t): 0.1200, 47.76/s/gpu Batch (t): 0.6700 LR: 0.000001 Step: 50 Total Loss: 6.8883 Recon Loss: 6.8770 
[32m[08/30 23:46:04 TiTok]: [0mData (t): 0.1210, 47.48/s/gpu Batch (t): 0.6740 LR: 0.000001 Step: 100 Total Loss: 6.6194 Recon Loss: 6.6190 
[32m[08/30 23:46:38 TiTok]: [0mData (t): 0.1200, 47.34/s/gpu Batch (t): 0.6760 LR: 0.000002 Step: 150 Total Loss: 6.4207 Recon Loss: 6.4206 
[32m[08/30 23:47:12 TiTok]: [0mData (t): 0.1190, 47.31/s/gpu Batch (t): 0.6764 LR: 0.000002 Step: 200 Total Loss: 6.2963 Recon Loss: 6.2962 
[32m[08/30 23:47:46 TiTok]: [0mData (t): 0.1200, 47.25/s/gpu Batch (t): 0.6772 LR: 0.000003 Step: 250 Total Loss: 6.1127 Recon Loss: 6.1125 
[32m[08/30 23:48:27 TiTok]: [0mData (t): 0.2744, 38.58/s/gpu Batch (t): 0.8294 LR: 0.000003 Step: 300 Total Loss: 5.9900 Recon Loss: 5.9896 
[32m[08/30 23:49:08 TiTok]: [0mData (t): 0.2740, 38.55/s/gpu Batch (t): 0.8300 LR: 0.000004 Step: 350 Total Loss: 5.9373 Recon Loss: 5.9369 
[32m[08/30 23:49:50 TiTok]: [0mData (t): 0.2744, 38.63/s/gpu Batch (t): 0.8284 LR: 0.000004 Step: 400 Total Loss: 5.8134 Recon Loss: 5.8130 
[32m[08/30 23:50:31 TiTok]: [0mData (t): 0.2784, 38.49/s/gpu Batch (t): 0.8315 LR: 0.000005 Step: 450 Total Loss: 5.6332 Recon Loss: 5.6329 
[32m[08/30 23:51:13 TiTok]: [0mData (t): 0.2730, 38.69/s/gpu Batch (t): 0.8270 LR: 0.000005 Step: 500 Total Loss: 5.6083 Recon Loss: 5.6081 
[32m[08/30 23:51:54 TiTok]: [0mData (t): 0.2708, 38.77/s/gpu Batch (t): 0.8254 LR: 0.000006 Step: 550 Total Loss: 5.4941 Recon Loss: 5.4935 
[32m[08/30 23:52:36 TiTok]: [0mData (t): 0.2735, 38.52/s/gpu Batch (t): 0.8306 LR: 0.000006 Step: 600 Total Loss: 5.4010 Recon Loss: 5.4004 
[32m[08/30 23:53:17 TiTok]: [0mData (t): 0.2704, 38.73/s/gpu Batch (t): 0.8263 LR: 0.000007 Step: 650 Total Loss: 5.0321 Recon Loss: 5.0314 
[32m[08/30 23:53:59 TiTok]: [0mData (t): 0.2700, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000007 Step: 700 Total Loss: 5.1442 Recon Loss: 5.1439 
[32m[08/30 23:54:40 TiTok]: [0mData (t): 0.2764, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000008 Step: 750 Total Loss: 5.1355 Recon Loss: 5.1351 
[32m[08/30 23:55:21 TiTok]: [0mData (t): 0.2729, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000008 Step: 800 Total Loss: 5.1114 Recon Loss: 5.1109 
[32m[08/30 23:56:02 TiTok]: [0mData (t): 0.2710, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000009 Step: 850 Total Loss: 5.0809 Recon Loss: 5.0805 
[32m[08/30 23:56:42 TiTok]: [0mData (t): 0.2703, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000009 Step: 900 Total Loss: 5.1613 Recon Loss: 5.1607 
[32m[08/30 23:57:23 TiTok]: [0mData (t): 0.2692, 39.59/s/gpu Batch (t): 0.8082 LR: 0.000010 Step: 950 Total Loss: 4.9408 Recon Loss: 4.9400 
[32m[08/30 23:58:04 TiTok]: [0mData (t): 0.2738, 35.13/s/gpu Batch (t): 0.9108 LR: 0.000010 Step: 1000 Total Loss: 4.8555 Recon Loss: 4.8548 
[32m[08/30 23:58:44 TiTok]: [0mData (t): 0.2721, 39.51/s/gpu Batch (t): 0.8098 LR: 0.000010 Step: 1050 Total Loss: 5.0722 Recon Loss: 5.0713 
[32m[08/30 23:59:25 TiTok]: [0mData (t): 0.2687, 39.69/s/gpu Batch (t): 0.8063 LR: 0.000011 Step: 1100 Total Loss: 4.8034 Recon Loss: 4.8026 
[32m[08/31 00:00:05 TiTok]: [0mData (t): 0.2880, 38.69/s/gpu Batch (t): 0.8270 LR: 0.000012 Step: 1150 Total Loss: 4.9761 Recon Loss: 4.9719 
[32m[08/31 00:00:46 TiTok]: [0mData (t): 0.2741, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000012 Step: 1200 Total Loss: 5.0008 Recon Loss: 4.9965 
[32m[08/31 00:01:27 TiTok]: [0mData (t): 0.2690, 39.60/s/gpu Batch (t): 0.8080 LR: 0.000013 Step: 1250 Total Loss: 4.8244 Recon Loss: 4.8230 
[32m[08/31 00:02:07 TiTok]: [0mData (t): 0.2724, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000013 Step: 1300 Total Loss: 4.8912 Recon Loss: 4.8893 
[32m[08/31 00:02:48 TiTok]: [0mData (t): 0.2753, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000014 Step: 1350 Total Loss: 4.8369 Recon Loss: 4.8352 
[32m[08/31 00:03:29 TiTok]: [0mData (t): 0.2735, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000014 Step: 1400 Total Loss: 4.8430 Recon Loss: 4.8413 
[32m[08/31 00:04:09 TiTok]: [0mData (t): 0.2707, 39.52/s/gpu Batch (t): 0.8097 LR: 0.000015 Step: 1450 Total Loss: 4.7546 Recon Loss: 4.7509 
[32m[08/31 00:04:50 TiTok]: [0mData (t): 0.2760, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000015 Step: 1500 Total Loss: 4.7216 Recon Loss: 4.7186 
[32m[08/31 00:05:31 TiTok]: [0mData (t): 0.2733, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000016 Step: 1550 Total Loss: 4.8248 Recon Loss: 4.8214 
[32m[08/31 00:06:11 TiTok]: [0mData (t): 0.2701, 39.50/s/gpu Batch (t): 0.8101 LR: 0.000016 Step: 1600 Total Loss: 4.7920 Recon Loss: 4.7895 
[32m[08/31 00:06:52 TiTok]: [0mData (t): 0.3133, 37.50/s/gpu Batch (t): 0.8533 LR: 0.000017 Step: 1650 Total Loss: 4.7772 Recon Loss: 4.7714 
[32m[08/31 00:07:33 TiTok]: [0mData (t): 0.3111, 37.60/s/gpu Batch (t): 0.8511 LR: 0.000017 Step: 1700 Total Loss: 4.7854 Recon Loss: 4.7822 
[32m[08/31 00:08:13 TiTok]: [0mData (t): 0.2769, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000017 Step: 1750 Total Loss: 4.5961 Recon Loss: 4.5926 
[32m[08/31 00:08:54 TiTok]: [0mData (t): 0.2800, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000018 Step: 1800 Total Loss: 4.4215 Recon Loss: 4.4185 
[32m[08/31 00:09:35 TiTok]: [0mData (t): 0.2795, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000018 Step: 1850 Total Loss: 4.5954 Recon Loss: 4.5919 
[32m[08/31 00:10:15 TiTok]: [0mData (t): 0.2736, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000019 Step: 1900 Total Loss: 4.5616 Recon Loss: 4.5579 
[32m[08/31 00:10:56 TiTok]: [0mData (t): 0.2720, 39.51/s/gpu Batch (t): 0.8100 LR: 0.000020 Step: 1950 Total Loss: 4.3984 Recon Loss: 4.3953 
[32m[08/31 00:11:37 TiTok]: [0mData (t): 0.2700, 35.24/s/gpu Batch (t): 0.9080 LR: 0.000020 Step: 2000 Total Loss: 4.3773 Recon Loss: 4.3741 
[32m[08/31 00:12:18 TiTok]: [0mData (t): 0.2770, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000021 Step: 2050 Total Loss: 4.4059 Recon Loss: 4.4028 
[32m[08/31 00:12:58 TiTok]: [0mData (t): 0.2790, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000021 Step: 2100 Total Loss: 4.3087 Recon Loss: 4.3055 
[32m[08/31 00:13:39 TiTok]: [0mData (t): 0.2720, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000022 Step: 2150 Total Loss: 4.4833 Recon Loss: 4.4800 
[32m[08/31 00:14:19 TiTok]: [0mData (t): 0.2725, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000022 Step: 2200 Total Loss: 4.3037 Recon Loss: 4.3007 
[32m[08/31 00:15:00 TiTok]: [0mData (t): 0.2759, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000023 Step: 2250 Total Loss: 4.4673 Recon Loss: 4.4637 
[32m[08/31 00:15:41 TiTok]: [0mData (t): 0.2735, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000023 Step: 2300 Total Loss: 4.2264 Recon Loss: 4.2232 
[32m[08/31 00:16:21 TiTok]: [0mData (t): 0.2726, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000023 Step: 2350 Total Loss: 4.4437 Recon Loss: 4.4403 
[32m[08/31 00:17:02 TiTok]: [0mData (t): 0.2684, 39.58/s/gpu Batch (t): 0.8084 LR: 0.000024 Step: 2400 Total Loss: 4.5096 Recon Loss: 4.5061 
[32m[08/31 00:17:43 TiTok]: [0mData (t): 0.2803, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000024 Step: 2450 Total Loss: 4.1603 Recon Loss: 4.1568 
[32m[08/31 00:18:23 TiTok]: [0mData (t): 0.2772, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000025 Step: 2500 Total Loss: 4.1798 Recon Loss: 4.1756 
[32m[08/31 00:19:04 TiTok]: [0mData (t): 0.2670, 39.76/s/gpu Batch (t): 0.8048 LR: 0.000026 Step: 2550 Total Loss: 4.2712 Recon Loss: 4.2675 
[32m[08/31 00:19:44 TiTok]: [0mData (t): 0.2660, 39.71/s/gpu Batch (t): 0.8059 LR: 0.000026 Step: 2600 Total Loss: 4.1655 Recon Loss: 4.1618 
[32m[08/31 00:20:25 TiTok]: [0mData (t): 0.2680, 39.65/s/gpu Batch (t): 0.8070 LR: 0.000027 Step: 2650 Total Loss: 4.2022 Recon Loss: 4.1988 
[32m[08/31 00:21:05 TiTok]: [0mData (t): 0.2740, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000027 Step: 2700 Total Loss: 4.0842 Recon Loss: 4.0807 
[32m[08/31 00:21:46 TiTok]: [0mData (t): 0.2713, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000028 Step: 2750 Total Loss: 4.2205 Recon Loss: 4.2167 
[32m[08/31 00:22:27 TiTok]: [0mData (t): 0.2710, 39.51/s/gpu Batch (t): 0.8100 LR: 0.000028 Step: 2800 Total Loss: 3.8478 Recon Loss: 3.8446 
[32m[08/31 00:23:07 TiTok]: [0mData (t): 0.2705, 39.53/s/gpu Batch (t): 0.8095 LR: 0.000028 Step: 2850 Total Loss: 4.0040 Recon Loss: 4.0006 
[32m[08/31 00:23:48 TiTok]: [0mData (t): 0.2740, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000029 Step: 2900 Total Loss: 4.0151 Recon Loss: 4.0119 
[32m[08/31 00:24:29 TiTok]: [0mData (t): 0.2870, 38.69/s/gpu Batch (t): 0.8270 LR: 0.000029 Step: 2950 Total Loss: 4.0436 Recon Loss: 4.0400 
[32m[08/31 00:25:10 TiTok]: [0mData (t): 0.2697, 35.16/s/gpu Batch (t): 0.9102 LR: 0.000030 Step: 3000 Total Loss: 4.1222 Recon Loss: 4.1182 
[32m[08/31 00:25:50 TiTok]: [0mData (t): 0.2870, 38.65/s/gpu Batch (t): 0.8280 LR: 0.000030 Step: 3050 Total Loss: 4.0020 Recon Loss: 3.9985 
[32m[08/31 00:26:31 TiTok]: [0mData (t): 0.2750, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000031 Step: 3100 Total Loss: 4.0245 Recon Loss: 4.0211 
[32m[08/31 00:27:12 TiTok]: [0mData (t): 0.2710, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000031 Step: 3150 Total Loss: 4.0903 Recon Loss: 4.0866 
[32m[08/31 00:27:52 TiTok]: [0mData (t): 0.2719, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000032 Step: 3200 Total Loss: 3.9210 Recon Loss: 3.9177 
[32m[08/31 00:28:33 TiTok]: [0mData (t): 0.2684, 39.63/s/gpu Batch (t): 0.8074 LR: 0.000033 Step: 3250 Total Loss: 3.9170 Recon Loss: 3.9137 
[32m[08/31 00:29:14 TiTok]: [0mData (t): 0.2684, 39.62/s/gpu Batch (t): 0.8078 LR: 0.000033 Step: 3300 Total Loss: 3.9258 Recon Loss: 3.9222 
[32m[08/31 00:29:54 TiTok]: [0mData (t): 0.2752, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000034 Step: 3350 Total Loss: 3.9459 Recon Loss: 3.9423 
[32m[08/31 00:30:35 TiTok]: [0mData (t): 0.2740, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000034 Step: 3400 Total Loss: 3.8643 Recon Loss: 3.8611 
[32m[08/31 00:31:15 TiTok]: [0mData (t): 0.2680, 39.65/s/gpu Batch (t): 0.8070 LR: 0.000034 Step: 3450 Total Loss: 3.9645 Recon Loss: 3.9609 
[32m[08/31 00:31:56 TiTok]: [0mData (t): 0.2743, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000035 Step: 3500 Total Loss: 3.9005 Recon Loss: 3.8972 
[32m[08/31 00:32:37 TiTok]: [0mData (t): 0.2780, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000036 Step: 3550 Total Loss: 3.8987 Recon Loss: 3.8952 
[32m[08/31 00:33:17 TiTok]: [0mData (t): 0.2717, 39.47/s/gpu Batch (t): 0.8107 LR: 0.000036 Step: 3600 Total Loss: 3.8152 Recon Loss: 3.8121 
[32m[08/31 00:33:58 TiTok]: [0mData (t): 0.2720, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000036 Step: 3650 Total Loss: 3.8942 Recon Loss: 3.8909 
[32m[08/31 00:34:38 TiTok]: [0mData (t): 0.2709, 39.55/s/gpu Batch (t): 0.8091 LR: 0.000037 Step: 3700 Total Loss: 3.9207 Recon Loss: 3.9171 
[32m[08/31 00:35:19 TiTok]: [0mData (t): 0.2815, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000038 Step: 3750 Total Loss: 3.9202 Recon Loss: 3.9167 
[32m[08/31 00:36:00 TiTok]: [0mData (t): 0.2725, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000038 Step: 3800 Total Loss: 3.6666 Recon Loss: 3.6634 
[32m[08/31 00:36:40 TiTok]: [0mData (t): 0.2709, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000039 Step: 3850 Total Loss: 3.6934 Recon Loss: 3.6901 
[32m[08/31 00:37:21 TiTok]: [0mData (t): 0.2722, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000039 Step: 3900 Total Loss: 3.6338 Recon Loss: 3.6306 
[32m[08/31 00:38:02 TiTok]: [0mData (t): 0.2653, 39.79/s/gpu Batch (t): 0.8043 LR: 0.000040 Step: 3950 Total Loss: 3.8093 Recon Loss: 3.8059 
[32m[08/31 00:38:42 TiTok]: [0mData (t): 0.2768, 34.99/s/gpu Batch (t): 0.9145 LR: 0.000040 Step: 4000 Total Loss: 3.8772 Recon Loss: 3.8736 
[32m[08/31 00:39:23 TiTok]: [0mData (t): 0.2693, 39.59/s/gpu Batch (t): 0.8083 LR: 0.000041 Step: 4050 Total Loss: 3.7811 Recon Loss: 3.7777 
[32m[08/31 00:40:03 TiTok]: [0mData (t): 0.2680, 39.55/s/gpu Batch (t): 0.8090 LR: 0.000041 Step: 4100 Total Loss: 3.7208 Recon Loss: 3.7176 
[32m[08/31 00:40:44 TiTok]: [0mData (t): 0.2699, 39.51/s/gpu Batch (t): 0.8099 LR: 0.000041 Step: 4150 Total Loss: 3.6910 Recon Loss: 3.6877 
[32m[08/31 00:41:25 TiTok]: [0mData (t): 0.2722, 39.49/s/gpu Batch (t): 0.8102 LR: 0.000042 Step: 4200 Total Loss: 3.7648 Recon Loss: 3.7614 
[32m[08/31 00:42:05 TiTok]: [0mData (t): 0.2698, 39.66/s/gpu Batch (t): 0.8068 LR: 0.000043 Step: 4250 Total Loss: 3.6354 Recon Loss: 3.6318 
[32m[08/31 00:42:46 TiTok]: [0mData (t): 0.2714, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000043 Step: 4300 Total Loss: 3.6500 Recon Loss: 3.6466 
[32m[08/31 00:43:26 TiTok]: [0mData (t): 0.2710, 39.51/s/gpu Batch (t): 0.8100 LR: 0.000044 Step: 4350 Total Loss: 3.7646 Recon Loss: 3.7612 
[32m[08/31 00:44:07 TiTok]: [0mData (t): 0.2765, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000044 Step: 4400 Total Loss: 3.6113 Recon Loss: 3.6080 
[32m[08/31 00:44:48 TiTok]: [0mData (t): 0.2730, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000045 Step: 4450 Total Loss: 3.7337 Recon Loss: 3.7304 
[32m[08/31 00:45:28 TiTok]: [0mData (t): 0.2710, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000045 Step: 4500 Total Loss: 3.6235 Recon Loss: 3.6202 
[32m[08/31 00:46:09 TiTok]: [0mData (t): 0.2680, 39.62/s/gpu Batch (t): 0.8077 LR: 0.000046 Step: 4550 Total Loss: 3.8015 Recon Loss: 3.7982 
[32m[08/31 00:46:50 TiTok]: [0mData (t): 0.2766, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000046 Step: 4600 Total Loss: 3.5408 Recon Loss: 3.5376 
[32m[08/31 00:47:30 TiTok]: [0mData (t): 0.2912, 38.54/s/gpu Batch (t): 0.8304 LR: 0.000047 Step: 4650 Total Loss: 3.6840 Recon Loss: 3.6806 
[32m[08/31 00:48:11 TiTok]: [0mData (t): 0.2740, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000047 Step: 4700 Total Loss: 3.6313 Recon Loss: 3.6280 
[32m[08/31 00:48:52 TiTok]: [0mData (t): 0.2689, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000048 Step: 4750 Total Loss: 3.6733 Recon Loss: 3.6701 
[32m[08/31 00:49:33 TiTok]: [0mData (t): 0.2828, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000048 Step: 4800 Total Loss: 3.6911 Recon Loss: 3.6877 
[32m[08/31 00:50:14 TiTok]: [0mData (t): 0.2770, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000048 Step: 4850 Total Loss: 3.4737 Recon Loss: 3.4704 
[32m[08/31 00:50:55 TiTok]: [0mData (t): 0.3242, 37.03/s/gpu Batch (t): 0.8642 LR: 0.000049 Step: 4900 Total Loss: 3.4242 Recon Loss: 3.4211 
[32m[08/31 00:51:36 TiTok]: [0mData (t): 0.3363, 36.52/s/gpu Batch (t): 0.8763 LR: 0.000050 Step: 4950 Total Loss: 3.6498 Recon Loss: 3.6467 
[32m[08/31 00:52:17 TiTok]: [0mData (t): 0.2840, 34.67/s/gpu Batch (t): 0.9230 LR: 0.000050 Step: 5000 Total Loss: 3.6242 Recon Loss: 3.6211 
