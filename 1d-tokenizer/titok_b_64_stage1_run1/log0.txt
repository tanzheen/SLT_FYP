[32m[09/01 20:15:03 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 20:15:03 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 128
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 1000000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 20:15:03 TiTok]: [0mCreating model and loss module.
[32m[09/01 20:15:04 TiTok]: [0mCreating optimizers.
[32m[09/01 20:15:04 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 20:15:04 TiTok]: [0mCreating dataloaders. Batch size = 128
[32m[09/01 20:15:26 TiTok]: [0mCreating evaluator.
[32m[09/01 20:15:26 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 20:15:26 TiTok]: [0m***** Running training *****
[32m[09/01 20:15:26 TiTok]: [0m  Num training steps = 1000000
[32m[09/01 20:15:26 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 20:15:26 TiTok]: [0mmixed precision = fp16
[32m[09/01 20:15:26 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 128
[32m[09/01 20:15:26 TiTok]: [0maccelerator device: cuda
[32m[09/01 20:15:26 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 20:15:26 TiTok]: [0mTraining from scratch.
[32m[09/01 20:16:23 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 20:16:23 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 128
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 1000000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 20:16:23 TiTok]: [0mCreating model and loss module.
[32m[09/01 20:16:24 TiTok]: [0mCreating optimizers.
[32m[09/01 20:16:24 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 20:16:24 TiTok]: [0mCreating dataloaders. Batch size = 128
[32m[09/01 20:16:46 TiTok]: [0mCreating evaluator.
[32m[09/01 20:16:46 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 20:16:46 TiTok]: [0m***** Running training *****
[32m[09/01 20:16:46 TiTok]: [0m  Num training steps = 1000000
[32m[09/01 20:16:46 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 20:16:46 TiTok]: [0mmixed precision = fp16
[32m[09/01 20:16:46 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 128
[32m[09/01 20:16:46 TiTok]: [0maccelerator device: cuda
[32m[09/01 20:16:46 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 20:16:46 TiTok]: [0mTraining from scratch.
[32m[09/01 20:21:43 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 20:21:43 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 128
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 1000000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 20:21:43 TiTok]: [0mCreating model and loss module.
[32m[09/01 20:21:44 TiTok]: [0mCreating optimizers.
[32m[09/01 20:21:44 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 20:21:44 TiTok]: [0mCreating dataloaders. Batch size = 128
[32m[09/01 20:22:05 TiTok]: [0mCreating evaluator.
[32m[09/01 20:22:05 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 20:22:05 TiTok]: [0m***** Running training *****
[32m[09/01 20:22:05 TiTok]: [0m  Num training steps = 1000000
[32m[09/01 20:22:05 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 20:22:05 TiTok]: [0mmixed precision = fp16
[32m[09/01 20:22:05 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 128
[32m[09/01 20:22:05 TiTok]: [0maccelerator device: cuda
[32m[09/01 20:22:05 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 20:22:05 TiTok]: [0mTraining from scratch.
[32m[09/01 20:26:07 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 20:26:07 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 128
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 1000000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 20:26:08 TiTok]: [0mCreating model and loss module.
[32m[09/01 20:26:08 TiTok]: [0mCreating optimizers.
[32m[09/01 20:26:08 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 20:26:08 TiTok]: [0mCreating dataloaders. Batch size = 128
[32m[09/01 20:26:30 TiTok]: [0mCreating evaluator.
[32m[09/01 20:26:30 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 20:26:30 TiTok]: [0m***** Running training *****
[32m[09/01 20:26:30 TiTok]: [0m  Num training steps = 1000000
[32m[09/01 20:26:30 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 20:26:30 TiTok]: [0mmixed precision = fp16
[32m[09/01 20:26:30 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 128
[32m[09/01 20:26:30 TiTok]: [0maccelerator device: cuda
[32m[09/01 20:26:30 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 20:26:30 TiTok]: [0mTraining from scratch.
[32m[09/01 20:32:41 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 20:32:41 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 128
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 1000000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 20:32:42 TiTok]: [0mCreating model and loss module.
[32m[09/01 20:32:43 TiTok]: [0mCreating optimizers.
[32m[09/01 20:32:43 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 20:32:43 TiTok]: [0mCreating dataloaders. Batch size = 128
[32m[09/01 20:33:29 TiTok]: [0mCreating evaluator.
[32m[09/01 20:33:30 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 20:33:30 TiTok]: [0m***** Running training *****
[32m[09/01 20:33:30 TiTok]: [0m  Num training steps = 1000000
[32m[09/01 20:33:30 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 20:33:30 TiTok]: [0mmixed precision = fp16
[32m[09/01 20:33:30 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 128
[32m[09/01 20:33:30 TiTok]: [0maccelerator device: cuda
[32m[09/01 20:33:30 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 20:33:30 TiTok]: [0mTraining from scratch.
[32m[09/01 20:49:04 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 20:49:04 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 32
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 1000000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 20:49:04 TiTok]: [0mCreating model and loss module.
[32m[09/01 20:49:05 TiTok]: [0mCreating optimizers.
[32m[09/01 20:49:05 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 20:49:05 TiTok]: [0mCreating dataloaders. Batch size = 32
[32m[09/01 20:49:27 TiTok]: [0mCreating evaluator.
[32m[09/01 20:49:27 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 20:49:27 TiTok]: [0m***** Running training *****
[32m[09/01 20:49:27 TiTok]: [0m  Num training steps = 1000000
[32m[09/01 20:49:27 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 20:49:27 TiTok]: [0mmixed precision = fp16
[32m[09/01 20:49:27 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 32
[32m[09/01 20:49:27 TiTok]: [0maccelerator device: cuda
[32m[09/01 20:49:27 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 20:49:27 TiTok]: [0mTraining from scratch.
[32m[09/01 20:50:42 TiTok]: [0mData (t): 0.0120, 53.55/s/gpu Batch (t): 0.5975 LR: 0.000001 Step: 50 Total Loss: 6.9325 Recon Loss: 6.9203 
[32m[09/01 20:51:12 TiTok]: [0mData (t): 0.0115, 53.60/s/gpu Batch (t): 0.5970 LR: 0.000001 Step: 100 Total Loss: 6.6372 Recon Loss: 6.6369 
[32m[09/01 20:51:42 TiTok]: [0mData (t): 0.0120, 52.92/s/gpu Batch (t): 0.6047 LR: 0.000002 Step: 150 Total Loss: 6.4383 Recon Loss: 6.4382 
[32m[09/01 20:52:13 TiTok]: [0mData (t): 0.0120, 52.93/s/gpu Batch (t): 0.6046 LR: 0.000002 Step: 200 Total Loss: 6.3549 Recon Loss: 6.3548 
[32m[09/01 20:52:43 TiTok]: [0mData (t): 0.0130, 52.80/s/gpu Batch (t): 0.6060 LR: 0.000003 Step: 250 Total Loss: 6.2104 Recon Loss: 6.2102 
[32m[09/01 20:53:13 TiTok]: [0mData (t): 0.0140, 52.90/s/gpu Batch (t): 0.6049 LR: 0.000003 Step: 300 Total Loss: 6.0910 Recon Loss: 6.0908 
[32m[09/01 20:53:43 TiTok]: [0mData (t): 0.0140, 52.90/s/gpu Batch (t): 0.6049 LR: 0.000004 Step: 350 Total Loss: 5.9931 Recon Loss: 5.9927 
[32m[09/01 20:54:14 TiTok]: [0mData (t): 0.0140, 50.79/s/gpu Batch (t): 0.6300 LR: 0.000004 Step: 400 Total Loss: 5.7849 Recon Loss: 5.7842 
[32m[09/01 20:54:44 TiTok]: [0mData (t): 0.0130, 53.08/s/gpu Batch (t): 0.6028 LR: 0.000005 Step: 450 Total Loss: 5.6693 Recon Loss: 5.6687 
[32m[09/01 20:55:14 TiTok]: [0mData (t): 0.0130, 52.89/s/gpu Batch (t): 0.6050 LR: 0.000005 Step: 500 Total Loss: 5.4757 Recon Loss: 5.4752 
[32m[09/01 20:55:45 TiTok]: [0mData (t): 0.0120, 52.06/s/gpu Batch (t): 0.6147 LR: 0.000006 Step: 550 Total Loss: 5.3969 Recon Loss: 5.3966 
[32m[09/01 20:56:15 TiTok]: [0mData (t): 0.0130, 53.10/s/gpu Batch (t): 0.6027 LR: 0.000006 Step: 600 Total Loss: 5.3542 Recon Loss: 5.3537 
[32m[09/01 20:56:45 TiTok]: [0mData (t): 0.0120, 53.21/s/gpu Batch (t): 0.6014 LR: 0.000007 Step: 650 Total Loss: 5.2569 Recon Loss: 5.2563 
[32m[09/01 20:57:15 TiTok]: [0mData (t): 0.0128, 52.73/s/gpu Batch (t): 0.6068 LR: 0.000007 Step: 700 Total Loss: 5.3364 Recon Loss: 5.3360 
[32m[09/01 20:57:45 TiTok]: [0mData (t): 0.0120, 52.91/s/gpu Batch (t): 0.6048 LR: 0.000008 Step: 750 Total Loss: 5.2698 Recon Loss: 5.2691 
[32m[09/01 20:58:16 TiTok]: [0mData (t): 0.0120, 52.98/s/gpu Batch (t): 0.6040 LR: 0.000008 Step: 800 Total Loss: 5.1541 Recon Loss: 5.1537 
[32m[09/01 20:58:46 TiTok]: [0mData (t): 0.0140, 53.02/s/gpu Batch (t): 0.6035 LR: 0.000009 Step: 850 Total Loss: 5.0446 Recon Loss: 5.0440 
[32m[09/01 20:59:16 TiTok]: [0mData (t): 0.0120, 53.47/s/gpu Batch (t): 0.5985 LR: 0.000009 Step: 900 Total Loss: 4.9278 Recon Loss: 4.9273 
[32m[09/01 20:59:46 TiTok]: [0mData (t): 0.0140, 53.16/s/gpu Batch (t): 0.6020 LR: 0.000010 Step: 950 Total Loss: 4.9696 Recon Loss: 4.9690 
[32m[09/01 21:00:17 TiTok]: [0mData (t): 0.0130, 44.02/s/gpu Batch (t): 0.7270 LR: 0.000010 Step: 1000 Total Loss: 4.9534 Recon Loss: 4.9530 
[32m[09/01 21:00:47 TiTok]: [0mData (t): 0.0120, 53.15/s/gpu Batch (t): 0.6020 LR: 0.000010 Step: 1050 Total Loss: 4.8775 Recon Loss: 4.8767 
[32m[09/01 21:01:17 TiTok]: [0mData (t): 0.0130, 53.29/s/gpu Batch (t): 0.6005 LR: 0.000011 Step: 1100 Total Loss: 5.0103 Recon Loss: 5.0064 
[32m[09/01 21:01:48 TiTok]: [0mData (t): 0.0130, 52.55/s/gpu Batch (t): 0.6090 LR: 0.000012 Step: 1150 Total Loss: 5.0913 Recon Loss: 5.0902 
[32m[09/01 21:07:55 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 21:07:55 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 64
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 500000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 21:07:56 TiTok]: [0mCreating model and loss module.
[32m[09/01 21:07:56 TiTok]: [0mCreating optimizers.
[32m[09/01 21:07:56 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 21:07:56 TiTok]: [0mCreating dataloaders. Batch size = 64
[32m[09/01 21:08:17 TiTok]: [0mCreating evaluator.
[32m[09/01 21:08:17 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 21:08:17 TiTok]: [0m***** Running training *****
[32m[09/01 21:08:17 TiTok]: [0m  Num training steps = 500000
[32m[09/01 21:08:17 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 21:08:17 TiTok]: [0mmixed precision = fp16
[32m[09/01 21:08:17 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 64
[32m[09/01 21:08:17 TiTok]: [0maccelerator device: cuda
[32m[09/01 21:08:17 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 21:08:17 TiTok]: [0mTraining from scratch.
[32m[09/01 21:10:00 TiTok]: [0mData (t): 0.0220, 57.22/s/gpu Batch (t): 1.1185 LR: 0.000001 Step: 50 Total Loss: 6.9229 Recon Loss: 6.9107 
[32m[09/01 21:10:57 TiTok]: [0mData (t): 0.0230, 56.75/s/gpu Batch (t): 1.1278 LR: 0.000001 Step: 100 Total Loss: 6.6358 Recon Loss: 6.6355 
[32m[09/01 21:11:54 TiTok]: [0mData (t): 0.0250, 54.63/s/gpu Batch (t): 1.1716 LR: 0.000002 Step: 150 Total Loss: 6.4376 Recon Loss: 6.4375 
[32m[09/01 21:12:51 TiTok]: [0mData (t): 0.0240, 56.42/s/gpu Batch (t): 1.1343 LR: 0.000002 Step: 200 Total Loss: 6.2914 Recon Loss: 6.2913 
[32m[09/01 21:14:10 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 21:14:10 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 100000
  eval_every: 100000
  generate_every: 5000
  log_every: 100
  log_grad_norm_every: 2000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers: 10
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 256
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 250000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 21:14:10 TiTok]: [0mCreating model and loss module.
[32m[09/01 21:14:11 TiTok]: [0mCreating optimizers.
[32m[09/01 21:14:11 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 21:14:11 TiTok]: [0mCreating dataloaders. Batch size = 256
[32m[09/01 21:15:45 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 21:15:45 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 100000
  eval_every: 100000
  generate_every: 5000
  log_every: 100
  log_grad_norm_every: 2000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers: 10
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 256
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 250000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 21:15:45 TiTok]: [0mCreating model and loss module.
[32m[09/01 21:15:46 TiTok]: [0mCreating optimizers.
[32m[09/01 21:15:46 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 21:15:46 TiTok]: [0mCreating dataloaders. Batch size = 256
[32m[09/01 21:16:06 TiTok]: [0mCreating evaluator.
[32m[09/01 21:16:07 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 21:16:07 TiTok]: [0m***** Running training *****
[32m[09/01 21:16:07 TiTok]: [0m  Num training steps = 250000
[32m[09/01 21:16:07 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 21:16:07 TiTok]: [0mmixed precision = fp16
[32m[09/01 21:16:07 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 256
[32m[09/01 21:16:07 TiTok]: [0maccelerator device: cuda
[32m[09/01 21:17:47 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 21:17:47 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 100000
  eval_every: 100000
  generate_every: 5000
  log_every: 100
  log_grad_norm_every: 2000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers: 10
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 256
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 250000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 21:17:47 TiTok]: [0mCreating model and loss module.
[32m[09/01 21:17:48 TiTok]: [0mCreating optimizers.
[32m[09/01 21:17:48 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 21:17:48 TiTok]: [0mCreating dataloaders. Batch size = 256
[32m[09/01 21:18:09 TiTok]: [0mCreating evaluator.
[32m[09/01 21:18:09 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 21:18:09 TiTok]: [0m***** Running training *****
[32m[09/01 21:18:09 TiTok]: [0m  Num training steps = 250000
[32m[09/01 21:18:09 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 21:18:09 TiTok]: [0mmixed precision = fp16
[32m[09/01 21:18:09 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 256
[32m[09/01 21:18:09 TiTok]: [0maccelerator device: cuda
[32m[09/01 21:18:09 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 21:18:09 TiTok]: [0mTraining from scratch.
[32m[09/01 21:20:17 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 21:20:17 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 100
  log_grad_norm_every: 2000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers: 10
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 256
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 125000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 21:20:18 TiTok]: [0mCreating model and loss module.
[32m[09/01 21:20:18 TiTok]: [0mCreating optimizers.
[32m[09/01 21:20:18 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 21:20:18 TiTok]: [0mCreating dataloaders. Batch size = 256
[32m[09/01 21:20:39 TiTok]: [0mCreating evaluator.
[32m[09/01 21:20:39 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 21:20:39 TiTok]: [0m***** Running training *****
[32m[09/01 21:20:39 TiTok]: [0m  Num training steps = 125000
[32m[09/01 21:20:39 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 21:20:39 TiTok]: [0mmixed precision = fp16
[32m[09/01 21:20:39 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 256
[32m[09/01 21:20:39 TiTok]: [0maccelerator device: cuda
[32m[09/01 21:20:39 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 21:20:39 TiTok]: [0mTraining from scratch.
[32m[09/01 21:26:41 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 21:26:41 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 100
  log_grad_norm_every: 2000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers: 10
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 128
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 250000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 21:26:41 TiTok]: [0mCreating model and loss module.
[32m[09/01 21:26:42 TiTok]: [0mCreating optimizers.
[32m[09/01 21:26:42 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 21:26:42 TiTok]: [0mCreating dataloaders. Batch size = 128
[32m[09/01 21:27:02 TiTok]: [0mCreating evaluator.
[32m[09/01 21:27:03 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 21:27:03 TiTok]: [0m***** Running training *****
[32m[09/01 21:27:03 TiTok]: [0m  Num training steps = 250000
[32m[09/01 21:27:03 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 21:27:03 TiTok]: [0mmixed precision = fp16
[32m[09/01 21:27:03 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 128
[32m[09/01 21:27:03 TiTok]: [0maccelerator device: cuda
[32m[09/01 21:27:03 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 21:27:03 TiTok]: [0mTraining from scratch.
[32m[09/01 21:31:11 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 21:31:11 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 200
  log_grad_norm_every: 4000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers: 10
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 128
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 250000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 21:31:11 TiTok]: [0mCreating model and loss module.
[32m[09/01 21:31:12 TiTok]: [0mCreating optimizers.
[32m[09/01 21:31:12 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 21:31:12 TiTok]: [0mCreating dataloaders. Batch size = 128
[32m[09/01 21:31:32 TiTok]: [0mCreating evaluator.
[32m[09/01 21:31:33 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 21:31:33 TiTok]: [0m***** Running training *****
[32m[09/01 21:31:33 TiTok]: [0m  Num training steps = 250000
[32m[09/01 21:31:33 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 21:31:33 TiTok]: [0mmixed precision = fp16
[32m[09/01 21:31:33 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 128
[32m[09/01 21:31:33 TiTok]: [0maccelerator device: cuda
[32m[09/01 21:31:33 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 21:31:33 TiTok]: [0mTraining from scratch.
[32m[09/01 21:37:27 TiTok]: [0mSaving config to titok_b_64_stage1_run1\config.yaml
[32m[09/01 21:37:27 TiTok]: [0mConfig:
experiment:
  project: titok_b_64_stage1
  name: titok_b_64_stage1_run1
  output_dir: titok_b_64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 200
  log_grad_norm_every: 4000
  resume: true
  init_weight: ''
  logging_dir: titok_b_64_stage1_run1\logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
losses:
  quantizer_weight: 1.0
dataset:
  params:
    params: null
    img_path: ../../CSL-Daily/sentence/frames_512x512/
    num_workers: 10
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 64
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 500000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_CSL.yaml
--experiment:
  project: titok_b64_CSL_stage1
  name: titok_b64_CSL_stage1_run1
  output_dir: titok_b64_CSL_stage1_run1
--training:
  per_gpu_batch_size: 32

[32m[09/01 21:37:28 TiTok]: [0mCreating model and loss module.
[32m[09/01 21:37:28 TiTok]: [0mCreating optimizers.
[32m[09/01 21:37:28 TiTok]: [0mCreating lr_schedulers.
[32m[09/01 21:37:28 TiTok]: [0mCreating dataloaders. Batch size = 64
[32m[09/01 21:37:49 TiTok]: [0mCreating evaluator.
[32m[09/01 21:37:49 TiTok]: [0mPreparing model, optimizer and dataloaders
[32m[09/01 21:37:49 TiTok]: [0m***** Running training *****
[32m[09/01 21:37:49 TiTok]: [0m  Num training steps = 500000
[32m[09/01 21:37:49 TiTok]: [0m  Gradient Accumulation steps = 1
[32m[09/01 21:37:49 TiTok]: [0mmixed precision = fp16
[32m[09/01 21:37:49 TiTok]: [0m  Total train batch size (w. parallel, distributed & accumulation) = 64
[32m[09/01 21:37:49 TiTok]: [0maccelerator device: cuda
[32m[09/01 21:37:49 TiTok]: [0mAll globbed checkpoints are: []
[32m[09/01 21:37:49 TiTok]: [0mTraining from scratch.
[32m[09/01 21:42:23 TiTok]: [0mData (t): 0.0230, 55.89/s/gpu Batch (t): 1.1451 LR: 0.000002 Step: 200 Total Loss: 6.2914 Recon Loss: 6.2913 
[32m[09/01 21:46:23 TiTok]: [0mData (t): 0.0230, 54.35/s/gpu Batch (t): 1.1776 LR: 0.000004 Step: 400 Total Loss: 5.7501 Recon Loss: 5.7496 
[32m[09/01 21:50:19 TiTok]: [0mData (t): 0.0230, 54.25/s/gpu Batch (t): 1.1796 LR: 0.000006 Step: 600 Total Loss: 5.3267 Recon Loss: 5.3263 
[32m[09/01 21:54:15 TiTok]: [0mData (t): 0.0230, 54.33/s/gpu Batch (t): 1.1780 LR: 0.000008 Step: 800 Total Loss: 5.0894 Recon Loss: 5.0890 
[32m[09/01 21:58:13 TiTok]: [0mData (t): 0.0230, 53.99/s/gpu Batch (t): 1.1855 LR: 0.000010 Step: 1000 Total Loss: 4.9734 Recon Loss: 4.9704 
[32m[09/01 22:02:10 TiTok]: [0mData (t): 0.0230, 53.88/s/gpu Batch (t): 1.1878 LR: 0.000012 Step: 1200 Total Loss: 4.8506 Recon Loss: 4.8494 
[32m[09/01 22:06:08 TiTok]: [0mData (t): 0.0230, 53.46/s/gpu Batch (t): 1.1971 LR: 0.000014 Step: 1400 Total Loss: 4.7995 Recon Loss: 4.7956 
[32m[09/01 22:10:05 TiTok]: [0mData (t): 0.0230, 53.35/s/gpu Batch (t): 1.1995 LR: 0.000016 Step: 1600 Total Loss: 4.5455 Recon Loss: 4.5412 
[32m[09/01 22:14:02 TiTok]: [0mData (t): 0.0230, 54.07/s/gpu Batch (t): 1.1836 LR: 0.000018 Step: 1800 Total Loss: 4.4277 Recon Loss: 4.4231 
[32m[09/01 22:17:58 TiTok]: [0mData (t): 0.0220, 54.14/s/gpu Batch (t): 1.1821 LR: 0.000020 Step: 2000 Total Loss: 4.2270 Recon Loss: 4.2220 
[32m[09/01 22:21:54 TiTok]: [0mData (t): 0.0230, 54.10/s/gpu Batch (t): 1.1831 LR: 0.000022 Step: 2200 Total Loss: 4.0995 Recon Loss: 4.0945 
[32m[09/01 22:25:50 TiTok]: [0mData (t): 0.0250, 55.69/s/gpu Batch (t): 1.1492 LR: 0.000024 Step: 2400 Total Loss: 3.9758 Recon Loss: 3.9706 
[32m[09/01 22:29:44 TiTok]: [0mData (t): 0.0230, 53.91/s/gpu Batch (t): 1.1871 LR: 0.000026 Step: 2600 Total Loss: 3.7836 Recon Loss: 3.7782 
[32m[09/01 22:33:40 TiTok]: [0mData (t): 0.0220, 54.21/s/gpu Batch (t): 1.1806 LR: 0.000028 Step: 2800 Total Loss: 3.8848 Recon Loss: 3.8792 
[32m[09/01 22:37:37 TiTok]: [0mData (t): 0.0250, 53.91/s/gpu Batch (t): 1.1871 LR: 0.000030 Step: 3000 Total Loss: 3.7311 Recon Loss: 3.7255 
[32m[09/01 22:41:33 TiTok]: [0mData (t): 0.0230, 54.14/s/gpu Batch (t): 1.1821 LR: 0.000032 Step: 3200 Total Loss: 3.8147 Recon Loss: 3.8088 
[32m[09/01 22:45:30 TiTok]: [0mData (t): 0.0230, 54.10/s/gpu Batch (t): 1.1831 LR: 0.000034 Step: 3400 Total Loss: 3.5934 Recon Loss: 3.5882 
[32m[09/01 22:49:26 TiTok]: [0mData (t): 0.0230, 54.20/s/gpu Batch (t): 1.1808 LR: 0.000036 Step: 3600 Total Loss: 3.5457 Recon Loss: 3.5403 
[32m[09/01 22:53:23 TiTok]: [0mData (t): 0.0230, 54.09/s/gpu Batch (t): 1.1832 LR: 0.000038 Step: 3800 Total Loss: 3.5607 Recon Loss: 3.5552 
[32m[09/01 22:57:19 TiTok]: [0mData (t): 0.0220, 49.78/s/gpu Batch (t): 1.2856 LR: 0.000040 Step: 4000 Total Loss: 3.3919 Recon Loss: 3.3866 
[32m[09/01 23:01:16 TiTok]: [0mData (t): 0.0230, 54.17/s/gpu Batch (t): 1.1815 LR: 0.000042 Step: 4200 Total Loss: 3.4148 Recon Loss: 3.4092 
[32m[09/01 23:05:13 TiTok]: [0mData (t): 0.0230, 53.11/s/gpu Batch (t): 1.2050 LR: 0.000044 Step: 4400 Total Loss: 3.5046 Recon Loss: 3.4988 
[32m[09/01 23:09:08 TiTok]: [0mData (t): 0.0240, 54.79/s/gpu Batch (t): 1.1681 LR: 0.000046 Step: 4600 Total Loss: 3.2935 Recon Loss: 3.2882 
[32m[09/01 23:13:01 TiTok]: [0mData (t): 0.0230, 54.82/s/gpu Batch (t): 1.1675 LR: 0.000048 Step: 4800 Total Loss: 3.4286 Recon Loss: 3.4228 
[32m[09/01 23:16:55 TiTok]: [0mData (t): 0.0230, 54.89/s/gpu Batch (t): 1.1659 LR: 0.000050 Step: 5000 Total Loss: 3.2662 Recon Loss: 3.2609 
[32m[09/01 23:16:55 TiTok]: [0mReconstructing images...
[32m[09/01 23:20:46 TiTok]: [0mData (t): 0.0230, 55.81/s/gpu Batch (t): 1.1468 LR: 0.000052 Step: 5200 Total Loss: 3.1968 Recon Loss: 3.1915 
[32m[09/01 23:24:36 TiTok]: [0mData (t): 0.0230, 57.40/s/gpu Batch (t): 1.1150 LR: 0.000054 Step: 5400 Total Loss: 3.0211 Recon Loss: 3.0159 
[32m[09/01 23:28:19 TiTok]: [0mData (t): 0.0230, 57.45/s/gpu Batch (t): 1.1141 LR: 0.000056 Step: 5600 Total Loss: 3.2477 Recon Loss: 3.2425 
[32m[09/01 23:32:02 TiTok]: [0mData (t): 0.0230, 57.50/s/gpu Batch (t): 1.1130 LR: 0.000058 Step: 5800 Total Loss: 3.1116 Recon Loss: 3.1064 
[32m[09/01 23:35:45 TiTok]: [0mData (t): 0.0220, 57.55/s/gpu Batch (t): 1.1121 LR: 0.000060 Step: 6000 Total Loss: 2.9689 Recon Loss: 2.9639 
[32m[09/01 23:39:27 TiTok]: [0mData (t): 0.0230, 57.45/s/gpu Batch (t): 1.1141 LR: 0.000062 Step: 6200 Total Loss: 3.0479 Recon Loss: 3.0428 
[32m[09/01 23:43:10 TiTok]: [0mData (t): 0.0230, 57.31/s/gpu Batch (t): 1.1167 LR: 0.000064 Step: 6400 Total Loss: 2.9026 Recon Loss: 2.8977 
[32m[09/01 23:46:53 TiTok]: [0mData (t): 0.0220, 57.65/s/gpu Batch (t): 1.1101 LR: 0.000066 Step: 6600 Total Loss: 2.8357 Recon Loss: 2.8308 
[32m[09/01 23:50:34 TiTok]: [0mData (t): 0.0230, 57.55/s/gpu Batch (t): 1.1121 LR: 0.000068 Step: 6800 Total Loss: 2.9350 Recon Loss: 2.9300 
[32m[09/01 23:54:16 TiTok]: [0mData (t): 0.0220, 57.82/s/gpu Batch (t): 1.1070 LR: 0.000070 Step: 7000 Total Loss: 3.0124 Recon Loss: 3.0076 
[32m[09/01 23:57:58 TiTok]: [0mData (t): 0.0240, 57.65/s/gpu Batch (t): 1.1101 LR: 0.000072 Step: 7200 Total Loss: 2.8472 Recon Loss: 2.8425 
[32m[09/02 00:01:40 TiTok]: [0mData (t): 0.0220, 57.71/s/gpu Batch (t): 1.1091 LR: 0.000074 Step: 7400 Total Loss: 2.9840 Recon Loss: 2.9792 
[32m[09/02 00:05:22 TiTok]: [0mData (t): 0.0230, 57.76/s/gpu Batch (t): 1.1080 LR: 0.000076 Step: 7600 Total Loss: 3.0037 Recon Loss: 2.9990 
[32m[09/02 00:09:04 TiTok]: [0mData (t): 0.0220, 57.76/s/gpu Batch (t): 1.1080 LR: 0.000078 Step: 7800 Total Loss: 2.9403 Recon Loss: 2.9356 
[32m[09/02 00:12:50 TiTok]: [0mData (t): 0.0230, 51.69/s/gpu Batch (t): 1.2381 LR: 0.000080 Step: 8000 Total Loss: 2.7975 Recon Loss: 2.7929 
