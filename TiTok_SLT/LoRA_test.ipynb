{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration, MBartTokenizer, \n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "    MBartTokenizerFast, MBartModel, MBartConfig,\n",
    "    \n",
    "  )\n",
    "import random\n",
    "from transformers.models.mbart.modeling_mbart import MBartAttention\n",
    "from pathlib import Path\n",
    "from typing import Union, Dict\n",
    "import pathlib\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import copy\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import Tensor\n",
    "import tabulate\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Create MBart model \n",
    "'''\n",
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "per_device_train_batch_size=16\n",
    "per_device_eval_batch_size=16\n",
    "learning_rate= 3e-4\n",
    "num_train_epochs=2\n",
    "lr_scheduler_type = 'cosine'\n",
    "warmup_ratio = 0.03\n",
    "lora_head = False\n",
    "lora_attention = True\n",
    "lora_encoder = True\n",
    "lora_decoder = False \n",
    "lora_rank = 4\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.0\n",
    "source_lang = \"en\"\n",
    "target_lang = \"vi\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer_en2vi = AutoTokenizer.from_pretrained(model_name)\n",
    "model_en2vi = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "for param in model_en2vi.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MBartForConditionalGeneration(\n",
       "  (model): MBartModel(\n",
       "    (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "    (encoder): MBartEncoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartEncoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): MBartDecoder(\n",
       "      (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x MBartDecoderLayer(\n",
       "          (self_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MBartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Test out normal MBart model first '''\n",
    "model_en2vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define LoRA layer'''\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha, lora_dropout=0.0):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "        self.lora_dropout = nn.Dropout(lora_dropout) # Add dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_output = self.alpha * (x @ self.A @ self.B)\n",
    "        lora_output = self.lora_dropout(lora_output) # Apply dropout to the LoRA output\n",
    "        return lora_output\n",
    "\n",
    "class LinearWithLoRAMerged(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha, lora_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha, lora_dropout\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        lora = self.lora.A @ self.lora.B\n",
    "        combined_weight = self.linear.weight + self.lora.alpha*lora.T\n",
    "        return F.linear(x, combined_weight, self.linear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.shared.weight: False\n",
      "model.encoder.embed_positions.weight: False\n",
      "model.encoder.layers.0.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.0.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.0.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.0.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.0.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.0.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.0.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.0.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.0.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.0.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.0.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.0.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.0.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.0.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.0.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.0.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.0.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.0.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.0.fc1.linear.weight: False\n",
      "model.encoder.layers.0.fc1.linear.bias: False\n",
      "model.encoder.layers.0.fc1.lora.A: True\n",
      "model.encoder.layers.0.fc1.lora.B: True\n",
      "model.encoder.layers.0.fc2.linear.weight: False\n",
      "model.encoder.layers.0.fc2.linear.bias: False\n",
      "model.encoder.layers.0.fc2.lora.A: True\n",
      "model.encoder.layers.0.fc2.lora.B: True\n",
      "model.encoder.layers.0.final_layer_norm.weight: False\n",
      "model.encoder.layers.0.final_layer_norm.bias: False\n",
      "model.encoder.layers.1.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.1.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.1.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.1.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.1.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.1.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.1.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.1.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.1.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.1.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.1.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.1.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.1.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.1.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.1.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.1.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.1.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.1.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.1.fc1.linear.weight: False\n",
      "model.encoder.layers.1.fc1.linear.bias: False\n",
      "model.encoder.layers.1.fc1.lora.A: True\n",
      "model.encoder.layers.1.fc1.lora.B: True\n",
      "model.encoder.layers.1.fc2.linear.weight: False\n",
      "model.encoder.layers.1.fc2.linear.bias: False\n",
      "model.encoder.layers.1.fc2.lora.A: True\n",
      "model.encoder.layers.1.fc2.lora.B: True\n",
      "model.encoder.layers.1.final_layer_norm.weight: False\n",
      "model.encoder.layers.1.final_layer_norm.bias: False\n",
      "model.encoder.layers.2.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.2.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.2.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.2.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.2.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.2.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.2.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.2.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.2.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.2.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.2.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.2.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.2.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.2.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.2.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.2.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.2.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.2.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.2.fc1.linear.weight: False\n",
      "model.encoder.layers.2.fc1.linear.bias: False\n",
      "model.encoder.layers.2.fc1.lora.A: True\n",
      "model.encoder.layers.2.fc1.lora.B: True\n",
      "model.encoder.layers.2.fc2.linear.weight: False\n",
      "model.encoder.layers.2.fc2.linear.bias: False\n",
      "model.encoder.layers.2.fc2.lora.A: True\n",
      "model.encoder.layers.2.fc2.lora.B: True\n",
      "model.encoder.layers.2.final_layer_norm.weight: False\n",
      "model.encoder.layers.2.final_layer_norm.bias: False\n",
      "model.encoder.layers.3.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.3.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.3.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.3.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.3.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.3.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.3.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.3.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.3.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.3.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.3.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.3.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.3.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.3.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.3.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.3.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.3.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.3.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.3.fc1.linear.weight: False\n",
      "model.encoder.layers.3.fc1.linear.bias: False\n",
      "model.encoder.layers.3.fc1.lora.A: True\n",
      "model.encoder.layers.3.fc1.lora.B: True\n",
      "model.encoder.layers.3.fc2.linear.weight: False\n",
      "model.encoder.layers.3.fc2.linear.bias: False\n",
      "model.encoder.layers.3.fc2.lora.A: True\n",
      "model.encoder.layers.3.fc2.lora.B: True\n",
      "model.encoder.layers.3.final_layer_norm.weight: False\n",
      "model.encoder.layers.3.final_layer_norm.bias: False\n",
      "model.encoder.layers.4.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.4.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.4.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.4.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.4.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.4.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.4.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.4.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.4.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.4.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.4.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.4.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.4.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.4.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.4.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.4.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.4.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.4.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.4.fc1.linear.weight: False\n",
      "model.encoder.layers.4.fc1.linear.bias: False\n",
      "model.encoder.layers.4.fc1.lora.A: True\n",
      "model.encoder.layers.4.fc1.lora.B: True\n",
      "model.encoder.layers.4.fc2.linear.weight: False\n",
      "model.encoder.layers.4.fc2.linear.bias: False\n",
      "model.encoder.layers.4.fc2.lora.A: True\n",
      "model.encoder.layers.4.fc2.lora.B: True\n",
      "model.encoder.layers.4.final_layer_norm.weight: False\n",
      "model.encoder.layers.4.final_layer_norm.bias: False\n",
      "model.encoder.layers.5.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.5.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.5.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.5.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.5.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.5.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.5.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.5.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.5.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.5.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.5.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.5.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.5.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.5.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.5.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.5.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.5.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.5.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.5.fc1.linear.weight: False\n",
      "model.encoder.layers.5.fc1.linear.bias: False\n",
      "model.encoder.layers.5.fc1.lora.A: True\n",
      "model.encoder.layers.5.fc1.lora.B: True\n",
      "model.encoder.layers.5.fc2.linear.weight: False\n",
      "model.encoder.layers.5.fc2.linear.bias: False\n",
      "model.encoder.layers.5.fc2.lora.A: True\n",
      "model.encoder.layers.5.fc2.lora.B: True\n",
      "model.encoder.layers.5.final_layer_norm.weight: False\n",
      "model.encoder.layers.5.final_layer_norm.bias: False\n",
      "model.encoder.layers.6.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.6.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.6.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.6.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.6.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.6.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.6.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.6.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.6.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.6.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.6.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.6.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.6.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.6.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.6.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.6.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.6.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.6.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.6.fc1.linear.weight: False\n",
      "model.encoder.layers.6.fc1.linear.bias: False\n",
      "model.encoder.layers.6.fc1.lora.A: True\n",
      "model.encoder.layers.6.fc1.lora.B: True\n",
      "model.encoder.layers.6.fc2.linear.weight: False\n",
      "model.encoder.layers.6.fc2.linear.bias: False\n",
      "model.encoder.layers.6.fc2.lora.A: True\n",
      "model.encoder.layers.6.fc2.lora.B: True\n",
      "model.encoder.layers.6.final_layer_norm.weight: False\n",
      "model.encoder.layers.6.final_layer_norm.bias: False\n",
      "model.encoder.layers.7.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.7.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.7.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.7.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.7.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.7.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.7.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.7.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.7.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.7.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.7.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.7.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.7.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.7.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.7.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.7.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.7.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.7.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.7.fc1.linear.weight: False\n",
      "model.encoder.layers.7.fc1.linear.bias: False\n",
      "model.encoder.layers.7.fc1.lora.A: True\n",
      "model.encoder.layers.7.fc1.lora.B: True\n",
      "model.encoder.layers.7.fc2.linear.weight: False\n",
      "model.encoder.layers.7.fc2.linear.bias: False\n",
      "model.encoder.layers.7.fc2.lora.A: True\n",
      "model.encoder.layers.7.fc2.lora.B: True\n",
      "model.encoder.layers.7.final_layer_norm.weight: False\n",
      "model.encoder.layers.7.final_layer_norm.bias: False\n",
      "model.encoder.layers.8.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.8.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.8.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.8.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.8.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.8.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.8.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.8.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.8.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.8.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.8.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.8.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.8.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.8.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.8.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.8.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.8.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.8.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.8.fc1.linear.weight: False\n",
      "model.encoder.layers.8.fc1.linear.bias: False\n",
      "model.encoder.layers.8.fc1.lora.A: True\n",
      "model.encoder.layers.8.fc1.lora.B: True\n",
      "model.encoder.layers.8.fc2.linear.weight: False\n",
      "model.encoder.layers.8.fc2.linear.bias: False\n",
      "model.encoder.layers.8.fc2.lora.A: True\n",
      "model.encoder.layers.8.fc2.lora.B: True\n",
      "model.encoder.layers.8.final_layer_norm.weight: False\n",
      "model.encoder.layers.8.final_layer_norm.bias: False\n",
      "model.encoder.layers.9.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.9.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.9.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.9.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.9.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.9.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.9.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.9.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.9.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.9.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.9.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.9.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.9.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.9.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.9.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.9.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.9.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.9.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.9.fc1.linear.weight: False\n",
      "model.encoder.layers.9.fc1.linear.bias: False\n",
      "model.encoder.layers.9.fc1.lora.A: True\n",
      "model.encoder.layers.9.fc1.lora.B: True\n",
      "model.encoder.layers.9.fc2.linear.weight: False\n",
      "model.encoder.layers.9.fc2.linear.bias: False\n",
      "model.encoder.layers.9.fc2.lora.A: True\n",
      "model.encoder.layers.9.fc2.lora.B: True\n",
      "model.encoder.layers.9.final_layer_norm.weight: False\n",
      "model.encoder.layers.9.final_layer_norm.bias: False\n",
      "model.encoder.layers.10.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.10.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.10.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.10.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.10.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.10.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.10.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.10.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.10.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.10.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.10.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.10.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.10.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.10.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.10.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.10.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.10.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.10.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.10.fc1.linear.weight: False\n",
      "model.encoder.layers.10.fc1.linear.bias: False\n",
      "model.encoder.layers.10.fc1.lora.A: True\n",
      "model.encoder.layers.10.fc1.lora.B: True\n",
      "model.encoder.layers.10.fc2.linear.weight: False\n",
      "model.encoder.layers.10.fc2.linear.bias: False\n",
      "model.encoder.layers.10.fc2.lora.A: True\n",
      "model.encoder.layers.10.fc2.lora.B: True\n",
      "model.encoder.layers.10.final_layer_norm.weight: False\n",
      "model.encoder.layers.10.final_layer_norm.bias: False\n",
      "model.encoder.layers.11.self_attn.k_proj.linear.weight: False\n",
      "model.encoder.layers.11.self_attn.k_proj.linear.bias: False\n",
      "model.encoder.layers.11.self_attn.k_proj.lora.A: True\n",
      "model.encoder.layers.11.self_attn.k_proj.lora.B: True\n",
      "model.encoder.layers.11.self_attn.v_proj.linear.weight: False\n",
      "model.encoder.layers.11.self_attn.v_proj.linear.bias: False\n",
      "model.encoder.layers.11.self_attn.v_proj.lora.A: True\n",
      "model.encoder.layers.11.self_attn.v_proj.lora.B: True\n",
      "model.encoder.layers.11.self_attn.q_proj.linear.weight: False\n",
      "model.encoder.layers.11.self_attn.q_proj.linear.bias: False\n",
      "model.encoder.layers.11.self_attn.q_proj.lora.A: True\n",
      "model.encoder.layers.11.self_attn.q_proj.lora.B: True\n",
      "model.encoder.layers.11.self_attn.out_proj.linear.weight: False\n",
      "model.encoder.layers.11.self_attn.out_proj.linear.bias: False\n",
      "model.encoder.layers.11.self_attn.out_proj.lora.A: True\n",
      "model.encoder.layers.11.self_attn.out_proj.lora.B: True\n",
      "model.encoder.layers.11.self_attn_layer_norm.weight: False\n",
      "model.encoder.layers.11.self_attn_layer_norm.bias: False\n",
      "model.encoder.layers.11.fc1.linear.weight: False\n",
      "model.encoder.layers.11.fc1.linear.bias: False\n",
      "model.encoder.layers.11.fc1.lora.A: True\n",
      "model.encoder.layers.11.fc1.lora.B: True\n",
      "model.encoder.layers.11.fc2.linear.weight: False\n",
      "model.encoder.layers.11.fc2.linear.bias: False\n",
      "model.encoder.layers.11.fc2.lora.A: True\n",
      "model.encoder.layers.11.fc2.lora.B: True\n",
      "model.encoder.layers.11.final_layer_norm.weight: False\n",
      "model.encoder.layers.11.final_layer_norm.bias: False\n",
      "model.encoder.layernorm_embedding.weight: False\n",
      "model.encoder.layernorm_embedding.bias: False\n",
      "model.encoder.layer_norm.weight: False\n",
      "model.encoder.layer_norm.bias: False\n",
      "model.decoder.embed_positions.weight: False\n",
      "model.decoder.layers.0.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.0.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.0.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.0.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.0.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.0.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.0.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.0.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.0.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.0.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.0.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.0.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.0.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.0.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.0.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.0.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.0.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.0.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.0.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.0.fc1.weight: False\n",
      "model.decoder.layers.0.fc1.bias: False\n",
      "model.decoder.layers.0.fc2.weight: False\n",
      "model.decoder.layers.0.fc2.bias: False\n",
      "model.decoder.layers.0.final_layer_norm.weight: False\n",
      "model.decoder.layers.0.final_layer_norm.bias: False\n",
      "model.decoder.layers.1.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.1.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.1.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.1.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.1.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.1.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.1.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.1.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.1.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.1.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.1.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.1.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.1.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.1.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.1.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.1.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.1.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.1.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.1.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.1.fc1.weight: False\n",
      "model.decoder.layers.1.fc1.bias: False\n",
      "model.decoder.layers.1.fc2.weight: False\n",
      "model.decoder.layers.1.fc2.bias: False\n",
      "model.decoder.layers.1.final_layer_norm.weight: False\n",
      "model.decoder.layers.1.final_layer_norm.bias: False\n",
      "model.decoder.layers.2.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.2.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.2.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.2.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.2.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.2.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.2.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.2.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.2.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.2.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.2.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.2.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.2.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.2.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.2.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.2.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.2.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.2.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.2.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.2.fc1.weight: False\n",
      "model.decoder.layers.2.fc1.bias: False\n",
      "model.decoder.layers.2.fc2.weight: False\n",
      "model.decoder.layers.2.fc2.bias: False\n",
      "model.decoder.layers.2.final_layer_norm.weight: False\n",
      "model.decoder.layers.2.final_layer_norm.bias: False\n",
      "model.decoder.layers.3.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.3.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.3.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.3.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.3.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.3.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.3.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.3.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.3.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.3.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.3.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.3.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.3.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.3.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.3.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.3.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.3.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.3.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.3.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.3.fc1.weight: False\n",
      "model.decoder.layers.3.fc1.bias: False\n",
      "model.decoder.layers.3.fc2.weight: False\n",
      "model.decoder.layers.3.fc2.bias: False\n",
      "model.decoder.layers.3.final_layer_norm.weight: False\n",
      "model.decoder.layers.3.final_layer_norm.bias: False\n",
      "model.decoder.layers.4.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.4.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.4.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.4.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.4.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.4.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.4.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.4.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.4.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.4.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.4.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.4.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.4.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.4.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.4.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.4.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.4.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.4.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.4.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.4.fc1.weight: False\n",
      "model.decoder.layers.4.fc1.bias: False\n",
      "model.decoder.layers.4.fc2.weight: False\n",
      "model.decoder.layers.4.fc2.bias: False\n",
      "model.decoder.layers.4.final_layer_norm.weight: False\n",
      "model.decoder.layers.4.final_layer_norm.bias: False\n",
      "model.decoder.layers.5.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.5.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.5.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.5.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.5.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.5.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.5.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.5.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.5.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.5.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.5.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.5.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.5.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.5.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.5.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.5.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.5.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.5.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.5.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.5.fc1.weight: False\n",
      "model.decoder.layers.5.fc1.bias: False\n",
      "model.decoder.layers.5.fc2.weight: False\n",
      "model.decoder.layers.5.fc2.bias: False\n",
      "model.decoder.layers.5.final_layer_norm.weight: False\n",
      "model.decoder.layers.5.final_layer_norm.bias: False\n",
      "model.decoder.layers.6.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.6.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.6.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.6.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.6.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.6.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.6.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.6.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.6.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.6.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.6.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.6.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.6.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.6.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.6.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.6.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.6.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.6.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.6.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.6.fc1.weight: False\n",
      "model.decoder.layers.6.fc1.bias: False\n",
      "model.decoder.layers.6.fc2.weight: False\n",
      "model.decoder.layers.6.fc2.bias: False\n",
      "model.decoder.layers.6.final_layer_norm.weight: False\n",
      "model.decoder.layers.6.final_layer_norm.bias: False\n",
      "model.decoder.layers.7.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.7.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.7.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.7.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.7.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.7.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.7.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.7.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.7.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.7.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.7.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.7.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.7.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.7.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.7.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.7.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.7.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.7.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.7.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.7.fc1.weight: False\n",
      "model.decoder.layers.7.fc1.bias: False\n",
      "model.decoder.layers.7.fc2.weight: False\n",
      "model.decoder.layers.7.fc2.bias: False\n",
      "model.decoder.layers.7.final_layer_norm.weight: False\n",
      "model.decoder.layers.7.final_layer_norm.bias: False\n",
      "model.decoder.layers.8.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.8.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.8.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.8.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.8.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.8.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.8.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.8.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.8.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.8.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.8.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.8.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.8.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.8.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.8.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.8.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.8.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.8.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.8.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.8.fc1.weight: False\n",
      "model.decoder.layers.8.fc1.bias: False\n",
      "model.decoder.layers.8.fc2.weight: False\n",
      "model.decoder.layers.8.fc2.bias: False\n",
      "model.decoder.layers.8.final_layer_norm.weight: False\n",
      "model.decoder.layers.8.final_layer_norm.bias: False\n",
      "model.decoder.layers.9.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.9.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.9.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.9.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.9.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.9.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.9.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.9.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.9.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.9.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.9.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.9.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.9.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.9.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.9.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.9.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.9.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.9.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.9.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.9.fc1.weight: False\n",
      "model.decoder.layers.9.fc1.bias: False\n",
      "model.decoder.layers.9.fc2.weight: False\n",
      "model.decoder.layers.9.fc2.bias: False\n",
      "model.decoder.layers.9.final_layer_norm.weight: False\n",
      "model.decoder.layers.9.final_layer_norm.bias: False\n",
      "model.decoder.layers.10.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.10.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.10.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.10.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.10.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.10.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.10.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.10.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.10.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.10.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.10.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.10.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.10.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.10.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.10.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.10.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.10.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.10.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.10.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.10.fc1.weight: False\n",
      "model.decoder.layers.10.fc1.bias: False\n",
      "model.decoder.layers.10.fc2.weight: False\n",
      "model.decoder.layers.10.fc2.bias: False\n",
      "model.decoder.layers.10.final_layer_norm.weight: False\n",
      "model.decoder.layers.10.final_layer_norm.bias: False\n",
      "model.decoder.layers.11.self_attn.k_proj.weight: False\n",
      "model.decoder.layers.11.self_attn.k_proj.bias: False\n",
      "model.decoder.layers.11.self_attn.v_proj.weight: False\n",
      "model.decoder.layers.11.self_attn.v_proj.bias: False\n",
      "model.decoder.layers.11.self_attn.q_proj.weight: False\n",
      "model.decoder.layers.11.self_attn.q_proj.bias: False\n",
      "model.decoder.layers.11.self_attn.out_proj.weight: False\n",
      "model.decoder.layers.11.self_attn.out_proj.bias: False\n",
      "model.decoder.layers.11.self_attn_layer_norm.weight: False\n",
      "model.decoder.layers.11.self_attn_layer_norm.bias: False\n",
      "model.decoder.layers.11.encoder_attn.k_proj.weight: False\n",
      "model.decoder.layers.11.encoder_attn.k_proj.bias: False\n",
      "model.decoder.layers.11.encoder_attn.v_proj.weight: False\n",
      "model.decoder.layers.11.encoder_attn.v_proj.bias: False\n",
      "model.decoder.layers.11.encoder_attn.q_proj.weight: False\n",
      "model.decoder.layers.11.encoder_attn.q_proj.bias: False\n",
      "model.decoder.layers.11.encoder_attn.out_proj.weight: False\n",
      "model.decoder.layers.11.encoder_attn.out_proj.bias: False\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.weight: False\n",
      "model.decoder.layers.11.encoder_attn_layer_norm.bias: False\n",
      "model.decoder.layers.11.fc1.weight: False\n",
      "model.decoder.layers.11.fc1.bias: False\n",
      "model.decoder.layers.11.fc2.weight: False\n",
      "model.decoder.layers.11.fc2.bias: False\n",
      "model.decoder.layers.11.final_layer_norm.weight: False\n",
      "model.decoder.layers.11.final_layer_norm.bias: False\n",
      "model.decoder.layernorm_embedding.weight: False\n",
      "model.decoder.layernorm_embedding.bias: False\n",
      "model.decoder.layer_norm.weight: False\n",
      "model.decoder.layer_norm.bias: False\n"
     ]
    }
   ],
   "source": [
    "def freeze_linear_layers(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            # Recursively freeze linear layers in children modules\n",
    "            freeze_linear_layers(child)\n",
    "\n",
    "\n",
    "def replace_linear_with_lora_recursive(model, rank, alpha, dropout):\n",
    "    if lora_encoder:\n",
    "        for layer in model.model.encoder.layers:\n",
    "            if lora_attention: \n",
    "                layer.self_attn.q_proj = LinearWithLoRAMerged(layer.self_attn.q_proj,rank,alpha,dropout)\n",
    "                layer.self_attn.k_proj = LinearWithLoRAMerged(layer.self_attn.k_proj,rank,alpha,dropout)\n",
    "                layer.self_attn.v_proj = LinearWithLoRAMerged(layer.self_attn.v_proj,rank,alpha,dropout)\n",
    "                layer.self_attn.out_proj = LinearWithLoRAMerged(layer.self_attn.out_proj,rank,alpha,dropout)\n",
    "            layer.fc1 = LinearWithLoRAMerged(layer.fc1,rank,alpha,dropout)\n",
    "            layer.fc2 = LinearWithLoRAMerged(layer.fc2,rank,alpha,dropout)\n",
    "    if lora_decoder:\n",
    "        for layer in model.model.decoder.layers:\n",
    "            if lora_attention:\n",
    "                layer.self_attn.q_proj = LinearWithLoRAMerged(layer.self_attn.q_proj,rank,alpha,dropout)\n",
    "                layer.self_attn.k_proj = LinearWithLoRAMerged(layer.self_attn.k_proj,rank,alpha,dropout)\n",
    "                layer.self_attn.v_proj = LinearWithLoRAMerged(layer.self_attn.v_proj,rank,alpha,dropout)\n",
    "                layer.self_attn.out_proj = LinearWithLoRAMerged(layer.self_attn.out_proj,rank,alpha,dropout)\n",
    "                layer.encoder_attn.q_proj = LinearWithLoRAMerged(layer.encoder_attn.q_proj,rank,alpha,dropout)\n",
    "                layer.encoder_attn.k_proj = LinearWithLoRAMerged(layer.encoder_attn.k_proj,rank,alpha,dropout)\n",
    "                layer.encoder_attn.v_proj = LinearWithLoRAMerged(layer.encoder_attn.v_proj,rank,alpha,dropout)\n",
    "                layer.encoder_attn.out_proj = LinearWithLoRAMerged(layer.encoder_attn.out_proj,rank,alpha,dropout)\n",
    "            layer.fc1 = LinearWithLoRAMerged(layer.fc1,rank,alpha,dropout)\n",
    "            layer.fc2 = LinearWithLoRAMerged(layer.fc2,rank,alpha,dropout)\n",
    "    if lora_head:\n",
    "        model.lm_head = LinearWithLoRAMerged(model.lm_head,rank,alpha,dropout)\n",
    "        \n",
    "model_lora = copy.deepcopy(model_en2vi)\n",
    "replace_linear_with_lora_recursive(model_lora,lora_rank, lora_alpha, lora_dropout)\n",
    "model_lora.to(device)\n",
    "freeze_linear_layers(model_lora)\n",
    "# Check if linear layers are frozen\n",
    "for name, param in model_lora.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                                               Parameters\n",
      "-------------------------------------------------  ------------\n",
      "model.encoder.layers.0.self_attn.k_proj.lora.A            4_096\n",
      "model.encoder.layers.0.self_attn.k_proj.lora.B            4_096\n",
      "model.encoder.layers.0.self_attn.v_proj.lora.A            4_096\n",
      "model.encoder.layers.0.self_attn.v_proj.lora.B            4_096\n",
      "model.encoder.layers.0.self_attn.q_proj.lora.A            4_096\n",
      "model.encoder.layers.0.self_attn.q_proj.lora.B            4_096\n",
      "model.encoder.layers.0.self_attn.out_proj.lora.A          4_096\n",
      "model.encoder.layers.0.self_attn.out_proj.lora.B          4_096\n",
      "model.encoder.layers.0.fc1.lora.A                         4_096\n",
      "model.encoder.layers.0.fc1.lora.B                        16_384\n",
      "model.encoder.layers.0.fc2.lora.A                        16_384\n",
      "model.encoder.layers.0.fc2.lora.B                         4_096\n",
      "model.encoder.layers.1.self_attn.k_proj.lora.A            4_096\n",
      "model.encoder.layers.1.self_attn.k_proj.lora.B            4_096\n",
      "model.encoder.layers.1.self_attn.v_proj.lora.A            4_096\n",
      "model.encoder.layers.1.self_attn.v_proj.lora.B            4_096\n",
      "model.encoder.layers.1.self_attn.q_proj.lora.A            4_096\n",
      "model.encoder.layers.1.self_attn.q_proj.lora.B            4_096\n",
      "model.encoder.layers.1.self_attn.out_proj.lora.A          4_096\n",
      "model.encoder.layers.1.self_attn.out_proj.lora.B          4_096\n",
      "model.encoder.layers.1.fc1.lora.A                         4_096\n",
      "model.encoder.layers.1.fc1.lora.B                        16_384\n",
      "model.encoder.layers.1.fc2.lora.A                        16_384\n",
      "model.encoder.layers.1.fc2.lora.B                         4_096\n",
      "model.encoder.layers.2.self_attn.k_proj.lora.A            4_096\n",
      "model.encoder.layers.2.self_attn.k_proj.lora.B            4_096\n",
      "model.encoder.layers.2.self_attn.v_proj.lora.A            4_096\n",
      "model.encoder.layers.2.self_attn.v_proj.lora.B            4_096\n",
      "model.encoder.layers.2.self_attn.q_proj.lora.A            4_096\n",
      "model.encoder.layers.2.self_attn.q_proj.lora.B            4_096\n",
      "model.encoder.layers.2.self_attn.out_proj.lora.A          4_096\n",
      "model.encoder.layers.2.self_attn.out_proj.lora.B          4_096\n",
      "model.encoder.layers.2.fc1.lora.A                         4_096\n",
      "model.encoder.layers.2.fc1.lora.B                        16_384\n",
      "model.encoder.layers.2.fc2.lora.A                        16_384\n",
      "model.encoder.layers.2.fc2.lora.B                         4_096\n",
      "model.encoder.layers.3.self_attn.k_proj.lora.A            4_096\n",
      "model.encoder.layers.3.self_attn.k_proj.lora.B            4_096\n",
      "model.encoder.layers.3.self_attn.v_proj.lora.A            4_096\n",
      "model.encoder.layers.3.self_attn.v_proj.lora.B            4_096\n",
      "model.encoder.layers.3.self_attn.q_proj.lora.A            4_096\n",
      "model.encoder.layers.3.self_attn.q_proj.lora.B            4_096\n",
      "model.encoder.layers.3.self_attn.out_proj.lora.A          4_096\n",
      "model.encoder.layers.3.self_attn.out_proj.lora.B          4_096\n",
      "model.encoder.layers.3.fc1.lora.A                         4_096\n",
      "model.encoder.layers.3.fc1.lora.B                        16_384\n",
      "model.encoder.layers.3.fc2.lora.A                        16_384\n",
      "model.encoder.layers.3.fc2.lora.B                         4_096\n",
      "model.encoder.layers.4.self_attn.k_proj.lora.A            4_096\n",
      "model.encoder.layers.4.self_attn.k_proj.lora.B            4_096\n",
      "model.encoder.layers.4.self_attn.v_proj.lora.A            4_096\n",
      "model.encoder.layers.4.self_attn.v_proj.lora.B            4_096\n",
      "model.encoder.layers.4.self_attn.q_proj.lora.A            4_096\n",
      "model.encoder.layers.4.self_attn.q_proj.lora.B            4_096\n",
      "model.encoder.layers.4.self_attn.out_proj.lora.A          4_096\n",
      "model.encoder.layers.4.self_attn.out_proj.lora.B          4_096\n",
      "model.encoder.layers.4.fc1.lora.A                         4_096\n",
      "model.encoder.layers.4.fc1.lora.B                        16_384\n",
      "model.encoder.layers.4.fc2.lora.A                        16_384\n",
      "model.encoder.layers.4.fc2.lora.B                         4_096\n",
      "model.encoder.layers.5.self_attn.k_proj.lora.A            4_096\n",
      "model.encoder.layers.5.self_attn.k_proj.lora.B            4_096\n",
      "model.encoder.layers.5.self_attn.v_proj.lora.A            4_096\n",
      "model.encoder.layers.5.self_attn.v_proj.lora.B            4_096\n",
      "model.encoder.layers.5.self_attn.q_proj.lora.A            4_096\n",
      "model.encoder.layers.5.self_attn.q_proj.lora.B            4_096\n",
      "model.encoder.layers.5.self_attn.out_proj.lora.A          4_096\n",
      "model.encoder.layers.5.self_attn.out_proj.lora.B          4_096\n",
      "model.encoder.layers.5.fc1.lora.A                         4_096\n",
      "model.encoder.layers.5.fc1.lora.B                        16_384\n",
      "model.encoder.layers.5.fc2.lora.A                        16_384\n",
      "model.encoder.layers.5.fc2.lora.B                         4_096\n",
      "model.encoder.layers.6.self_attn.k_proj.lora.A            4_096\n",
      "model.encoder.layers.6.self_attn.k_proj.lora.B            4_096\n",
      "model.encoder.layers.6.self_attn.v_proj.lora.A            4_096\n",
      "model.encoder.layers.6.self_attn.v_proj.lora.B            4_096\n",
      "model.encoder.layers.6.self_attn.q_proj.lora.A            4_096\n",
      "model.encoder.layers.6.self_attn.q_proj.lora.B            4_096\n",
      "model.encoder.layers.6.self_attn.out_proj.lora.A          4_096\n",
      "model.encoder.layers.6.self_attn.out_proj.lora.B          4_096\n",
      "model.encoder.layers.6.fc1.lora.A                         4_096\n",
      "model.encoder.layers.6.fc1.lora.B                        16_384\n",
      "model.encoder.layers.6.fc2.lora.A                        16_384\n",
      "model.encoder.layers.6.fc2.lora.B                         4_096\n",
      "model.encoder.layers.7.self_attn.k_proj.lora.A            4_096\n",
      "model.encoder.layers.7.self_attn.k_proj.lora.B            4_096\n",
      "model.encoder.layers.7.self_attn.v_proj.lora.A            4_096\n",
      "model.encoder.layers.7.self_attn.v_proj.lora.B            4_096\n",
      "model.encoder.layers.7.self_attn.q_proj.lora.A            4_096\n",
      "model.encoder.layers.7.self_attn.q_proj.lora.B            4_096\n",
      "model.encoder.layers.7.self_attn.out_proj.lora.A          4_096\n",
      "model.encoder.layers.7.self_attn.out_proj.lora.B          4_096\n",
      "model.encoder.layers.7.fc1.lora.A                         4_096\n",
      "model.encoder.layers.7.fc1.lora.B                        16_384\n",
      "model.encoder.layers.7.fc2.lora.A                        16_384\n",
      "model.encoder.layers.7.fc2.lora.B                         4_096\n",
      "model.encoder.layers.8.self_attn.k_proj.lora.A            4_096\n",
      "model.encoder.layers.8.self_attn.k_proj.lora.B            4_096\n",
      "model.encoder.layers.8.self_attn.v_proj.lora.A            4_096\n",
      "model.encoder.layers.8.self_attn.v_proj.lora.B            4_096\n",
      "model.encoder.layers.8.self_attn.q_proj.lora.A            4_096\n",
      "model.encoder.layers.8.self_attn.q_proj.lora.B            4_096\n",
      "model.encoder.layers.8.self_attn.out_proj.lora.A          4_096\n",
      "model.encoder.layers.8.self_attn.out_proj.lora.B          4_096\n",
      "model.encoder.layers.8.fc1.lora.A                         4_096\n",
      "model.encoder.layers.8.fc1.lora.B                        16_384\n",
      "model.encoder.layers.8.fc2.lora.A                        16_384\n",
      "model.encoder.layers.8.fc2.lora.B                         4_096\n",
      "model.encoder.layers.9.self_attn.k_proj.lora.A            4_096\n",
      "model.encoder.layers.9.self_attn.k_proj.lora.B            4_096\n",
      "model.encoder.layers.9.self_attn.v_proj.lora.A            4_096\n",
      "model.encoder.layers.9.self_attn.v_proj.lora.B            4_096\n",
      "model.encoder.layers.9.self_attn.q_proj.lora.A            4_096\n",
      "model.encoder.layers.9.self_attn.q_proj.lora.B            4_096\n",
      "model.encoder.layers.9.self_attn.out_proj.lora.A          4_096\n",
      "model.encoder.layers.9.self_attn.out_proj.lora.B          4_096\n",
      "model.encoder.layers.9.fc1.lora.A                         4_096\n",
      "model.encoder.layers.9.fc1.lora.B                        16_384\n",
      "model.encoder.layers.9.fc2.lora.A                        16_384\n",
      "model.encoder.layers.9.fc2.lora.B                         4_096\n",
      "model.encoder.layers.10.self_attn.k_proj.lora.A           4_096\n",
      "model.encoder.layers.10.self_attn.k_proj.lora.B           4_096\n",
      "model.encoder.layers.10.self_attn.v_proj.lora.A           4_096\n",
      "model.encoder.layers.10.self_attn.v_proj.lora.B           4_096\n",
      "model.encoder.layers.10.self_attn.q_proj.lora.A           4_096\n",
      "model.encoder.layers.10.self_attn.q_proj.lora.B           4_096\n",
      "model.encoder.layers.10.self_attn.out_proj.lora.A         4_096\n",
      "model.encoder.layers.10.self_attn.out_proj.lora.B         4_096\n",
      "model.encoder.layers.10.fc1.lora.A                        4_096\n",
      "model.encoder.layers.10.fc1.lora.B                       16_384\n",
      "model.encoder.layers.10.fc2.lora.A                       16_384\n",
      "model.encoder.layers.10.fc2.lora.B                        4_096\n",
      "model.encoder.layers.11.self_attn.k_proj.lora.A           4_096\n",
      "model.encoder.layers.11.self_attn.k_proj.lora.B           4_096\n",
      "model.encoder.layers.11.self_attn.v_proj.lora.A           4_096\n",
      "model.encoder.layers.11.self_attn.v_proj.lora.B           4_096\n",
      "model.encoder.layers.11.self_attn.q_proj.lora.A           4_096\n",
      "model.encoder.layers.11.self_attn.q_proj.lora.B           4_096\n",
      "model.encoder.layers.11.self_attn.out_proj.lora.A         4_096\n",
      "model.encoder.layers.11.self_attn.out_proj.lora.B         4_096\n",
      "model.encoder.layers.11.fc1.lora.A                        4_096\n",
      "model.encoder.layers.11.fc1.lora.B                       16_384\n",
      "model.encoder.layers.11.fc2.lora.A                       16_384\n",
      "model.encoder.layers.11.fc2.lora.B                        4_096\n",
      "-------------------------------------------------  ------------\n",
      "TOTAL                                                   884_736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "def format_with_underscore(n):\n",
    "    \"\"\"Mini helper function to format a number with underscore as thousand separator\"\"\"\n",
    "    return f\"{n:_}\"\n",
    "def parameter_count_table(model, output_file_path=None, output_print=True, add_dtypes=False, show_nograd_paras=False):\n",
    "    \n",
    "    table = [[\"Module\", \"Parameters\"]]\n",
    "    if add_dtypes:\n",
    "        table = [[\"Module\", \"Parameters\", \"dtype\"]]\n",
    "    total_params = 0\n",
    "    max_len = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if (not parameter.requires_grad) and (not show_nograd_paras): continue\n",
    "        params = parameter.numel()\n",
    "        formatted_params = format_with_underscore(params)\n",
    "        max_len = max(max_len, len(formatted_params))\n",
    "        if add_dtypes:\n",
    "            table.append([str(name), formatted_params, parameter.dtype])\n",
    "        else:\n",
    "            table.append([str(name), formatted_params])\n",
    "        total_params += params\n",
    "\n",
    "    table.append(tabulate.SEPARATING_LINE)\n",
    "\n",
    "    formatted_total = format_with_underscore(total_params)\n",
    "    max_len = max(max_len, len(formatted_total))\n",
    "    if add_dtypes:\n",
    "        table.append([\"TOTAL\", formatted_total])\n",
    "    else:\n",
    "        table.append([\"TOTAL\", formatted_total, ''])\n",
    "\n",
    "    # Right align the numbers in the table\n",
    "    for row in table[1:]:\n",
    "        if row is not tabulate.SEPARATING_LINE:\n",
    "            row[1] = row[1].rjust(max_len)\n",
    "\n",
    "    tabulated_table = tabulate.tabulate(table, headers=\"firstrow\")\n",
    "    if output_file_path is not None:\n",
    "        with open(output_file_path, 'w') as f:\n",
    "            f.write(tabulated_table)\n",
    "    if output_print:\n",
    "        print(tabulated_table)\n",
    "        print(\"\")\n",
    "parameter_count_table(model_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 610,879,488\n",
      "Trainable Parameters: 610,879,488\n",
      "Frozen Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration\n",
    "\n",
    "# Load the mBART model\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')\n",
    "\n",
    "# Function to count total parameters, trainable parameters, and frozen parameters\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.model.parameters())  # Total parameters\n",
    "    trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)  # Trainable parameters\n",
    "    frozen_params = total_params - trainable_params  # Frozen parameters\n",
    "\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters: {frozen_params:,}\")\n",
    "\n",
    "# Apply the function to count parameters in the mBART model\n",
    "count_parameters(mbart_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder - Total Parameters: 408,264,704\n",
      "Encoder - Trainable Parameters: 408,264,704\n",
      "Encoder - Frozen Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration\n",
    "\n",
    "# Load the mBART model\n",
    "mbart_model = MBartForConditionalGeneration.from_pretrained('facebook/mbart-large-50-many-to-many-mmt')\n",
    "# Function to count total parameters, trainable parameters, and frozen parameters in the encoder\n",
    "def count_encoder_parameters(model):\n",
    "\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.model.encoder.parameters())  # Total encoder parameters\n",
    "    trainable_params = sum(p.numel() for p in model.model.encoder.parameters() if p.requires_grad)  # Trainable encoder parameters\n",
    "    frozen_params = total_params - trainable_params  # Frozen encoder parameters\n",
    "\n",
    "    print(f\"Encoder - Total Parameters: {total_params:,}\")\n",
    "    print(f\"Encoder - Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Encoder - Frozen Parameters: {frozen_params:,}\")\n",
    "\n",
    "# Apply the function to count parameters in the encoder of the mBART model\n",
    "count_encoder_parameters(mbart_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
