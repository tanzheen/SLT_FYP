{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file ../mbart_model/config.json\n",
      "Model config MBartConfig {\n",
      "  \"_name_or_path\": \"facebook/mbart-large-50-many-to-many-mmt\",\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 200,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "loading weights file ../mbart_model/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing MBartForConditionalGeneration.\n",
      "\n",
      "All the weights of MBartForConditionalGeneration were initialized from the model checkpoint at ../mbart_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MBartForConditionalGeneration for predictions without further training.\n",
      "loading configuration file ../mbart_model/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2, 250004,  35378,      4,   3642,    621,    398,     32,      2,\n",
      "              1,      1],\n",
      "        [     2, 250004,   4865,    765,    398,   2809,   1257,     47,  78684,\n",
      "             32,      2],\n",
      "        [     2, 250004,    984,    398,   3444,     47,    738,    100,     10,\n",
      "          11675,     32]])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m forced_bos_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mlang_code_to_id[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzh_CN\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Perform inference with the model to generate translations\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, attention_mask\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     29\u001b[0m                          forced_bos_token_id\u001b[38;5;241m=\u001b[39mforced_bos_token_id)\n\u001b[1;32m     31\u001b[0m predictions \u001b[38;5;241m=\u001b[39m [] \n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outputs)): \n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import torch \n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers.models.mbart.modeling_mbart import shift_tokens_right\n",
    "# Load pre-trained MBART model and tokenizer (MBART-50 for multilingual tasks)\n",
    "model_name = \"../mbart_model\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Example batch of input sentences in various languages\n",
    "batch_sentences = [\n",
    "    \"Hello, how are you?\",   # English\n",
    "    \"What have you been up to recently?\", # French\n",
    "    \"Do you want to go for a run?\",    # Spanish\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the input batch of sentences\n",
    "inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs = shift_tokens_right(inputs.input_ids, tokenizer.pad_token_id)\n",
    "print(inputs )\n",
    "# Generate translations (for example, to French) or any other target language\n",
    "# Specify the target language for the model to generate in\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"zh_CN\"]\n",
    "\n",
    "# Perform inference with the model to generate translations\n",
    "outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], \n",
    "                         forced_bos_token_id=forced_bos_token_id)\n",
    "\n",
    "predictions = [] \n",
    "for i in range(len(outputs)): \n",
    "    predictions.append(outputs[i, :])\n",
    "\n",
    "pad_tensor = torch.ones(200-len(predictions[0]))\n",
    "predictions[0] = torch.cat((predictions[0],pad_tensor.long()),dim = 0)\n",
    "predictions = pad_sequence(predictions,batch_first=True,padding_value=1)\n",
    "\n",
    "# Decode the generated outputs back to text\n",
    "translated_sentences = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "# Print the generated translations\n",
    "for i, translation in enumerate(translated_sentences):\n",
    "    print(f\"Original: {batch_sentences[i]}\")\n",
    "    print(f\"Translated: {translation}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM adaptor 2 shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After padding: torch.Size([10, 50, 32])\n",
      "After initial projection: torch.Size([10, 50, 512])\n",
      "After permute (before first conv): torch.Size([10, 512, 50])\n",
      "After first conv block: torch.Size([10, 512, 23])\n",
      "After intermediate projection: torch.Size([10, 23, 1024])\n",
      "After permute (before second conv): torch.Size([10, 1024, 23])\n",
      "After second conv block: torch.Size([10, 1024, 9])\n",
      "Final output shape: torch.Size([10, 9, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define a dummy PAD_IDX for padding purposes\n",
    "PAD_IDX = 0\n",
    "\n",
    "class LLMAdapter2(nn.Module):\n",
    "    '''\n",
    "    LLM adapter aims to capture temporal relations and transform 32 tokens into 1024 tokens.\n",
    "    This version introduces an additional projection layer between the two convolution layers.\n",
    "    '''\n",
    "    def __init__(self, num_tokens=32, hidden_dim=1024, kernel_size=5):\n",
    "        super(LLMAdapter2, self).__init__()\n",
    "        \n",
    "        # Store parameters\n",
    "        self.num_tokens = num_tokens\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # First projection from input tokens to hidden_dim/2\n",
    "        self.proj = nn.Linear(self.num_tokens, self.hidden_dim // 2)\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv1d(self.hidden_dim // 2, self.hidden_dim // 2, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.hidden_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=2, ceil_mode=False)\n",
    "        )\n",
    "\n",
    "        # New projection layer between convolution layers\n",
    "        self.intermediate_proj = nn.Linear(self.hidden_dim // 2, self.hidden_dim)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv1d(self.hidden_dim, self.hidden_dim, kernel_size=kernel_size, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=2, ceil_mode=False)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, src_length):\n",
    "        # Input shape: (batch_size, num_frames, num_tokens)\n",
    "        \n",
    "        # Split the input into individual batches according to src_length\n",
    "        start = 0\n",
    "        x_batch = []\n",
    "        for length in src_length:\n",
    "            end = start + length\n",
    "            x_batch.append(x[start:end])\n",
    "            start = end\n",
    "        \n",
    "        # Pad sequences to ensure uniform batch sizes\n",
    "        x = pad_sequence(x_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "        print(f\"After padding: {x.shape}\")  # Check padding result (batch_size, num_frames, num_tokens)\n",
    "        \n",
    "        # Apply the initial projection layer\n",
    "        x = self.proj(x)  # Shape: (batch_size, num_frames, hidden_dim / 2)\n",
    "        print(f\"After initial projection: {x.shape}\")  # Should be (batch_size, num_frames, 512)\n",
    "        \n",
    "        # Permute to (batch_size, hidden_dim / 2, num_frames) for Conv1d\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After permute (before first conv): {x.shape}\")  # Should be (batch_size, 512, num_frames)\n",
    "        \n",
    "        # First convolutional block\n",
    "        x = self.conv_block_1(x)  # Shape: (batch_size, hidden_dim / 2, reduced_num_frames)\n",
    "        print(f\"After first conv block: {x.shape}\")  # Check after first conv\n",
    "        \n",
    "        # Apply the intermediate projection layer\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch_size, reduced_num_frames, hidden_dim / 2)\n",
    "        x = self.intermediate_proj(x)  # Shape: (batch_size, reduced_num_frames, hidden_dim)\n",
    "        print(f\"After intermediate projection: {x.shape}\")  # Should be (batch_size, reduced_num_frames, 1024)\n",
    "        x = x.permute(0, 2, 1)  # Back to (batch_size, hidden_dim, reduced_num_frames)\n",
    "        print(f\"After permute (before second conv): {x.shape}\")  # Check before second conv\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = self.conv_block_2(x)  # Shape: (batch_size, hidden_dim, further_reduced_num_frames)\n",
    "        print(f\"After second conv block: {x.shape}\")  # Check after second conv\n",
    "        \n",
    "        # Convert back to (batch_size, further_reduced_num_frames, hidden_dim)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"Final output shape: {x.shape}\")  # Should be (batch_size, further_reduced_num_frames, 1024)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of LLMAdapter2\n",
    "model = LLMAdapter2()\n",
    "\n",
    "# Test input\n",
    "batch_size = 10\n",
    "num_frames = 50  # Let's assume each sequence has 15 frames\n",
    "num_tokens = 32  # As specified in the model\n",
    "\n",
    "# Random test tensor simulating a batch of 10 sequences, each with 15 frames and 32 tokens\n",
    "test_input = torch.rand((batch_size * num_frames, num_tokens))\n",
    "\n",
    "# Source lengths for each batch (assuming all sequences have 15 frames)\n",
    "src_length = torch.tensor([num_frames] * batch_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(test_input, src_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing LLM adaptor 3 shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before padding: torch.Size([500, 32])\n",
      "After padding: torch.Size([10, 50, 32])\n",
      "After permute: torch.Size([10, 32, 50])\n",
      "After temporal_conv: torch.Size([10, 128, 42])\n",
      "After second permute: torch.Size([10, 42, 128])\n",
      "After final_proj: torch.Size([10, 42, 1024])\n",
      "before out shape : torch.Size([10, 42, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Define a dummy PAD_IDX for this example\n",
    "PAD_IDX = 0\n",
    "\n",
    "class LLMAdapter3(nn.Module):\n",
    "    def __init__(self, num_tokens=32, hidden_dim=1024, kernel_size=5):\n",
    "        super(LLMAdapter3, self).__init__()\n",
    "        self.num_tokens = num_tokens\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Temporal convolution over the time dimension\n",
    "        self.temporal_conv = nn.Sequential(\n",
    "            nn.Conv1d(self.num_tokens, self.num_tokens * 2, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.num_tokens * 2),  # Channels must match Conv1d output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Reduce kernel size for pooling to avoid sequence collapse\n",
    "            nn.AvgPool1d(kernel_size=1, ceil_mode=False),  \n",
    "\n",
    "            nn.Conv1d(self.num_tokens * 2, self.num_tokens * 4, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(self.num_tokens * 4),  # Channels must match Conv1d output channels\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool1d(kernel_size=1, ceil_mode=False)  # Adjusted pooling to avoid reducing size to zero\n",
    "        )\n",
    "        \n",
    "        # Final projection layer\n",
    "        self.final_proj = nn.Sequential(\n",
    "            nn.Linear(self.num_tokens * 4, self.hidden_dim)\n",
    "        )\n",
    "        self.out = nn.Sequential(nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x, src_length):\n",
    "        start = 0\n",
    "        x_batch = []\n",
    "        for length in src_length:\n",
    "            end = start + length\n",
    "            x_batch.append(x[start:end])\n",
    "            start = end\n",
    "        print(f\"Before padding: {x.shape}\") \n",
    "        x = pad_sequence(x_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "        print(f\"After padding: {x.shape}\")  # Print shape after padding\n",
    "        \n",
    "        # Permute to match Conv1d expected shape: (batch_size, channels, sequence_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After permute: {x.shape}\")  # Shape should now be (batch_size, num_tokens, num_frames)\n",
    "        \n",
    "        # Apply temporal convolution\n",
    "        x = self.temporal_conv(x)\n",
    "        print(f\"After temporal_conv: {x.shape}\")  # Check shape after convolution\n",
    "        \n",
    "        # Permute back to (batch_size, sequence_length, hidden_dim)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        print(f\"After second permute: {x.shape}\")  # Shape should be (batch_size, num_frames, num_tokens*4)\n",
    "        \n",
    "        # Apply final projection (we need to flatten or reshape input to match Linear input requirements)\n",
    "        batch_size, seq_len, hidden_dim = x.shape\n",
    "        x = self.final_proj(x)\n",
    "        #x = self.final_proj(x.reshape(batch_size * seq_len, hidden_dim))\n",
    "        print(f\"After final_proj: {x.shape}\")  # Check final shape\n",
    "\n",
    "        print(f\"before out shape : {x.shape}\")\n",
    "        x = self.out(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "# Create an instance of LLMAdapter3\n",
    "model = LLMAdapter3()\n",
    "\n",
    "# Test input\n",
    "batch_size = 10\n",
    "num_frames = 50 # Let's assume each sequence has 15 frames\n",
    "num_tokens = 32  # As specified in the model\n",
    "\n",
    "# Random test tensor simulating a batch of 10 sequences, each with 15 frames and 32 tokens\n",
    "test_input = torch.rand((batch_size * num_frames, num_tokens))\n",
    "\n",
    "# Source lengths for each batch (assuming all sequences have 15 frames)\n",
    "src_length = torch.tensor([num_frames] * batch_size)\n",
    "\n",
    "# Forward pass\n",
    "output = model(test_input, src_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tokens dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test some generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from omegaconf import OmegaConf\n",
    "import torch.distributed\n",
    "from train_sign_utils import * \n",
    "import os \n",
    "from accelerate import Accelerator\n",
    "from logger import setup_logger\n",
    "from accelerate.utils import set_seed\n",
    "import sys \n",
    "from transformers import MBart50Tokenizer\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/24 22:58:22 Sign2Text]: \u001b[0mCreating model and loss module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titok weights loaded successfully from: TiTok_weights/ema_model/pytorch_model.bin\n",
      "TiTok weights are frozen chowwww!\n",
      "MBart encoder weights are graded!\n",
      "MBart decoder weights are frozen!\n",
      "Number of trainable parameters: 152209408\n",
      "\u001b[32m[10/24 22:58:37 Sign2Text]: \u001b[0mloading weight from ./frozen_sign2text/ema_model/pytorch_model.bin, msg: <All keys matched successfully>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10/24 22:58:39 Sign2Text]: \u001b[0mCreating Signloaders. Batch_size = 1\n",
      "train dataloader done!\n",
      "dev dataloader done!\n",
      "train dataloader done!\n"
     ]
    }
   ],
   "source": [
    "## Take in a configuration \n",
    "import train_sign_utils\n",
    "import signdata\n",
    "import seq_model\n",
    "from imp import reload\n",
    "reload(seq_model)\n",
    "reload(signdata)\n",
    "reload(train_sign_utils)\n",
    "from train_sign_utils import create_model, create_signloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = OmegaConf.load(\"./configs/Sign2Text_CSL_config_v3.yaml\")\n",
    "\n",
    "\n",
    "output_dir = config.experiment.output_dir\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "config.experiment.logging_dir = os.path.join(output_dir, \"logs\")\n",
    "# Load the model \n",
    "accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=config.training.gradient_accumulation_steps,\n",
    "        mixed_precision=config.training.mixed_precision,\n",
    "        project_dir=config.experiment.logging_dir,\n",
    "        split_batches=False\n",
    "    )\n",
    "\n",
    "\n",
    "logger = setup_logger(name=\"Sign2Text\", log_level=\"INFO\",\n",
    "    output_file=f\"{output_dir}/log{accelerator.process_index}.txt\")\n",
    "\n",
    "# We need to initialize the trackers we use, and also store our configuration.\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if config.training.seed is not None:\n",
    "    set_seed(config.training.seed, device_specific=True)\n",
    "\n",
    "# Create model \n",
    "model, ema_model = create_model(config, logger, accelerator)\n",
    "# Create signloaders \n",
    "tokenizer = MBart50Tokenizer.from_pretrained(config.training.tokenizer,\n",
    "                                            src_lang=config.dataset.lang,\n",
    "                                              tgt_lang= config.dataset.lang)\n",
    "train_dataloader, dev_dataloader, test_dataloader = create_signloader(config, logger, accelerator, tokenizer, \"cpu\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating!:   0%|          | 0/18401 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 32])\n",
      "tensor([120], device='cuda:0')\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 11])\n",
      "torch.Size([1, 27])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating!:   0%|          | 0/18401 [00:13<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader.dataset[0][1].shape\n",
    "\n",
    "for i, (src, tgt) in enumerate(tqdm(train_dataloader, desc=f\"Generating!\")):\n",
    "\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']\n",
    "    print(batch.shape)\n",
    "    print(src_length)\n",
    "    print(tgt_attn.shape)\n",
    "    print(tgt_input.shape)\n",
    "    print(input_attn.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_images(model, images, tgt_labels, input_attn, src_length, config, accelerator,  logger, tokenizer): \n",
    "\n",
    "    logger.info(\"Translating images...\")\n",
    "    model = accelerator.unwrap_model(model).to(\"cuda\")\n",
    "    images = torch.clone(images)\n",
    "    \n",
    "    # Set appropriate dtype based on mixed precision\n",
    "    dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "\n",
    "    with torch.no_grad(): \n",
    "\n",
    "        # Directly generate translations using model.generate\n",
    "        output = model.generate(\n",
    "            src_input=images, \n",
    "            src_attn=input_attn, \n",
    "            src_length=src_length,\n",
    "            max_new_tokens=150, \n",
    "            num_beams=4, \n",
    "            decoder_start_token_id=tokenizer.lang_code_to_id[config.dataset.lang]\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Use tokenizer to decode generated token IDs to translations\n",
    "        pred_translations = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "        # Decode the target labels (ground truth)\n",
    "        gt_translations = tokenizer.batch_decode(tgt_labels, skip_special_tokens=True)\n",
    "\n",
    "    \n",
    "    return pred_translations, gt_translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating!:   0%|          | 0/18401 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape  torch.Size([96, 3, 256, 256])\n",
      "input attn shape torch.Size([1, 21])\n",
      "tgt attn shape torch.Size([1, 11])\n",
      "src length shape torch.Size([1])\n",
      "tgt input shape torch.Size([1, 11])\n",
      "fw_out \n",
      "\u001b[32m[10/24 22:24:31 Sign2Text]: \u001b[0mTranslating images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating!:   0%|          | 0/18401 [00:32<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m     fw_out \u001b[38;5;241m=\u001b[39m model( src_input \u001b[38;5;241m=\u001b[39m original_images,tgt_input \u001b[38;5;241m=\u001b[39m tgt_input, src_attn\u001b[38;5;241m=\u001b[39minput_attn, tgt_attn \u001b[38;5;241m=\u001b[39m tgt_attn, src_length \u001b[38;5;241m=\u001b[39m src_length)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfw_out \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m     pred, gt \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtgt_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 17\u001b[0m, in \u001b[0;36mtranslate_images\u001b[1;34m(model, images, tgt_labels, input_attn, src_length, config, accelerator, logger, tokenizer)\u001b[0m\n\u001b[0;32m     12\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Directly generate translations using model.generate\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_attn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang_code_to_id\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Use tokenizer to decode generated token IDs to translations\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     pred_translations \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32me:\\SLT_FYP\\final_product\\seq_model.py:140\u001b[0m, in \u001b[0;36mSignModel.generate\u001b[1;34m(self, src_input, src_attn, src_length, max_new_tokens, num_beams, decoder_start_token_id)\u001b[0m\n\u001b[0;32m    138\u001b[0m encoded_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtitok\u001b[38;5;241m.\u001b[39mencode(x\u001b[38;5;241m=\u001b[39msrc_input)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_encoding_indices\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    139\u001b[0m hidden_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapter(encoded_tokens\u001b[38;5;241m.\u001b[39mfloat(), src_length)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m--> 140\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMbart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msrc_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_tokens\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1745\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_attention_mask_for_generation(\n\u001b[0;32m   1740\u001b[0m         inputs_tensor, generation_config\u001b[38;5;241m.\u001b[39m_pad_token_tensor, generation_config\u001b[38;5;241m.\u001b[39m_eos_token_tensor\n\u001b[0;32m   1741\u001b[0m     )\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   1744\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1746\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[0;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:549\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[0;32m    547\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    548\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 549\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:917\u001b[0m, in \u001b[0;36mMBartEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    915\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43minputs_embeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "# Load the dataset and dataloader \n",
    "for i, (src, tgt) in enumerate(tqdm(train_dataloader, desc=f\"Generating!\")):\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    batch = src['input_ids']\n",
    "    src_length = src['src_length_batch']\n",
    "    tgt_attn = tgt.attention_mask\n",
    "    tgt_input = tgt['input_ids']\n",
    "    input_attn = src['attention_mask']\n",
    "\n",
    "    images = batch.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "    #print(f\"imagges type: {images.type()}\")\n",
    "    tgt_input = tgt['input_ids'].to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    input_attn = input_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    tgt_attn = tgt_attn.to(\n",
    "            accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "        )\n",
    "    \n",
    "    original_images= torch.clone(images)\n",
    "    print(\"images shape \", original_images.shape)\n",
    "    print(\"input attn shape\", input_attn.shape)\n",
    "    print(\"tgt attn shape\", tgt_attn.shape)\n",
    "    print(\"src length shape\", src_length.shape)\n",
    "    print(\"tgt input shape\", tgt_input.shape)\n",
    "    # Save a batch of translated images to check by reading\n",
    "\n",
    "    fw_out = model( src_input = original_images,tgt_input = tgt_input, src_attn=input_attn, tgt_attn = tgt_attn, src_length = src_length)\n",
    "    print(f\"fw_out \")\n",
    "    pred, gt = translate_images(\n",
    "        model=model,\n",
    "        images=images,\n",
    "        tgt_labels=tgt_input,\n",
    "        input_attn=input_attn, \n",
    "        src_length=src_length,\n",
    "        config=config,\n",
    "        accelerator=accelerator,\n",
    "        logger=logger, \n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "    break \n",
    "\n",
    "\n",
    "print(f\"predictions: {pred}\")\n",
    "print(f\"ground truth: {gt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignModel(\n",
       "  (titok): TiTok(\n",
       "    (encoder): TiTokEncoder(\n",
       "      (patch_embed): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n",
       "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (conv_out): Conv2d(1024, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (decoder): TiTokDecoder(\n",
       "      (decoder_embed): Linear(in_features=12, out_features=1024, bias=True)\n",
       "      (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (transformer): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (ffn): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Tanh()\n",
       "        (2): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (conv_out): Identity()\n",
       "    )\n",
       "    (quantize): VectorQuantizer(\n",
       "      (embedding): Embedding(4096, 12)\n",
       "    )\n",
       "  )\n",
       "  (Mbart): MBartForConditionalGeneration(\n",
       "    (model): MBartModel(\n",
       "      (shared): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "      (encoder): MBartEncoder(\n",
       "        (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "        (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x MBartEncoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): ReLU()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): MBartDecoder(\n",
       "        (embed_tokens): MBartScaledWordEmbedding(250054, 1024, padding_idx=1)\n",
       "        (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x MBartDecoderLayer(\n",
       "            (self_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MBartAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=1024, out_features=250054, bias=False)\n",
       "  )\n",
       "  (adapter): LLMAdapter3(\n",
       "    (temporal_conv): Sequential(\n",
       "      (0): Conv1d(32, 64, kernel_size=(5,), stride=(1,))\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (4): Conv1d(64, 128, kernel_size=(5,), stride=(1,))\n",
       "      (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    )\n",
       "    (final_proj): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "    )\n",
       "    (out): Sequential(\n",
       "      (0): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking LLM adaptation and linking it to gloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Reading in the labelled annotations\n",
    "import pickle\n",
    "def read_CSL_annotations(CSL_annot_path):\n",
    "    with open(CSL_annot_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "train_labels = read_CSL_annotations(\"../../CSL-Daily/sentence_label/processed/labels_train.pkl\") \n",
    "dev_labels = read_CSL_annotations(\"../../CSL-Daily/sentence_label/processed/labels_dev.pkl\")\n",
    "test_labels = read_CSL_annotations(\"../../CSL-Daily/sentence_label/processed/labels_test.pkl\")\n",
    "combined_labels = read_CSL_annotations(\"../../CSL-Daily/sentence_label/csl2020ct_v2.pkl\")\n",
    "\n",
    "def find_entry_by_name(data, name):\n",
    "    # Iterate through the list of dictionaries in 'info'\n",
    "    for entry in data['info']:\n",
    "        if entry['name'] == name:\n",
    "            return entry\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_gloss(combined_annotations, name): \n",
    "  # find entry in combined_annotations \n",
    "  entry = find_entry_by_name(combined_labels, name)\n",
    "  gloss_entry = entry['label_gloss']\n",
    "  return gloss_entry\n",
    "\n",
    "\n",
    "def match_frames_w_gloss(tensor_lst, gloss): \n",
    "  # divide the frames into glosses\n",
    "  num_tensors = len(tensor_lst)\n",
    "  num_gloss = len(gloss)\n",
    "  tensors_per_gloss = num_tensors//num_gloss\n",
    "\n",
    "  # create a dictionary to hold the frames referring to a gloss\n",
    "  gloss_dict = {}\n",
    "  for i, g in enumerate(gloss):\n",
    "    if i == num_gloss-1: \n",
    "      gloss_dict[g] = [tensor_lst[i*tensors_per_gloss:]]\n",
    "    else: \n",
    "      gloss_dict[g] = [tensor_lst[i*tensors_per_gloss:(i+1)*tensors_per_gloss]]\n",
    "  \n",
    "  return gloss_dict\n",
    "\n",
    "def find_and_combine_glossdicts(phase, num_samples = None , name_lst= None , dir=\"../../CSL-Daily/sentence/frames_512x512\"):\n",
    "  gloss_dict_lst = {}\n",
    "  ## make assertions to prevent error \n",
    "  assert phase in ['train', 'dev', 'test'], \"Phase must be either train, dev or test\"\n",
    "  assert num_samples is not None or name_lst is not None, \"Either number of samples or name list must be provided\"\n",
    "\n",
    "  if name_lst is None:\n",
    "    video_lst = os.listdir(f\"{dir}/{phase}\")\n",
    "    # Sample random number of videos\n",
    "    name_lst =  random.sample(video_lst, num_samples)\n",
    "\n",
    "  ## Gather various gloss dicts \n",
    "  for name in name_lst: \n",
    "    gloss = get_gloss(combined_labels, name)\n",
    "    tensor_lst = gather_vid_emb(name, phase)\n",
    "    gloss_dict_lst[name] = match_frames_w_gloss(tensor_lst, gloss)\n",
    "\n",
    "  # Comebine gloss dicts \n",
    "  combined_gloss_dict = {}\n",
    "  for name in name_lst: \n",
    "    for k, v in gloss_dict_lst[name].items(): \n",
    "      if k in combined_gloss_dict: \n",
    "        combined_gloss_dict[k].extend(v)\n",
    "      else: \n",
    "        combined_gloss_dict[k] = v\n",
    "  return gloss_dict_lst, combined_gloss_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapting train!:   0%|          | 3/18401 [08:04<887:43:20, 173.70s/it]"
     ]
    }
   ],
   "source": [
    "'''Checking after LLM adaptation'''\n",
    " \n",
    "# Separate TikTok model and LLM adapter for testing\n",
    "titok = model.titok\n",
    "adapter = model.adapter\n",
    "titok.eval()\n",
    "adapter.eval()\n",
    "# run the video frames through the TikTok model \n",
    "# Load the dataset and dataloader \n",
    "with torch.no_grad():\n",
    "    for i, (src, tgt) in enumerate(tqdm(train_dataloader, desc=f\"adapting train!\")):\n",
    "        phase = \"train\"\n",
    "        titok.to(\"cuda\")\n",
    "        adapter.to(\"cuda\")\n",
    "        batch = src['input_ids']\n",
    "        src_length = src['src_length_batch']\n",
    "        tgt_attn = tgt.attention_mask\n",
    "        tgt_input = tgt['input_ids']\n",
    "        input_attn = src['attention_mask']\n",
    "\n",
    "        images = batch.to(\n",
    "                    accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "                )\n",
    "        #print(f\"imagges type: {images.type()}\")\n",
    "        tgt_input = tgt['input_ids'].to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "        input_attn = input_attn.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "        tgt_attn = tgt_attn.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "        \n",
    "        original_images= torch.clone(images)\n",
    "        # print(f\"names: {src['name_batch']}\")\n",
    "        # print(\"images shape \", original_images.shape)\n",
    "        # print(\"input attn shape\", input_attn.shape)\n",
    "        # print(\"tgt attn shape\", tgt_attn.shape)\n",
    "        # print(\"src length shape\", src_length.shape)\n",
    "        # print(\"tgt input shape\", tgt_input.shape)\n",
    "        # Save a batch of translated images to check by reading\n",
    "        encoded_tokens = titok.encode(x=original_images)[1]['min_encoding_indices'].squeeze()\n",
    "        #print(f\"encoded tokens shape: {encoded_tokens.shape}\")\n",
    "        hidden_values = adapter(encoded_tokens.float(), src_length)\n",
    "        #print(f\"hidden values shape: {hidden_values.shape}\")\n",
    "\n",
    "        for i,  hid_val in enumerate(hidden_values): \n",
    "            hid_val = hid_val[input_attn[i]==1]\n",
    "            #print(f\"hid val: {hid_val.shape}\")\n",
    "\n",
    "            for j, token in enumerate(hid_val): \n",
    "                # save the token as a .pth file \n",
    "                path = os.path.join(config.dataset.img_path,phase )\n",
    "                final_path = os.path.join(path, src['name_batch'][i])\n",
    "                #print(final_path)\n",
    "                torch.save(token, f\"{final_path}/aft_adapter_{j}.pth\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Save newly compressed and temporal conv as .pth files and also consider the gloss used for the translation\n",
    "    for i, (src, tgt) in enumerate(tqdm(dev_dataloader, desc=f\"adapting dev!\")):\n",
    "        phase = \"dev\"\n",
    "        titok.to(\"cuda\")\n",
    "        adapter.to(\"cuda\")\n",
    "        batch = src['input_ids']\n",
    "        src_length = src['src_length_batch']\n",
    "        tgt_attn = tgt.attention_mask\n",
    "        tgt_input = tgt['input_ids']\n",
    "        input_attn = src['attention_mask']\n",
    "\n",
    "        images = batch.to(\n",
    "                    accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "                )\n",
    "        #print(f\"imagges type: {images.type()}\")\n",
    "        tgt_input = tgt['input_ids'].to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "        input_attn = input_attn.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "        tgt_attn = tgt_attn.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "        \n",
    "        original_images= torch.clone(images)\n",
    "        # print(f\"names: {src['name_batch']}\")\n",
    "        # print(\"images shape \", original_images.shape)\n",
    "        # print(\"input attn shape\", input_attn.shape)\n",
    "        # print(\"tgt attn shape\", tgt_attn.shape)\n",
    "        # print(\"src length shape\", src_length.shape)\n",
    "        # print(\"tgt input shape\", tgt_input.shape)\n",
    "        # Save a batch of translated images to check by reading\n",
    "        encoded_tokens = titok.encode(x=original_images)[1]['min_encoding_indices'].squeeze()\n",
    "        #print(f\"encoded tokens shape: {encoded_tokens.shape}\")\n",
    "        hidden_values = adapter(encoded_tokens.float(), src_length)\n",
    "        #print(f\"hidden values shape: {hidden_values.shape}\")\n",
    "\n",
    "        for i,  hid_val in enumerate(hidden_values): \n",
    "            hid_val = hid_val[input_attn[i]==1]\n",
    "            \n",
    "\n",
    "            for j, token in enumerate(hid_val): \n",
    "                # save the token as a .pth file \n",
    "                path = os.path.join(config.dataset.img_path,phase )\n",
    "                final_path = os.path.join(path, src['name_batch'][i])\n",
    "                #print(final_path)\n",
    "                torch.save(token, f\"{final_path}/aft_adapter_{j}.pth\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Save newly compressed and temporal conv as .pth files and also consider the gloss used for the translation\n",
    "    for i, (src, tgt) in enumerate(tqdm(test_dataloader, desc=f\"adapting test!\")):\n",
    "        phase = \"test\"\n",
    "        titok ,adapter= accelerator.prepare(titok,adapter)\n",
    "        batch = src['input_ids']\n",
    "        src_length = src['src_length_batch']\n",
    "        tgt_attn = tgt.attention_mask\n",
    "        tgt_input = tgt['input_ids']\n",
    "        input_attn = src['attention_mask']\n",
    "\n",
    "        images = batch.to(\n",
    "                    accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "                )\n",
    "        #print(f\"imagges type: {images.type()}\")\n",
    "        tgt_input = tgt['input_ids'].to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "        input_attn = input_attn.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "        tgt_attn = tgt_attn.to(\n",
    "                accelerator.device, memory_format=torch.contiguous_format, non_blocking=True\n",
    "            )\n",
    "        \n",
    "        original_images= torch.clone(images)\n",
    "        # print(f\"names: {src['name_batch']}\")\n",
    "        # print(\"images shape \", original_images.shape)\n",
    "        # print(\"input attn shape\", input_attn.shape)\n",
    "        # print(\"tgt attn shape\", tgt_attn.shape)\n",
    "        # print(\"src length shape\", src_length.shape)\n",
    "        # print(\"tgt input shape\", tgt_input.shape)\n",
    "        # Save a batch of translated images to check by reading\n",
    "        encoded_tokens = titok.encode(x=original_images)[1]['min_encoding_indices'].squeeze()\n",
    "        #print(f\"encoded tokens shape: {encoded_tokens.shape}\")\n",
    "        hidden_values = adapter(encoded_tokens.float(), src_length)\n",
    "        #print(f\"hidden values shape: {hidden_values.shape}\")\n",
    "\n",
    "        for i,  hid_val in enumerate(hidden_values): \n",
    "            hid_val = hid_val[input_attn[i]==1]\n",
    "            #print(f\"hid val: {hid_val.shape}\")\n",
    "            \n",
    "\n",
    "            for j, token in enumerate(hid_val): \n",
    "                # save the token as a .pth file \n",
    "                path = os.path.join(config.dataset.img_path,phase )\n",
    "                final_path = os.path.join(path, src['name_batch'][i])\n",
    "                #print(final_path)\n",
    "                torch.save(token, f\"{final_path}/aft_adapter_{j}.pth\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Save newly compressed and temporal conv as .pth files and also consider the gloss used for the translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing pretrained GFSLT-VLP models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBartForConditionalGeneration(\n",
      "  (model): MBartModel(\n",
      "    (shared): MBartScaledWordEmbedding(2454, 1024, padding_idx=1)\n",
      "    (encoder): MBartEncoder(\n",
      "      (embed_tokens): MBartScaledWordEmbedding(2454, 1024, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): ReLU()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): MBartDecoder(\n",
      "      (embed_tokens): MBartScaledWordEmbedding(2454, 1024, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=2454, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''Visual Encoder part'''\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "visual_encoder_path = \"../pretrain_models/mytran\"\n",
    "\n",
    "trans_encoder = MBartForConditionalGeneration.from_pretrained(visual_encoder_path)\n",
    "print(trans_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the printed model architecture, the model has both an encoder and decoder layers. However, it only has 3 of each. In the GFSLT-VLP, only the encoder is taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBartForConditionalGeneration(\n",
      "  (model): MBartModel(\n",
      "    (shared): MBartScaledWordEmbedding(2454, 1024, padding_idx=1)\n",
      "    (encoder): MBartEncoder(\n",
      "      (embed_tokens): MBartScaledWordEmbedding(2454, 1024, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): MBartDecoder(\n",
      "      (embed_tokens): MBartScaledWordEmbedding(2454, 1024, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=2454, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "'''Text transformer'''\n",
    "\n",
    "text_transformer_path = \"../pretrain_models/MBart_proun\"\n",
    "text_transformer = MBartForConditionalGeneration.from_pretrained(text_transformer_path)\n",
    "print(text_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original target_ids: tensor([[101, 200, 300, 400, 500, 102]])\n",
      "Shifted decoder_input_ids: tensor([[102, 101, 200, 300, 400, 500]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.models.mbart.modeling_mbart import shift_tokens_right\n",
    "\n",
    "# Define our example inputs\n",
    "target_ids = torch.tensor([[101, 200, 300, 400, 500, 102]])  # Shape (1, 6)\n",
    "\n",
    "# Shift tokens with decoder start token <bos> (100)\n",
    "shifted_target_ids = shift_tokens_right(target_ids, pad_token_id=1)\n",
    "\n",
    "print(\"Original target_ids:\", target_ids)\n",
    "print(\"Shifted decoder_input_ids:\", shifted_target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: tensor([[250025,  61168,  41779,   1322,  70871,     30,      2]])\n",
      "Decoded Sentence: 今天是个好天气。\n",
      "\n",
      "Shifted Token IDs: tensor([[     2, 250025,  61168,  41779,   1322,  70871,     30]])\n",
      "Shifted Decoded Sentence: 今天是个好天气。\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBart50Tokenizer\n",
    "import torch\n",
    "from transformers.models.mbart.modeling_mbart import shift_tokens_right\n",
    "# Initialize the MBart50 tokenizer\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer.src_lang = \"zh_CN\"  # Set source language to Simplified Chinese\n",
    "\n",
    "# Example Chinese sentence\n",
    "sentence = \"今天是个好天气。\"  # \"Today is a good weather.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokenized = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n",
    "input_ids = tokenized['input_ids']\n",
    "\n",
    "# Display the token IDs and decoded sentence to verify\n",
    "print(\"Token IDs:\", input_ids)\n",
    "print(\"Decoded Sentence:\", tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# Shift tokens with decoder start token <bos> (100)\n",
    "shifted_input_ids = shift_tokens_right(input_ids, pad_token_id=1)\n",
    "\n",
    "print(\"\\nShifted Token IDs:\", shifted_input_ids)\n",
    "print(\"Shifted Decoded Sentence:\", tokenizer.decode(shifted_input_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
